title,articleUrls,keywords,text
Neural Network on Beer Dataset,https://medium.com/swlh/neural-network-on-beer-dataset-55d62a0e7c32?source=tag_archive---------0-----------------------,"Neural Network,Beer,R,Nn,Ann","Artificial neural networks (ANNs), usually simply called neural networks (NNs), are computing systems vaguely inspired by the biological neural networks that constitute animal brains.An ANN is based on a collection of connected units or nodes called artificial neurons, which loosely model the neurons in a biological brain. Each connection, like the synapses in a biological brain, can transmit a signal to other neurons. An artificial neuron that receives a signal then processes it and can signal neurons connected to it. The “signal” at a connection is a real number, and the output of each neuron is computed by some non-linear function of the sum of its inputs. The connections are called edges. Neurons and edges typically have a weight that adjusts as learning proceeds. The weight increases or decreases the strength of the signal at a connection. Neurons may have a threshold such that a signal is sent only if the aggregate signal crosses that threshold. Typically, neurons are aggregated into layers. Different layers may perform different transformations on their inputs. Signals travel from the first layer (the input layer), to the last layer (the output layer), possibly after traversing the layers multiple times.Photo by Vlad Tchompalov on UnsplashNeural networks learn (or are trained) by processing examples, each of which contains a known “input” and “result,” forming probability-weighted associations between the two, which are stored within the data structure of the net itself. The training of a neural network from a given example is usually conducted by determining the difference between the processed output of the network (often a prediction) and a target output. This is the error. The network then adjusts it’s weighted associations according to a learning rule and using this error value. Successive adjustments will cause the neural network to produce output which is increasingly similar to the target output. After a sufficient number of these adjustments the training can be terminated based upon certain criteria. This is known as [[supervised learning]].Let’s workInstall Packagespackages <- c(""xts"",""zoo"",""PerformanceAnalytics"", ""GGally"", ""ggplot2"", ""ellipse"", ""plotly"")newpack  = packages[!(packages %in% installed.packages()[,""Package""])]if(length(newpack)) install.packages(newpack)a=lapply(packages, library, character.only=TRUE)Load datasetbeer <- read.csv(""MyData.csv"")head(beer)summary(beer)Clase               Color        BoilGravity        IBU         Length:1000        Min.   : 1.99   Min.   : 1.0   Min.   :  0.00   Class :character   1st Qu.: 5.83   1st Qu.:27.0   1st Qu.: 32.90   Mode  :character   Median : 7.79   Median :33.0   Median : 47.90                      Mean   :13.45   Mean   :33.8   Mean   : 51.97                      3rd Qu.:12.57   3rd Qu.:39.0   3rd Qu.: 67.77                      Max.   :50.00   Max.   :90.0   Max.   :144.53        ABV         Min.   : 2.390   1st Qu.: 5.240   Median : 5.990   Mean   : 6.093   3rd Qu.: 6.810   Max.   :10.380Visualization of Iris Data SetYou can also embed plots, for example:pairs(beer[2:5],       main = ""Craft Beer Data -- 5 types"",      pch = 21, bg = c(""red"", ""green"", ""blue"", ""orange"", ""yellow""))library(GGally)pm <- ggpairs(beer,lower=list(combo=wrap(""facethist"",  binwidth=0.5)),title=""Craft Beer"", mapping=aes(color=Clase))pmlibrary(PerformanceAnalytics)chart.Correlation2 <- function (R, histogram = TRUE, method = NULL, ...)    {        x = checkData(R, method = ""matrix"")        if (is.null(method)) #modified            method = 'pearson'          use.method <- method #added        panel.cor <- function(x, y, digits = 2, prefix = """",                               use = ""pairwise.complete.obs"",                               method = use.method, cex.cor, ...)         { #modified        usr <- par(""usr"")        on.exit(par(usr))        par(usr = c(0, 1, 0, 1))        r <- cor(x, y, use = use, method = method)        txt <- format(c(r, 0.123456789), digits = digits)[1]        txt <- paste(prefix, txt, sep = """")        if (missing(cex.cor))             cex <- 0.8/strwidth(txt)        test <- cor.test(as.numeric(x), as.numeric(y), method = method)        Signif <- symnum(test$p.value, corr = FALSE, na = FALSE,                          cutpoints = c(0, 0.001, 0.01, 0.05, 0.1, 1),                         symbols = c(""***"",""**"", ""*"", ""."", "" ""))        text(0.5, 0.5, txt, cex = cex * (abs(r) + 0.3)/1.3)        text(0.8, 0.8, Signif, cex = cex, col = 2)        }    f <- function(t)        {        dnorm(t, mean = mean(x), sd = sd.xts(x))        }    dotargs <- list(...)    dotargs$method <- NULL    rm(method)    hist.panel = function(x, ... = NULL)         {        par(new = TRUE)        hist(x, col = ""light gray"", probability = TRUE, axes = FALSE,              main = """", breaks = ""FD"")        lines(density(x, na.rm = TRUE), col = ""red"", lwd = 1)        rug(x)        }    if (histogram)         pairs(x, gap = 0, lower.panel = panel.smooth,               upper.panel = panel.cor, diag.panel = hist.panel)    else pairs(x, gap = 0, lower.panel = panel.smooth, upper.panel = panel.cor)    }#if method option not set default is 'pearson'chart.Correlation2(beer[,2:5], histogram=TRUE, pch=""21"")library(plotly)pm <- GGally::ggpairs(beer, aes(color = Clase), lower=list(combo=wrap(""facethist"",  binwidth=0.5)))class(pm)pm‘gg’‘ggmatrix’Setup and Train the Neural Network for Beer DataNeural Network emulates how the human brain works by having a network of neurons that are interconnected and sending stimulating signal to each other.In the Neural Network model, each neuron is equivalent to a logistic regression unit. Neurons are organized in multiple layers where every neuron at layer i connects out to every neuron at layer i+1 and nothing else.The tuning parameters in Neural network includes the number of hidden layers, number of neurons in each layer, as well as the learning rate.There are no fixed rules to set these parameters and depends a lot in the problem domain. My default choice is to use a single hidden layer and set the number of neurons to be the same as the input variables. The number of neurons at the output layer depends on how many binary outputs need to be learned. In a classification problem, this is typically the number of possible values at the output category.The learning happens via an iterative feedback mechanism where the error of training data output is used to adjusted the corresponding weights of input. This adjustment will be propagated back to previous layers and the learning algorithm is known as back-propagation.library(neuralnet)beer <- beer%>%    select(""IBU"",""ABV"",""Color"",""BoilGravity"",""Clase"")head(beer)# Binarize the categorical outputbeer <- cbind(beer, beer$Clase == 'ALE')beer <- cbind(beer, beer$Clase == 'IPA')beer <- cbind(beer, beer$Clase == 'PALE')beer <- cbind(beer, beer$Clase == 'STOUT')beer <- cbind(beer, beer$Clase == 'PORTER')names(beer)[6] <- 'ALE'names(beer)[7] <- 'IPA'names(beer)[8] <- 'PALE'names(beer)[9] <- 'STOUT'names(beer)[10] <- 'PORTER'head(beer)set.seed(101)beer.train.idx <- sample(x = nrow(beer), size = nrow(beer)*0.5)beer.train <- beer[beer.train.idx,]beer.valid <- beer[-beer.train.idx,]Visulization of the Neural Network on Beer DataHere is the plot of the Neural network we learnNeural network is very good at learning non-linear function and also multiple outputs can be learnt at the same time. However, the training time is relatively long and it is also susceptible to local minimum traps. This can be mitigated by doing multiple rounds and pick the best learned model.nn <- neuralnet(ALE+IPA+PALE+STOUT+PORTER ~ IBU+ABV+Color+BoilGravity, data=beer.train, hidden=c(5))plot(nn, rep = ""best"")Resultbeer.prediction <- compute(nn, beer.valid[-5:-10])idx <- apply(beer.prediction$net.result, 1, which.max)predicted <- c('ALE','IPA', 'PALE', 'STOUT', 'PORTER')[idx]table(predicted, beer.valid$Clase)predicted ALE IPA PALE PORTER STOUT    ALE    17   3   12      0     0    IPA     1 203   21      0     2    PALE   29  26   84      1     0    STOUT   0   4    0     30    67Accuracy of model is calculated as follows((17+203+84+0+67)/nrow(beer.valid))*10074.2# nn$result.matrixstr(nn)List of 14 $ call               : language neuralnet(formula = ALE + IPA + PALE + STOUT + PORTER ~ IBU + ABV + Color +      BoilGravity, data = beer.train, hidden = c(5)) $ response           : logi [1:500, 1:5] FALSE FALSE FALSE FALSE FALSE FALSE ...  ..- attr(*, ""dimnames"")=List of 2  .. ..$ : chr [1:500] ""841"" ""825"" ""430"" ""95"" ...  .. ..$ : chr [1:5] ""ALE"" ""IPA"" ""PALE"" ""STOUT"" ... $ covariate          : num [1:500, 1:4] 62.3 27.1 39 72.3 67.8 ...  ..- attr(*, ""dimnames"")=List of 2  .. ..$ : chr [1:500] ""841"" ""825"" ""430"" ""95"" ...  .. ..$ : chr [1:4] ""IBU"" ""ABV"" ""Color"" ""BoilGravity"" $ model.list         :List of 2  ..$ response : chr [1:5] ""ALE"" ""IPA"" ""PALE"" ""STOUT"" ...  ..$ variables: chr [1:4] ""IBU"" ""ABV"" ""Color"" ""BoilGravity"" $ err.fct            :function (x, y)    ..- attr(*, ""type"")= chr ""sse"" $ act.fct            :function (x)    ..- attr(*, ""type"")= chr ""logistic"" $ linear.output      : logi TRUE $ data               :'data.frame':	500 obs. of  10 variables:  ..$ IBU        : num [1:500] 62.3 27.1 39 72.3 67.8 ...  ..$ ABV        : num [1:500] 5.9 5.07 6.57 5.7 6.86 5.21 4.22 5.57 5.76 7.76 ...  ..$ Color      : num [1:500] 5.61 32.07 39.92 9.62 8.29 ...  ..$ BoilGravity: int [1:500] 37 25 40 37 31 28 19 27 30 44 ...  ..$ Clase      : chr [1:500] ""IPA"" ""PORTER"" ""STOUT"" ""PALE"" ...  ..$ ALE        : logi [1:500] FALSE FALSE FALSE FALSE FALSE FALSE ...  ..$ IPA        : logi [1:500] TRUE FALSE FALSE FALSE TRUE FALSE ...  ..$ PALE       : logi [1:500] FALSE FALSE FALSE TRUE FALSE TRUE ...  ..$ STOUT      : logi [1:500] FALSE FALSE TRUE FALSE FALSE FALSE ...  ..$ PORTER     : logi [1:500] FALSE TRUE FALSE FALSE FALSE FALSE ... $ exclude            : NULL $ net.result         :List of 1  ..$ : num [1:500, 1:5] 0.00942 0.01859 0.01845 0.00916 0.00478 ...  .. ..- attr(*, ""dimnames"")=List of 2  .. .. ..$ : chr [1:500] ""841"" ""825"" ""430"" ""95"" ...  .. .. ..$ : NULL $ weights            :List of 1  ..$ :List of 2  .. ..$ : num [1:5, 1:5] -10.8295 0.0944 0.9985 -0.1776 0.0445 ...  .. ..$ : num [1:6, 1:5] 0.0576 -0.058 -0.4324 0.4371 -0.0437 ... $ generalized.weights:List of 1  ..$ : num [1:500, 1:20] -0.08239 -0.000124 -0.000822 -0.082905 -0.093232 ...  .. ..- attr(*, ""dimnames"")=List of 2  .. .. ..$ : chr [1:500] ""841"" ""825"" ""430"" ""95"" ...  .. .. ..$ : NULL $ startweights       :List of 1  ..$ :List of 2  .. ..$ : num [1:5, 1:5] -0.5 1.832 -0.329 0.261 -1.112 ...  .. ..$ : num [1:6, 1:5] 0.341 1.107 0.689 0.471 -1.64 ... $ result.matrix      : num [1:58, 1] 8.02e+01 8.76e-03 7.37e+04 -1.08e+01 9.44e-02 ...  ..- attr(*, ""dimnames"")=List of 2  .. ..$ : chr [1:58] ""error"" ""reached.threshold"" ""steps"" ""Intercept.to.1layhid1"" ...  .. ..$ : NULL - attr(*, ""class"")= chr ""nn""beer.net <- neuralnet(ALE+IPA+PALE+STOUT+PORTER ~ IBU+ABV+Color+BoilGravity,                       data=beer.train, hidden=c(5),  err.fct = ""ce"",                       linear.output = F, lifesign = ""minimal"",                       threshold = 0.1)hidden: 5    thresh: 0.1    rep: 1/1    steps:   86036	error: 431.94881	time: 24.02 secsplot(beer.net, rep=""best"")Predicting Resultbeer.prediction <- compute(beer.net, beer.valid[-5:-10])idx <- apply(beer.prediction$net.result, 1, which.max)predicted <- c('ALE','IPA', 'PALE', 'STOUT', 'PORTER')[idx]table(predicted, beer.valid$Clase)predicted ALE IPA PALE PORTER STOUT   ALE     26   4    9      0     0   IPA      0 197   30      1     3   PALE    21  33   78      0     0   PORTER   0   1    0     10     6   STOUT    0   1    0     20    60Accuracy of model is calculated as follows((26+197+78+10+60)/nrow(beer.valid))*10074.2ConclusionAs you can see the accuracy is equal!I hope it will help you to develop your training.Never give up!See you in Linkedin!References:https://rpubs.com/vitorhs/irishttps://rstudio-pubs-static.s3.amazonaws.com/392220_1323b4203fb04ac4a326d20b6d445d63.html#loading-data-from-uci-ml-repositoryThe StartupMedium's largest active publication, followed by +723K people. Follow to join our community.Follow53 Public domain.Neural NetworkBeerRNnAnn53 claps53 clapsWritten byOscar RojoFollowCurrently studing a Master in Data Science. Passionate about learning new skills. Former branch risk analyst. https://www.linkedin.com/in/oscar-rojo-martin/FollowThe StartupFollowMedium's largest active publication, followed by +723K people. Follow to join our community.FollowWritten byOscar RojoFollowCurrently studing a Master in Data Science. Passionate about learning new skills. Former branch risk analyst. https://www.linkedin.com/in/oscar-rojo-martin/The StartupFollowMedium's largest active publication, followed by +723K people. Follow to join our community.More From MediumArithmetic Mean and Its Applications in Data AnalyticsMahbubul Alam in Towards AIDeploy a Digital Twin in 6 Months for $1M USDDanny CastonguayAn introduction to frequent pattern mining researchMadalina CiortanMaking Decisions with TreesSteven Loaiza in The StartupFor the love of regressionSyed Misbah in Data DecodedModelling Resale Flat Prices in Singapore | How do spatial attributes play a part?HiddenJellyfishCross-platform geospatial visualization with deck.gl-nativeGoogle Earth in Google Earth and Earth EngineAvoiding technical debt in social science researchSkye Toor in Pew Research Center: DecodedLearn more.Medium is an open platform where 170 million readers come to find insightful and dynamic thinking."
A Complete Introduction To Time Series Analysis (with R):: Linear processes I,https://medium.com/analytics-vidhya/a-complete-introduction-to-time-series-analysis-with-r-linear-processes-i-88a1b55db9ef?source=tag_archive---------1-----------------------,"Time Series Analysis,Machine Learning,Forecasting,R,Statistics","All linear processes can be expressed as a weighted sum of past noise, given certain conditions.In the last tutorial , we saw how we could express the probabilistic form of the best linear predictor of a future observation based on the data at hand. We will see how to implement this in R later! In this article, we will study an important class of time series: linear processes. Let’s jump right into it!q-correlation and strong stationarityHow can more rigorously define the stationarity of time series? How about “semi-stationarity”? We use the concept of q-correlation to do this.A time-series process is called strictly stationary or strongly stationary ifthat is, if the joint distribution of the observations X_{1}, …, X_{n} is the same as the one of the h-lagged set of observations.PropertiesAll elements of a strongly stationary time series are identically distributed (but not necessarily independent!)The distribution of any subset of observations is the same as the h-lagged setFinite second moment implies weakly stationarityFor convenience, here’s once more the definition of weak stationarity:Note that strong stationarity is a much stronger condition! There are two important things to notice:Strong stationarity implies weak stationarity, but the opposite is not true!The I.I.D process is strongly stationary.The main question is: how do we construct / how can we characterize stationary processes? In particular, if we know that the IID is stationary, perhaps there is some mapping to more general sequences. In such a case, how can we determine their “level of stationarity”? The following concepts.q-dependence and q-correlationWhat the previous proposition is saying is that if we have an IID sequence, under certain conditions, we can construct a new series that is also strongly stationary, using some function g. Further, we can define q-dependence by saying that observations |t-s| lags apart are independent, but everything in between is dependent, and similarly for correlation.Linear ProcessesNow we finally get to one of the most important parts of Time Series Analysis: linear processes.Example: MA(1)The MA(1) processExample: AR(1)The AR(1) processand we have thatIf you are curious about why the coefficients are that way, you can attempt to solve the recursion by plugging back the definition of X_{t} substituting {t-1}. Note that we have to play the restriction on the AR(1) coefficients so that it satisfies the conditions of a linear process as we described above.Linear processes in terms of the backward operatorRemember the backward shift operator that we saw back in the differencing section? We can also use it to represent linear processes.That is, we define the Psi operator as the infinite polynomial above, in which each term is exponentiated accordingly. If we plug in the backward shift operator, we then have a concrete way to represent the linear process briefly! This will become very useful to represent a more complex process without having to write down everything explicitly. Another way to see it is that we can see a linear process as an operation applied to the noise at time t.Next timeThat’s it for today! Next time, we will continue studying some more properties of the linear process, along with some interesting propositions, and other useful operators commonly used when dealing with Time Series. Until then!A Complete Introduction To Time Series Analysis (with R):: Linear processes IILast time, we left off at the representing Linear Processes in terms of the backward-shift operator:medium.comLast timePrediction 1 → Best Linear Predictors IIA Complete Introduction To Time Series Analysis (with R):: Prediction 1 → Best Predictors IIIn the last article, we saw how we could obtain the best linear predictor (BLP) of X_{n+h} given a function of X_{n}…medium.comMain pageA Complete Introduction To Time Series Analysis (with R)During these times of the Covid19 pandemic, you have perhaps heard about the collaborative efforts to predict new…medium.comFollow me athttps://blog.jairparraml.com/https://www.linkedin.com/in/hair-parra-526ba19b/https://github.com/JairParrahttps://medium.com/@hair.parraHair Parra — Data Engineer / Software Engineer- DevX Analytics — Cisco | LinkedInhttps://blog.jairparraml.com/ *** B.A. major Computer Science. Double minor in Statistics and Linguistics, at McGill…www.linkedin.comAnalytics VidhyaAnalytics Vidhya is a community of Analytics and Data…Follow5 Sign up for Data Science Blogathon: Win Lucrative Prizes!By Analytics VidhyaLaunching the Second Data Science Blogathon – An Unmissable Chance to Write and Win Prizesprizes worth INR 30,000+! Take a lookGet this newsletterBy signing up, you will create a Medium account if you don’t already have one. Review our Privacy Policy for more information about our privacy practices.Check your inboxMedium sent you an email at  to complete your subscription.Time Series AnalysisMachine LearningForecastingRStatistics5 claps5 clapsWritten byHair ParraFollowData Scientist & Data Engineer at Cisco, Canada. McGill University CS, Stats & Linguistics graduate. Polyglot.FollowAnalytics VidhyaFollowAnalytics Vidhya is a community of Analytics and Data Science professionals. We are building the next-gen data science ecosystem https://www.analyticsvidhya.comFollowWritten byHair ParraFollowData Scientist & Data Engineer at Cisco, Canada. McGill University CS, Stats & Linguistics graduate. Polyglot.Analytics VidhyaFollowAnalytics Vidhya is a community of Analytics and Data Science professionals. We are building the next-gen data science ecosystem https://www.analyticsvidhya.comMore From MediumSentiment Classification with BOWDipika Baad in The StartupSliding Window Price PredictionsLauren Faulds in Analytics VidhyaDemocratizing ML: Rise of the Teachable MachinesShuvam Manna in The StartupAutoencoders & Power of Mathematical OptimizationsChan Woo Kim in Human Machine LearningReview: Recursive Deep Models for Semantic Compositionality Over a Sentiment TreebankAnindya S. Das in The StartupMixed Formal Learning:Sandra Carrico in GLYNT.AIXGBoost Classifier Hand Written Digit recognitionNiketanpanchal in Analytics VidhyaHow to not build a three-point ROC curveOtávio Vasques in DataLab LogLearn more.Medium is an open platform where 170 million readers come to find insightful and dynamic thinking."
Predicting large text data with spark via the R package sparklyr,https://medium.com/@zumaia/predicting-large-text-data-with-spark-via-the-r-package-sparklyr-bfe102b2dce0?source=tag_archive---------2-----------------------,"Spark,R,Predict,Sparklyr,Wordcloud","Unlike the classical programming languages that are very slow and even sometimes fail to load very large data sets since they use only a single core, Apache Spark is known as the fastest distributed system that can handle with ease large datasets by deploying all the available machines and cores to build cluster, so that the computing time of each task performed on the data will be drastically reduced since each worker node in the cluster takes in charge small part of the task in question. Even that the native language of spark is scala (but it can also support java and sql), the good news for R users is that they can benefit from spark without having to learn the above supported languages by making use of the R package sparklyr. In this article we trained random forest model using text data which is in practice known as large data set. for illustration purposes and to make things faster however we used a small data set about email messages and also constrained ourselves to use the local mode in which spark created a cluster from the available cores in my machine. Notice that the same codes in this paper can be used in the cloud whatever the size of our data, even with billions of data points, except for the connection method to spark which is slightly different. Since the raw data requires some transformation to be consumed by the model, we applied the well-known method called tokenization to create the model features, then trained and evaluated a random forest model applied on the design matrix after having been filled using the TF method. Lastly, we trained the same model (random forest model with the same hyperparameter values) using another mothed called TF-IDF method (Sparck , 1972).Photo by Mahir Uysal on UnsplashKeywordsLarge dataset, R, spark, sparklyr, cluster, tokenization, TF, TF-IDF, random forest model, machine learning.IntroductionR is one of the best programming languages for statistical analysis, and provides data scientist by super powerful tools that make their work super easy and more exciting. However, since the amount of information today is growing exponentially, R and all the classical languages (python, java,…etc.) that use one single machine (one single core node) would face a great challenges to handle and deal with large dataset that, in some cases, its size can even exceed the memory size. As a solution to the above classical programming language limitations, spark and hadoop are two new systems. Both use a computing distributed system that run multiple tasks using multiple machines (called nodes, and together called cluster) at the same time. However, spark has the superiority over hadoop by its ability to load the data in memory which makes it much higher faster (Luraschi, 2014). Spark creates a cluster using either physical machines or virtual machines provided by some cloud provider such as google, amazon, microsoft…etc (it can also creat a cluster using the available cores in a single machine known as local mode). Its native language is scala, but also can support sql and java. Thankfully, spark provides a high level APIs in python and R so that the R users can use spark as a platform to work with large datasets using their familiar codes and without having to learn scala, sql or java. However, the connection between R and spark is not straightforward, it is set by the help of sparklyr package, which is like any other R packages, with its own functions and supports almost all the famous dplyr R package functions. Usually, most of text data are considered as large datasets, either due to their large sizes or the large computing time required for their manipulations or modulizations. That is why, in this paper, we will train Random forest model using sparklyr to predict whether a text message is spam or ham from the data set SMSSpamCollection uploaded from kaggle website. To convert the character features to numeric type we will use two famous methods , TF transformation, and TF-IDF (Jones, 1972) transformation. This article will be divided into the following sections:Data Preparation: we will illustrate how do we read, clean, and prepare the data to be consumed by the model.TF Method: we will train a random forest model (James et al, 2013) on the term frequency TF features.TF-IDF method: We will train the random forest model on the TF_IDF features.Add features: we will create another feature from the data to be used as a new predictor.InformationTo do this exercise my computer has worked on:Spark version 3.0.0Jupyter notebook 6.0.3R version 4.0.2 (2020–06–22)Data preparationFirst, we call the R packages tidyverse and sparklyr, and we set up the connection to spark using the following R codes.packages <- c(""sparklyr"", ""wordcloud"", ""tm"", ""slam"", ""NLP"", ""RColorBrewer"")newpack  = packages[!(packages %in% installed.packages()[,""Package""])]if(length(newpack)) install.packages(newpack)a=lapply(packages, library, character.only=TRUE)Let’s see the session infosessionInfo()R version 4.0.2 (2020-06-22)Platform: x86_64-pc-linux-gnu (64-bit)Running under: Ubuntu 20.04.1 LTSMatrix products: defaultBLAS:   /usr/lib/x86_64-linux-gnu/blas/libblas.so.3.9.0LAPACK: /usr/lib/x86_64-linux-gnu/lapack/liblapack.so.3.9.0locale: [1] LC_CTYPE=es_ES.UTF-8       LC_NUMERIC=C               [3] LC_TIME=es_ES.UTF-8        LC_COLLATE=es_ES.UTF-8     [5] LC_MONETARY=es_ES.UTF-8    LC_MESSAGES=es_ES.UTF-8    [7] LC_PAPER=es_ES.UTF-8       LC_NAME=C                  [9] LC_ADDRESS=C               LC_TELEPHONE=C            [11] LC_MEASUREMENT=es_ES.UTF-8 LC_IDENTIFICATION=C       attached base packages:[1] stats     graphics  grDevices utils     datasets  methods   base     other attached packages:[1] slam_0.1-47        tm_0.7-7           NLP_0.2-0          wordcloud_2.6     [5] RColorBrewer_1.1-2 sparklyr_1.3.1    loaded via a namespace (and not attached): [1] Rcpp_1.0.5        pillar_1.4.4      compiler_4.0.2    dbplyr_1.4.4      [5] r2d3_0.2.3        base64enc_0.1-3   tools_4.0.2       digest_0.6.25     [9] uuid_0.1-4        jsonlite_1.7.0    evaluate_0.14     tibble_3.0.2     [13] lifecycle_0.2.0   pkgconfig_2.0.3   rlang_0.4.6       IRdisplay_0.7.0  [17] DBI_1.1.0         rstudioapi_0.11   parallel_4.0.2    IRkernel_1.1     [21] xml2_1.3.2        repr_1.1.0        httr_1.4.1        dplyr_1.0.0      [25] generics_0.0.2    htmlwidgets_1.5.1 vctrs_0.3.1       rprojroot_1.3-2  [29] tidyselect_1.1.0  glue_1.4.1        forge_0.2.0       R6_2.4.1         [33] pbdZMQ_0.3-3      purrr_0.3.4       blob_1.2.1        magrittr_1.5     [37] backports_1.1.8   ellipsis_0.3.1    htmltools_0.5.0   assertthat_0.2.1 [41] crayon_1.3.4suppressPackageStartupMessages(library(sparklyr))suppressPackageStartupMessages(library(tidyverse))sc<-spark_connect(master = ""local"")Second, get the sms saved on my smartphone with an app.I use “SMS Backup & Restore”SMS Backup & Restore - Aplicaciones en Google PlaySMS Backup & Restore is an app that backs up (creates a copy of) SMS & MMS messages and call logs currently available…play.google.comThird, we load the xml data and transform in dataframelibrary(xml2)x <- read_xml(""sms.xml"")xml_name(x)‘allsms’xml_children(x){xml_nodeset (463)} [1] <sms address=""Allianz"" time=""29 jul. 2020 10:00:41"" date=""1596009641280"" ... [2] <sms address=""Loterias"" time=""26 jul. 2020 16:54:11"" date=""1595775251153 ... [3] <sms address=""Loterias"" time=""26 jul. 2020 13:10:24"" date=""1595761824372 ... [4] <sms address=""Lowi"" time=""24 jul. 2020 18:58:49"" date=""1595609929489"" ty ... [5] <sms address=""Lowi"" time=""24 jul. 2020 18:58:22"" date=""1595609902331"" ty ... [6] <sms address=""TARJETACTF"" time=""24 jul. 2020 12:38:37"" date=""15955871177 ... [7] <sms address=""SANT-ENVIOS"" time=""21 jul. 2020 13:20:07"" date=""1595330407 ... [8] <sms address=""Hawkers Co"" time=""21 jul. 2020 10:02:15"" date=""15953185358 ... [9] <sms address=""powert"" time=""20 jul. 2020 14:12:52"" date=""1595247172541""  ...[10] <sms address=""SEGURIDAD"" time=""20 jul. 2020 14:06:35"" date=""159524679541 ...[11] <sms address=""Loterias"" time=""19 jul. 2020 13:54:36"" date=""1595159676865 ...[12] <sms address=""34973900233"" time=""19 jul. 2020 13:07:31"" date=""1595156851 ...[13] <sms address=""34973900233"" time=""19 jul. 2020 13:07:10"" date=""1595156830 ...[14] <sms address=""217512"" time=""15 jul. 2020 17:33:20"" date=""1594827200346""  ...[15] <sms address=""INFOSNET"" time=""15 jul. 2020 14:19:11"" date=""1594815551892 ...[16] <sms address=""Loterias"" time=""15 jul. 2020 12:34:00"" date=""1594809240461 ...[17] <sms address=""ING"" time=""14 jul. 2020 10:45:36"" date=""1594716336196"" typ ...[18] <sms address=""Fintonic"" time=""14 jul. 2020 10:29:56"" date=""1594715396086 ...[19] <sms address=""INFOSNET"" time=""13 jul. 2020 14:41:10"" date=""1594644070835 ...[20] <sms address=""SegSocial"" time=""13 jul. 2020 9:46:39"" date=""1594626399661 ......baz <- xml_find_all(x, "".//sms"")xml_path(baz)address <- xml_attr(baz, ""address"")time <- xml_attr(baz, ""time"")type <- xml_attr(baz, ""type"")date <- xml_attr(baz, ""date"")body <- xml_attr(baz, ""body"")read <- xml_attr(baz, ""read"")service <- xml_attr(baz, ""service_center"")name <- xml_attr(baz, ""name"")X = data.frame(address, time, type, date, body, read, service, name, stringsAsFactors = FALSE)names(X)<-c(""sender"", ""time"",""label"",""ref"",""message"",""read"",""phone_number"",""name"")X$label <- str_replace(X$label, ""1"", ""spam"")X$label <- str_replace(X$label, ""2"", ""ham"")‘/allsms/sms[1]’‘/allsms/sms[2]’‘/allsms/sms[3]’…………‘/allsms/sms[461]’‘/allsms/sms[462]’‘/allsms/sms[463]’write.table(X, file = ""sms.txt"", sep = ""\t"",            row.names = TRUE, col.names = NA)path <- ""sms.txt""mydata<-spark_read_csv(sc,name=""SMS"",path=path, header=TRUE, delimiter = ""\t"",overwrite = TRUE)knitr::kable(head(mydata,2))| _c0|sender   |time                  |label |          ref|message                                                                                                                                                 | read| phone_number|name                    ||---:|:--------|:---------------------|:-----|------------:|:-------------------------------------------------------------------------------------------------------------------------------------------------------|----:|------------:|:-----------------------||   1|Allianz  |29 jul. 2020 10:00:41 |spam  | 1.596010e+12|Allianz METEO: Alerta en DONOSTIA-SAN SEBASTIAN mañana por: Temperaturas elevadas.  Mas info. 902300186  https://www.allianz.es/centro-respuesta-rapida |    1|  34609090909|NA                      ||   2|Loterias |26 jul. 2020 16:54:11 |spam  | 1.595775e+12|Tu apuesta PRIM en www.loteriasyapuestas.es ha sido premiada con 2,00 EUR. Ya dispones del importe en tu Lotobolsa                                      |    1| summary(mydata)Length Class          Modesrc 1      src_spark      listops 2      op_base_remote listsdf_dim(mydata)4639str(mydata)List of 2 $ src:List of 1  ..$ con:List of 13  .. ..$ master      : chr ""local[4]""  .. ..$ method      : chr ""shell""  .. ..$ app_name    : chr ""sparklyr""  .. ..$ config      :List of 6  .. .. ..$ spark.env.SPARK_LOCAL_IP.local           : chr ""127.0.0.1""  .. .. ..$ sparklyr.connect.csv.embedded            : chr ""^1.*""  .. .. ..$ spark.sql.legacy.utcTimestampFunc.enabled: logi TRUE  .. .. ..$ sparklyr.connect.cores.local             : int 4  .. .. ..$ spark.sql.shuffle.partitions.local       : int 4  .. .. ..$ sparklyr.shell.driver-memory             : chr ""2g""  .. .. ..- attr(*, ""config"")= chr ""default""  .. .. ..- attr(*, ""file"")= chr ""/home/oscar/R/x86_64-pc-linux-gnu-library/4.0/sparklyr/conf/config-template.yml""  .. ..$ state       :<environment: 0x55b8cbc89bd8>   .. ..$ extensions  :List of 5  .. .. ..$ jars        : chr(0)   .. .. ..$ packages    : chr(0)   .. .. ..$ initializers: list()  .. .. ..$ catalog_jars: chr(0)   .. .. ..$ repositories: chr(0)   .. ..$ spark_home  : chr ""/opt/spark""  .. ..$ backend     : 'sockconn' int 5  .. .. ..- attr(*, ""conn_id"")=<externalptr>   .. ..$ monitoring  : 'sockconn' int 6  .. .. ..- attr(*, ""conn_id"")=<externalptr>   .. ..$ gateway     : 'sockconn' int 4  .. .. ..- attr(*, ""conn_id"")=<externalptr>   .. ..$ output_file : chr ""/tmp/RtmpQqu9TZ/filea40f3932a5af_spark.log""  .. ..$ sessionId   : num 43247  .. ..$ home_version: chr ""3.0.0""  .. ..- attr(*, ""class"")= chr [1:3] ""spark_connection"" ""spark_shell_connection"" ""DBIConnection""  ..- attr(*, ""class"")= chr [1:3] ""src_spark"" ""src_sql"" ""src"" $ ops:List of 2  ..$ x   : 'ident' chr ""SMS""  ..$ vars: chr [1:9] ""_c0"" ""sender"" ""time"" ""label"" ...  ..- attr(*, ""class"")= chr [1:3] ""op_base_remote"" ""op_base"" ""op"" - attr(*, ""class"")= chr [1:4] ""tbl_spark"" ""tbl_sql"" ""tbl_lazy"" ""tbl""colnames(mydata)‘_c0’‘sender’‘time’‘label’‘ref’‘message’‘read’‘phone_number’‘name’We can also take a look at some messages by displaying the first three rows.select(mydata,message)%>%  head(3) %>%   knitr::kable(""html"")<table> <thead>  <tr>   <th style=""text-align:left;""> message </th>  </tr> </thead><tbody>  <tr>   <td style=""text-align:left;""> Allianz METEO: Alerta en DONOSTIA-SAN SEBASTIAN mañana por: Temperaturas elevadas.  Mas info. 902300186  https://www.allianz.es/centro-respuesta-rapida </td>  </tr>  <tr>   <td style=""text-align:left;""> Tu apuesta PRIM en www.loteriasyapuestas.es ha sido premiada con 2,00 EUR. Ya dispones del importe en tu Lotobolsa </td>  </tr>  <tr>   <td style=""text-align:left;""> Tu apuesta BONO en www.loteriasyapuestas.es ha sido premiada con 5,00 EUR. Ya dispones del importe en tu Lotobolsa </td>  </tr></tbody></table>Modeling text data requires special attention since most of the machine learning algorithms require numeric data, so how do we can transform the text entries in messages into numeric type?.The most well known approach is called tokenization, this simply means splitting each text in the column messages into small pieces called tokens (also called bag of words) in a way such that each token has meaningful effect to discriminating between the dependent variable labels. For example, if we think that arbitrary numbers or some symbols like / or dots … etc. do not have any discriminating impact then we can remove them from the entries.Each row in this data (which is labeled as ham or spam ) is considered as document ( 5574 documents in our case) that has a text (which is a collection of tokens), and the whole data after tokenization (as a rectangular matrix) is called corpus.To keep things simple let’s suppose that everything except the words are useless for predicting the labels, so we can use the function spark sql function regexp_replace to remove everything except letters, then we rename the resulted column cleaned.options(max.print=999999)require(sparklyr)#newdata <- X %<>%#    mutate(cleaned=str_replace(message,""[^a-zA-Z]"","" ""))%>%#    mutate(cleaned = tolower(cleaned))%>%#    select(type,cleaned)newdata<-mydata%>%    mutate(cleaned=regexp_replace(message,""[^a-zA-Z]"","" ""))%>%    mutate(cleaned=lower(cleaned))%>%    select(label,cleaned)#newdata%>%#    select(cleaned)%>%#    head(3)%>%#    knitr::kable()newdata%>%    select(cleaned)%>%    head(3)%>%    knitr::kable()|cleaned                                                                                                                                                 ||:-------------------------------------------------------------------------------------------------------------------------------------------------------||allianz meteo  alerta en donostia san sebastian ma ana por  temperaturas elevadas   mas info             https   www allianz es centro respuesta rapida ||tu apuesta prim en www loteriasyapuestas es ha sido premiada con      eur  ya dispones del importe en tu lotobolsa                                      ||tu apuesta bono en www loteriasyapuestas es ha sido premiada con      eur  ya dispones del importe en tu lotobolsa                                      |At this stage and before going ahead we should split the data between training set and testing set. However, since we have an imbalanced data with roughly 7,12% of ham’s and 92,87% of spam’s, we should preserve the proportion of the labels by splitting the data in a such way to get stratified samples.# newdata %>%#     group_by(type)%>%#     summarise(n = n()) %>%#     mutate(freq = n / sum(n))newdata%>%    group_by(label)%>%    count()%>%    collect()%>%    mutate(prop=n/sum(n))%>%    knitr::kable()|label |   n|      prop||:-----|---:|---------:||ham   |  33| 0.0712743||spam  | 430| 0.9287257|To accomplish this task by hand, first we filter the data between ham and spam, then each set will be split randomly between training set and testing set, and next we rbind together the training sets in one set and then we do the same thing for testing sets.dataham<-newdata%>%  filter(label==""ham"")dataspam<-newdata%>%  filter(label==""spam"")partitionham<-dataham%>%    sdf_random_split(training=0.8,test=0.2,seed = 111)partitionspam<-dataspam%>%    sdf_random_split(training=0.8,test=0.2,seed = 111)train<-sdf_bind_rows(partitionham$training,partitionspam$training)%>%    compute(""train"")test<-sdf_bind_rows(partitionham$test,partitionspam$test)%>%    compute(""test"")head(train)[38;5;246m# Source: spark<?> [?? x 2][39m  label cleaned                                                                   [3m[38;5;246m<chr>[39m[23m [3m[38;5;246m<chr>[39m[23m                                                                   [38;5;250m1[39m ham   [38;5;246m""[39m me parece bien la chapa[38;5;246m""[39m                                              [38;5;250m2[39m ham   [38;5;246m""[39mbuenas  estuve el sabado  dime cuando vas a pasar por la casa  tenemos…[38;5;250m3[39m ham   [38;5;246m""[39mbuenas  ma ana a las   h en alkain [38;5;246m""[39m                                   [38;5;250m4[39m ham   [38;5;246m""[39mbuenas  qtal  he estado en morlans  quieres una reforma semi completa …[38;5;250m5[39m ham   [38;5;246m""[39mbuenas alfonso  acu rdate de mi  ya me diras cuando vas [38;5;246m""[39m              [38;5;250m6[39m ham   [38;5;246m""[39mbuenas alfonso  el grifo del agua se puede abrir    [38;5;246m""[39mhead(test)[38;5;246m# Source: spark<?> [?? x 2][39m  label cleaned                                                                   [3m[38;5;246m<chr>[39m[23m [3m[38;5;246m<chr>[39m[23m                                                                   [38;5;250m1[39m ham   [38;5;246m""[39mbuenas  como van mis ventanas    [38;5;246m""[39m                                     [38;5;250m2[39m ham   [38;5;246m""[39mbuenas  he visto que recoges electrodom sticos  tengo nevera  lavadora…[38;5;250m3[39m ham   [38;5;246m""[39mbuenas  mejor        que voy con leire [38;5;246m""[39m                               [38;5;250m4[39m ham   [38;5;246m""[39mno puedo hablar  ll mame luego [38;5;246m""[39m                                       [38;5;250m5[39m ham   [38;5;246m""[39mok[38;5;246m""[39m                                                                    [38;5;250m6[39m ham   [38;5;246m""[39mok  genial [38;5;246m""[39mTF modelSince machine learning models require inputs as numeric data, the common practice in text analysis thus is to convert each single text into tokens (or pieces) so that these tokens will be the features that can be used to discriminate between class labels, In our case, they are a simple words. Using the TF method, if a particular word exists in a particular document we assign the number of frequency of this word (or just 1 if we do not care about the frequency) in the corresponding cell in the design matrix (which is called Document Term Matrix DTM), otherwise we assign zero.this method will give us a very large and sparse rectangular matrix with huge number of features compared to the number of documents, that is why spark can help to handle this type of data. Due to its popularity, we will fit random forest model, which known as one of the most powerful machine learning models, to the transformed data. to be brief We will make use of the spark feature pipline that helps us to group all the following required steps to enable running the model:convert the dependent variable labels to integer type.tokenize the cleaned messages into words (tokens).remove stop words from the tokens since they tend to spread out randomly among documents.replace each term in each document by its frequency number.define the model that will be used (here random forest model).At the final step we use ml_random_forest function and we keep all the default values, for example, 20 for number of trees, 5 for the max depth, and gini as the impurity function, and do not forget to set the seed to get the result reproducible. lastly we call the ml_fit function to fit the model.pipline<-ml_pipeline(sc)%>%    ft_string_indexer(input_col = ""label"",output_col=""class"")%>%    ft_tokenizer(input_col = ""cleaned"", output_col=""words"")%>%    ft_stop_words_remover(input_col = ""words"",output_col = ""cleaned_words"")%>%    ft_count_vectorizer(input_col = ""cleaned_words"",                        output_col=""terms"",                        min_df=5,                        binary=TRUE)%>%    ft_vector_assembler(input_cols = ""terms"",                        output_col=""features"")%>%    ml_random_forest_classifier(label_col=""class"",                                features_col=""features"",                                seed=222)piplinePipeline (Estimator) with 6 stages<pipeline_a40f182d58ae>   Stages   |--1 StringIndexer (Estimator)  |    <string_indexer_a40f18afeb99>   |     (Parameters -- Column Names)  |      input_col: label  |      output_col: class  |     (Parameters)  |      handle_invalid: error  |      string_order_type: frequencyDesc  |--2 Tokenizer (Transformer)  |    <tokenizer_a40f18f1fc84>   |     (Parameters -- Column Names)  |      input_col: cleaned  |      output_col: words  |--3 StopWordsRemover (Transformer)  |    <stop_words_remover_a40f2e833f77>   |     (Parameters -- Column Names)  |      input_col: words  |      output_col: cleaned_words  |--4 CountVectorizer (Estimator)  |    <count_vectorizer_a40f55801106>   |     (Parameters -- Column Names)  |      input_col: cleaned_words  |      output_col: terms  |     (Parameters)  |      binary: TRUE  |      maxDF: 9223372036854775808  |      min_df: 5  |      min_tf: 1  |      vocab_size: 262144  |--5 VectorAssembler (Transformer)  |    <vector_assembler_a40f3142a26b>   |     (Parameters -- Column Names)  |      input_cols: terms  |      output_col: features  |--6 RandomForestClassifier (Estimator)  |    <random_forest_classifier_a40f39a3d1a3>   |     (Parameters -- Column Names)  |      features_col: features  |      label_col: class  |      prediction_col: prediction  |      probability_col: probability  |      raw_prediction_col: rawPrediction  |     (Parameters)  |      bootstrap: TRUE  |      cache_node_ids: FALSE  |      checkpoint_interval: 10  |      feature_subset_strategy: auto  |      impurity: gini  |      leafCol:   |      max_bins: 32  |      max_depth: 5  |      max_memory_in_mb: 256  |      min_info_gain: 0  |      min_instances_per_node: 1  |      minWeightFractionPerNode: 0  |      num_trees: 20  |      seed: 222  |      subsampling_rate: 1model_rf<-ml_fit(pipline,train)To evaluate our model we use the ml_transfrom function.ml_transform(model_rf,train)%>%  ml_binary_classification_evaluator(label_col = ""class"",                                     metric_name= ""areaUnderROC"")0.963448275862069Notice that in binary classification model sparklyr provides only two metrics areaUnderROC and areaUnderPR (Murphy, 2012). Using the former metric we get high score which is about 0.97.This rate is ranged between 0 and 1, The higher the rate the best the model. However, since this rate is resulted from the training data, it might be the result of an overfitting (Lantz, 2016) problem, that is why the more reliable one is that that resulted from the testing set,which is now 0.963.ml_transform(model_rf,test)%>%    ml_binary_classification_evaluator(label_col = ""class"",                                       metric_name= ""areaUnderROC"")0.935975609756098Fortunately The two rate values are very close to each other indicating the good generalization of our model.To get the prediction we use the ml_predict function .pred<-ml_predict(model_rf,test)As we see some columns are nested. This is not problem since you can extract the elements of this list using the function unlist. For instance, we can show the most used words in each class label using the package wordcloudp1<-pred%>%    filter(label==""ham"")%>%    pull(cleaned_words)%>%    unlist()library(""wordcloud"")wordcloud::wordcloud(p1,max.words = 50, random.order = FALSE,                     colors=c(""blue"",""red"",""green"",""yellow""),random.color = TRUE)Warning message in tm_map.SimpleCorpus(corpus, tm::removePunctuation):“transformation drops documents”Warning message in tm_map.SimpleCorpus(corpus, function(x) tm::removeWords(x, tm::stopwords())):“transformation drops documents”p2<-pred%>%    filter(label==""spam"")%>%    pull(cleaned_words)%>%    unlist()wordcloud::wordcloud(p2,max.words = 50,random.order = FALSE,                      colors=c(""blue"",""red"",""green"",""purple""),random.color = TRUE)Warning message in tm_map.SimpleCorpus(corpus, tm::removePunctuation):“transformation drops documents”Warning message in tm_map.SimpleCorpus(corpus, function(x) tm::removeWords(x, tm::stopwords())):“transformation drops documents”From the upper figure we see that the most common words in hm’s are: get, good, know, whereas the lower figure shows the most ones for spam’s, which are: call, free, mobile. This means that if we receive a new email message that has the word free for instance , it will be more probable to be spam.TF-IDF modelThe main drawback of TF method is that it does not take into account the distribution of each term across the documents that reflects how much information each term provides. To measure the information of each term we compute its DF document frequency value which is the number of documents d where the term t appears, and hence the inverse document frequency IDF value for each pair (d,t) will be computed as follows:[idf(t,d)=log(\frac{N}{1+|d\epsilon D,t\epsilon d|})]Where N is the total number of documents (number of rows).By multiplying TF with IDF we get TF-IDF value for each term. In the above TF pipline we include the function ft_idf , then we fit again random forest model on the transformed data, and we evaluate the model directly by using the test data.pipline2<-ml_pipeline(sc)%>%    ft_string_indexer(input_col = ""label"",output_col=""class"")%>%    ft_tokenizer(input_col = ""cleaned"", output_col=""words"")%>%    ft_stop_words_remover(input_col = ""words"",                          output_col = ""cleaned_words"")%>%    ft_count_vectorizer(input_col = ""cleaned_words"",                         output_col=""tf_terms"")%>%    ft_idf(input_col = ""tf_terms"", output_col=""tfidf_terms"")%>%    ml_random_forest_classifier(label_col=""class"",                                 features_col=""tfidf_terms"",                                seed=222)model_rf.tfidf <- ml_fit(pipline2, train)ml_transform(model_rf.tfidf,test)%>%    ml_binary_classification_evaluator(label_col = ""class"",                                       metric_name= ""areaUnderROC"")0.929878048780488Using this more complex model than the previous one is not justified for this data since their rates are close to each other.Add new featuresCustomizing new features from the data that we think they are more relevant than the old ones is a popular strategy used to improve prediction quality. For example, with our data we think that spam messages tend to be shorter than ham messages, we can, thus, add the messages’ lengths as new features.train1 <- train %>% mutate(lengths=nchar(cleaned))test1 <- test %>% mutate(lengths=nchar(cleaned))Now let’s retrain the above models again with this new added feature.TF modelpipline_tf<-ml_pipeline(sc)%>%    ft_string_indexer(input_col = ""label"",output_col=""class"")%>%    ft_tokenizer(input_col = ""cleaned"", output_col=""words"")%>%    ft_stop_words_remover(input_col = ""words"",                          output_col = ""cleaned_words"")%>%    ft_count_vectorizer(input_col = ""cleaned_words"",                        output_col=""terms"",                        min_df=5,                        binary=TRUE)%>%    ft_vector_assembler(input_cols = c(""terms"",""lengths""),                        output_col=""features"")%>%    ml_random_forest_classifier(label_col=""class"",                                features_col=""features"",                                seed=222)model_rf_new<-ml_fit(pipline_tf,train1)ml_transform(model_rf_new,test1)%>%    ml_binary_classification_evaluator(label_col = ""class"",                                       metric_name= ""areaUnderROC"")0.9375Fortunately, our expectation about this new feature is confirmed since we have got a significant improvement compared to the previous results.tf_idf modelpipline_tfidf<-ml_pipeline(sc)%>%    ft_string_indexer(input_col = ""label"",                      output_col=""class"")%>%    ft_tokenizer(input_col = ""cleaned"",                  output_col=""words"")%>%    ft_stop_words_remover(input_col = ""words"",                          output_col = ""cleaned_words"")%>%    ft_count_vectorizer(input_col = ""cleaned_words"",                         output_col=""tf_terms"")%>%    ft_idf(input_col = ""tf_terms"", output_col=""tfidf_terms"")%>%    ft_vector_assembler(input_cols = c(""tfidf_terms"",""lengths""),                        output_col=""features"")%>%    ml_random_forest_classifier(label_col=""class"",                                features_col=""features"",seed=222)model_rf_new2 <- ml_fit(pipline_tfidf, train1)ml_transform(model_rf_new2,test1)%>%    ml_binary_classification_evaluator(label_col = ""class"",                                       metric_name= ""areaUnderROC"")0.967987804878049Again, as we said before, the use of idf method is not justified, and it would be better to stay with the tf method.n-gram modelIn contrast to the function ft_tokenizer that splits the text into tokens where each token has a single word, each token resulted from the sparklyr function ft_ngram has n words respecting the same appearance order as in the original text. To well understand let’s take the following example.data <- copy_to(sc, data.frame(x=""I like both R and python""), overwrite = TRUE)data[38;5;246m# Source: spark<?> [?? x 1][39m  x                         [3m[38;5;246m<chr>[39m[23m                   [38;5;250m1[39m I like both R and pythonthe ft_tokenizer function gives the following tokens:ft_tokenizer(data, ""x"", ""y"") %>%     mutate(y1=explode(y)) %>%     select(y1)[38;5;246m# Source: spark<?> [?? x 1][39m  y1      [3m[38;5;246m<chr>[39m[23m [38;5;250m1[39m i     [38;5;250m2[39m like  [38;5;250m3[39m both  [38;5;250m4[39m r     [38;5;250m5[39m and   [38;5;250m6[39m pythonWhereas, with ft_ngram, where (n=2) we get the following tokensdata  %>%      ft_tokenizer(""x"", ""y"") %>%     ft_ngram(""y"", ""y1"", n=2) %>%    mutate(z=explode(y1)) %>%     select(z)[38;5;246m# Source: spark<?> [?? x 1][39m  z           [3m[38;5;246m<chr>[39m[23m     [38;5;250m1[39m i like    [38;5;250m2[39m like both [38;5;250m3[39m both r    [38;5;250m4[39m r and     [38;5;250m5[39m and pythonNow let’s train 2_gram Random forest model.pipline_2gram<-ml_pipeline(sc)%>%    ft_string_indexer(input_col = ""label"",                      output_col=""class"")%>%    ft_tokenizer(input_col = ""cleaned"",                  output_col=""words"")%>%    ft_stop_words_remover(input_col = ""words"",                          output_col = ""cleaned_words"")%>%    ft_ngram(input_col = ""cleaned_words"",              output_col=""ngram_words"", n=2) %>%     ft_count_vectorizer(input_col = ""ngram_words"",                         output_col=""tf_terms"")%>%    ft_vector_assembler(input_cols = c(""tf_terms"",""lengths""),                        output_col=""features"")%>%    ml_random_forest_classifier(label_col=""class"",                                features_col=""features"",seed=222)model_rf_2gram <- ml_fit(pipline_2gram, train1)ml_transform(model_rf_2gram,test1)%>%    ml_binary_classification_evaluator(label_col = ""class"",                                        metric_name= ""areaUnderROC"")0.803353658536585You should know that this function takes only tokens with tow words exactly, not tokens with less or equal 2 words. That is why we have obtained a lower rate than the previous models.When you are satisfied by your final model, you can save it for further use as follows.#ml_save(model_rf_ngram,""spark_ngram"",overwrite = TRUE)The last thing to mention, is when you finish your work do not forget to free your resources by disconnecting from spark as followsspark_disconnect(sc)Conclusion:This article is a brief introduction to illustrate how easy to handle and model large data set with the combination of the two powerful languages R and spark. we have used a text data set since this type of data that characterizes the most large datasets encountered in the real world.I hope it will help you to develop your training.Never give up!See you in Linkedin!References:https://spark.rstudio.comhttps://www.r-bloggers.com/predicting-large-text-data-with-spark-via-the-r-package-sparklyr/Written byOscar RojoCurrently studing a Master in Data Science. Passionate about learning new skills. Former branch risk analyst. https://www.linkedin.com/in/oscar-rojo-martin/FollowPublic domain.SparkRPredictSparklyrWordcloudMore from Oscar RojoFollowCurrently studing a Master in Data Science. Passionate about learning new skills. Former branch risk analyst. https://www.linkedin.com/in/oscar-rojo-martin/More From MediumStories from the city, stories from the cloud: an introduction to city open data portals in the…Zoë Wilkinson Saldaña in Messy DataWhat is Principal Component Analysis (PCA) and when should I use it?Jose CachoComprehensive Guide To Optimize Your Pandas CodeEyal Trabelsi in Towards AIHealthcare Analytics: Exploration vs. ConfirmationElder Research, Inc.What Are You Going to Write About for Nightingale?Isaac Levy-Rubinett in NightingaleDescriptive Statistics: Measures of SpreadFrancis Morales in The StartupNovel Coronavirus: Simple analysis and predictionsAlberto Naranjo in The StartupAirflow for Data ScientistsSTATWORX BlogAboutHelpLegalGet the Medium app"
Things you must know before starting any programming language,https://medium.com/@shresthvarshney980/thinks-you-must-know-before-starting-any-programming-language-9b3234acb768?source=tag_archive---------3-----------------------,"Programming,Coding,Python,R,Programming Languages","What’s Programming?Programming is the way to communicate with the computer and give instructions to system that how it has to work. Every application that we use in our system is programmed by using any programming language.Why Programming?Computer works on binary digits i.e., 0 and 1 and it is very tough for human to understand computer language. So human has created a middle path for us to communicate with system easily and the middle path is called as programming. In today’s world when everything is going in momentum with fast growing technologies we need to communicate with our system easily and this target is achieved when our system is highly programmed.How’s Programming?Programming can be done by programming languages. Programming languages are understandable by both human as well as system. They are like a bridge between human language and computer language. Every programming language has its own syntax and permanent keywords via which we can program our system.What’s behind programming?When we write any code in our system in any programming language the system first compile the code by compiler. Compiler is a device which change code language to system language so that it can be understandable by system. There are many types of compiler but here we are not concern about it.How we can do programming on our system?To do programming on our system we have to learn one programming language. There are many types of programming languages like C, C++, C#, Python, Java, R etc.Before starting any language let us first compare the syntax of all programming language with same output “Hello World!”C Syntax:#include<stdio.h>int main(){printf(“Hello World”);return 0;}C++ Syntax:#include<iostream>int main(){std::cout << “Hello World!”;return 0;}C# Syntax:namespace HelloWorld{class Hello{static void Main(string[] args){System.Console.WriteLine(“Hello World!”);}}}Java Syntax:class HelloWorld{public static void main(String[] args){System.out.println(“Hello, World!”);}}Python Syntax:print(“Hello World!”)R Syntax:myString <- “Hello, World!”print ( myString)First Step after deciding any programming language:Firstly we have to decide that which programming language we want to learn and which is best for our carrier. Then instead of going for any paid course we first go for free beginner course of that language and if we are comfortable with that language then only we have to go for any paid courses.Here are some free courses to learn any programming language:Link for free C Course to learn: The Complete C programmingLink for free C++ Course to learn: C++ Tutorial for Complete BeginnersLink for free C# Course to learn: Diploma in C# ProgrammingLink for free Java Course to learn: Java Tutorial for Complete BeginnersLink for free Python Course to learn: Python for BeginnerLink for free R Course to learn: R Basics — R Programming Language IntroductionWritten byShresth VarshneyFollow3 3 3 ProgrammingCodingPythonRProgramming LanguagesMore from Shresth VarshneyFollowMore From MediumMigrating to C++ From Other LanguagesAlain GalvanConditional imports across Flutter and WebAntonello Galipò in Flutter CommunityIntroducing Kubectl Flame: Effortless Profiling on KubernetesEden Federman in The StartupSQL Server Transactions and Isolation LevelsMatt Eland in The StartupWhat Is the Walrus Operator in Python?Jonathan Hsu in Better ProgrammingCxO Abstract: Serverless Applications using AWSTravis Giffin in The StartupHow to Grant Access to Private Files Using Pre-Signed URLs on AWSNasi Jofche in Better ProgrammingPerforming Optical Character Recognition with Python and Pytesseract using AnacondaPranav Manoj in Analytics VidhyaAboutHelpLegalGet the Medium app"
N/A,https://medium.com/@cmbosma/create-new-column-based-on-values-in-another-column-in-r-ea49075f7691?source=tag_archive---------4-----------------------,R,"Create New Column Based on Values in Another Column in RHere is a short post on how to create a new column with the values from one column based on the values in separate column. There are a few situations where this might be useful. One is if you have data in long format with your surveys questions in one column and you want to separate the scores of one of the surveys into a separate column for time-series/longitudinal analyses. Another situation might be that you have several conditions and you want to create a column of scores retaining scores of a subset of the conditions.I recently needed to do this to tidy up some data in long format and noticed that there are no obvious blog posts or stack overflow entries addressing this data tidying scenario. The solutions you will find come with a caveat: if you create a new vector based on values in another vector, you will likely generate a vector with a length that does not match that of the data frame, precluding you from adding it to the data frame.I will provide an abstract example and then a real-life example.Below we have a table with columns varA and varB. We want to make a new column, varC, with values corresponding to rows with ‘c’ in the varA column.varA varB a 12 b 34 c 23 a 34 b 23 c 13Let’s first illustrate the problem. For example, you would think that df$newVar <- df$varB[(varA == ""c"")] would create a new column with values from varB that correspond with varA. The problem is that this approach, and many other approaches, will create a vector the length of the cases that fit the parameters you have for the new vector. In this case, since there are only two rows with “c”, the length of the new vector would be 2.Okay, now let’s work on the solution. We can use the mutate and ifelse functions from the tidyverse to accomplish our task:# Load in the `tidyverse` package for the `dplyr` package tidying functions and piping.library(tidyverse)df <- df %>%     mutate(varC = ifelse(varA == ""b"", varB, NA)What does this code do?From the data frame, create varC using and ifelse statement and assign it to the original data frame.In the ifelse statement, we indicate that if values in varA equal “b”, then keep the corresponding value from column varB, else NA.Here is the table you would get from this R code.varA varB varC a 12 NA b 34 34 c 23 NA a 34 NA b 23 23 c 13 NASince these abstract examples can be a little tricky to apply to our own stuff, here’s an example resembling a real data set, such as an experiential momentary assessment data set. Below is a table with data in long format. We want to make separate a column for negative affect subscale of the Positive and Negative Affect Schedule (PANAS-X) to conduct analyses solely on those scores. Although this can be accomplished with the data in wide format, you may want to keep it in long format.id time_index panas_x_subscale score 001 1 panas_x_sad 23 001 2 pans_x_pa 11 001 3 panas_x_na 6 002 1 panas_x_sad 3 002 2 panas_x_pa 2 002 3 panas_x_na 4 003 1 panas_x_sad 15 003 2 panas_x_pa 10 003 3 panas_x_na 8df <- df %>%    mutate(panas_x_na = ifelse(panas_x_subscale == ""panas_x_na"", score, NA)Below is a data table amended with a vector containing values copied from the score column corresponding with “panas_x_na” in the panas_x_subscale column.id time_index panas_x_subscale score panas_x_na 001 1 panas_x_sad 23 NA 001 2 pans_x_pa 11 NA 001 3 panas_x_na 6 6 002 1 panas_x_sad 3 NA 002 2 panas_x_pa 2 NA 002 3 panas_x_na 4 4 003 1 panas_x_sad 15 NA 003 2 panas_x_pa 10 NA 003 3 panas_x_na 8 8Written byColin M. BosmaFollowRMore from Colin M. BosmaFollowMore From MediumCreate Your Own Coefficient Plot Function in PythonJessica Forrest-Baldini in Analytics VidhyaHypothesis Testing for Inference using a DatasetJoju John Varghese in The StartupExperimenting with PySpark to Match Large Data SourcesCivis Analytics in The Civis JournalDecision Trees ClassifierArman Hussain in The StartupEvaluation Metrics for Classification Models Series — Part 1:Ana Preciado — Data Science To Go in The StartupConstructing a Career in Data Viz: Getting StartedWill Chase in NightingaleDimensions of Museum DataColin Brooks in Whitney DigitalOverview of data distributionsMadalina CiortanAboutHelpLegalGet the Medium app"
N/A,https://medium.com/@cmbosma/create-new-column-based-on-values-in-another-column-in-r-639303ff2694?source=tag_archive---------5-----------------------,R,"Create New Column Based on Values in Another Column in RHere is a short post on how to create a new column with the values from one column based on the values in separate column. There are a few situations where this might be useful. One is if you have data in long format with your surveys questions in one column and you want to separate the scores of one of the surveys into a separate column for time-series/longitudinal analyses. Another situation might be that you have several conditions and you want to create a column of scores retaining scores of a subset of the conditions.I recently needed to do this to tidy up some data in long format and noticed that there are no obvious blog posts or stack overflow entries addressing this data tidying scenario. The solutions you will find come with a caveat: if you create a new vector based on values in another vector, you will likely generate a vector with a length that does not match that of the data frame, precluding you from adding it to the data frame.I will provide an abstract example and then a real-life example.Below we have a table with columns varA and varB. We want to make a new column, varC, with values corresponding to rows with ‘c’ in the varA column.varA varB a 12 b 34 c 23 a 34 b 23 c 13Let’s first illustrate the problem. For example, you would think that df$newVar <- df$varB[(varA == ""c"")] would create a new column with values from varB that correspond with varA. The problem is that this approach, and many other approaches, will create a vector the length of the cases that fit the parameters you have for the new vector. In this case, since there are only two rows with “c”, the length of the new vector would be 2.Okay, now let’s work on the solution. We can use the mutate and ifelse functions from the tidyverse to accomplish our task:# Load in the `tidyverse` package for the `dplyr` package tidying functions and piping.library(tidyverse)df <- df %>%     mutate(varC = ifelse(varA == ""b"", varB, NA)What does this code do?From the data frame, create varC using and ifelse statement and assign it to the original data frame.In the ifelse statement, we indicate that if values in varA equal “b”, then keep the corresponding value from column varB, else NA.Here is the table you would get from this R code.varA varB varC a 12 NA b 34 34 c 23 NA a 34 NA b 23 23 c 13 NASince these abstract examples can be a little tricky to apply to our own stuff, here’s an example resembling a real data set, such as an experiential momentary assessment data set. Below is a table with data in long format. We want to make separate a column for negative affect subscale of the Positive and Negative Affect Schedule (PANAS-X) to conduct analyses solely on those scores. Although this can be accomplished with the data in wide format, you may want to keep it in long format depending on the analyses or data visualization you are interested in conducting.id time_index panas_x_subscale score 001 1 panas_x_sad 23 001 2 pans_x_pa 11 001 3 panas_x_na 6 002 1 panas_x_sad 3 002 2 panas_x_pa 2 002 3 panas_x_na 4 003 1 panas_x_sad 15 003 2 panas_x_pa 10 003 3 panas_x_na 8df <- df %>%    mutate(panas_x_na = ifelse(panas_x_subscale == ""panas_x_na"", score, NA)Below is a data table amended with a vector containing values copied from the score column corresponding with “panas_x_na” in the panas_x_subscale column.id time_index panas_x_subscale score panas_x_na 001 1 panas_x_sad 23 NA 001 2 pans_x_pa 11 NA 001 3 panas_x_na 6 6 002 1 panas_x_sad 3 NA 002 2 panas_x_pa 2 NA 002 3 panas_x_na 4 4 003 1 panas_x_sad 15 NA 003 2 panas_x_pa 10 NA 003 3 panas_x_na 8 8Written byColin M. BosmaFollowRMore from Colin M. BosmaFollowMore From MediumPython Getting Started Tutorial: Scientific Calculation with PandasData Analysis EnthusiastHouse Sales Predictor Using Deep LearningUmair Ayub in The StartupHow I measured your programming work by accessing online data using the REST API(Blog+ Tutorial).Garry Tiscovschi in Data SolsticeFinding Potentially Habitable Exoplanets in the NASA Dataset using PythonAdarsh Bulusu in The StartupHow to Get Financial Ratios Using PythonIbrahim Saidi in Data Driven InvestorConstructing a Career in Data Visualization: The HowWill Chase in Nightingale5 Scraping TipsArtem Rys in python4youDo NBA Superstars Really Get All the Calls?Peter Li in SportsRaidAboutHelpLegalGet the Medium app"
N/A,https://medium.com/@cmbosma/create-new-column-based-on-values-in-another-column-in-r-5232befe3708?source=tag_archive---------6-----------------------,R,"Create New Column Based on Values in Another Column in RHere is a short post on how to create a new column with the values from one column based on the values in separate column. There are a few situations where this might be useful. One is if you have data in long format with your surveys questions in one column and you want to separate the scores of one of the surveys into a separate column for time-series/longitudinal analyses. Another situation might be that you have several conditions and you want to create a column of scores retaining scores of a subset of the conditions.I recently needed to do this to tidy up some data in long format and noticed that there are no obvious blog posts or stack overflow entries addressing this data tidying scenario. The solutions you will find come with a caveat: if you create a new vector based on values in another vector, you will likely generate a vector with a length that does not match that of the data frame, precluding you from adding it to the data frame.I will provide an abstract example and then a real-life example.Below we have a table with columns varA and varB. We want to make a new column, varC, with values corresponding to rows with ‘c’ in the varA column.varA varB a 12 b 34 c 23 a 34 b 23 c 13Let’s first illustrate the problem. For example, you would think that df$newVar <- df$varB[(varA == ""c"")] would create a new column with values from varB that correspond with varA. The problem is that this approach, and many other approaches, will create a vector the length of the cases that fit the parameters you have for the new vector. In this case, since there are only two rows with “c”, the length of the new vector would be 2.Okay, now let’s work on the solution. We can use the mutate and ifelse functions from the tidyverse to accomplish our task:# Load in the `tidyverse` package for the `dplyr` package tidying functions and piping.library(tidyverse)df <- df %>%     mutate(varC = ifelse(varA == ""b"", varB, NA)What does this code do?From the data frame, create varC using and ifelse statement and assign it to the original data frame.In the ifelse statement, we indicate that if values in varA equal “b”, then keep the corresponding value from column varB, else NA.Here is the table you would get from this R code.varA varB varC a 12 NA b 34 34 c 23 NA a 34 NA b 23 23 c 13 NASince these abstract examples can be a little tricky to apply to our own stuff, here’s an example resembling a real data set, such as an experiential momentary assessment data set. Below is a table with data in long format. We want to make separate a column for negative affect subscale of the Positive and Negative Affect Schedule (PANAS-X) to conduct analyses solely on those scores. Although this can be accomplished with the data in wide format, you may want to keep it in long format depending on the analyses or data visualization you are interested in conducting.id time_index panas_x_subscale score 001 1 panas_x_sad 23 001 2 pans_x_pa 11 001 3 panas_x_na 6 002 1 panas_x_sad 3 002 2 panas_x_pa 2 002 3 panas_x_na 4 003 1 panas_x_sad 15 003 2 panas_x_pa 10 003 3 panas_x_na 8df <- df %>%    mutate(panas_x_na = ifelse(panas_x_subscale == ""panas_x_na"", score, NA)Below is a data table amended with a vector containing values copied from the score column corresponding with “panas_x_na” in the panas_x_subscale column.id time_index panas_x_subscale score panas_x_na 001 1 panas_x_sad 23 NA 001 2 pans_x_pa 11 NA 001 3 panas_x_na 6 6 002 1 panas_x_sad 3 NA 002 2 panas_x_pa 2 NA 002 3 panas_x_na 4 4 003 1 panas_x_sad 15 NA 003 2 panas_x_pa 10 NA 003 3 panas_x_na 8 8Written byColin M. BosmaFollowRMore from Colin M. BosmaFollowMore From MediumFrom Data Science to Knowledge ScienceDan McCrearyComputing Histograms Using Java ArraysRishi Sidhu in Data Driven InvestorData Analysis of Bike Sharing Rental using PythonSriyanda Afrida WAssociation rules: Operation of the apriori algorithmMartin Bernaola in Analytics VidhyaSlicing and Dicing with PandasVeena PrabhakaranUsing Regression modeling to predict purchasing habits at StarbucksRavish Chawla in ML 2 VecWeb Scraping with PythonBetaLabPython Deep Learning: Part 1Jon C-137AboutHelpLegalGet the Medium app"
Estatística e Sorvete…,https://medium.com/data-hackers/estat%C3%ADstica-e-sorvete-435438cdd490?source=tag_archive---------0-----------------------,"Estatistica,R,Ciencia De Dados,Data Science","Photo by Irene Kredenets on UnsplashResumoNeste artigo, você aprenderá um pouco sobre cálculos de probabilidade no R Studio. Por se tratar de uma linguagem estatística, o R já vem com vários testes incluídos na base da ferramenta, com funções que podem lhe poupar muito trabalho se você souber como usá-las.Falaremos sobre três deles aqui:Probabilidade para Distribuições BinomiaisProbabilidade para distribuições de PoissonProbabilidade para distribuições normaisAntes de começar …Certo, você leu o resumo, está interessado no tópico, mas ainda não entendeu o que o sorvete tem a ver com tudo isso, certo?Bom, eu só queria trabalhar neste artigo com um conjunto de dados sobre sorvete. Nada mais. Aqui está como você pode criar um pequeno dataset no R Studio:ice_cream <- data.frame(month= c(1,2,3,4,5,6,7,8,9,10,11,12),                        sales= sample(100:500,                                      size=12, replace=T,                                       set.seed(12)),                        customers= sample(50:450,                                          size=12, replace=T,                                           set.seed(12)))| month| sales| customers||-----:|-----:|---------:||     1|   127|        77||     2|   427|       377||     3|   477|       427||     4|   208|       158||     5|   167|       117||     6|   113|        63||     7|   171|       121||     8|   357|       307||     9|   109|        59||    10|   103|        53||    11|   257|       207||    12|   426|       376|Tudo pronto. Vamos la!Distribuição BinomialDistribuições binomiais, como o nome já diz, são aquelas em que podemos obter dois resultados possíveis: Sim / Não, Certo / Errado, Verdadeiro / Falso, Sucesso / Falha.Este teste é útil quando você precisa saber a probabilidade de um evento ocorrer se você tentar ’n’ vezes.Usando nosso exemplo de sorvete, imagine que nossa loja tenha 15 taças predefinidas no menu, mas queríamos nos concentrar na venda de Sundae. Se quiséssemos saber qual é a probabilidade de uma pessoa entrar na loja e escolher um Sundae entre todas as outras 15 opções, cairíamos em um problema clássico de estatística: a chance é de 1/15 (6,67%), certo?Mas sabendo que temos mais de um cliente por dia, qual seria a probabilidade de 5 clientes escolherem um Sundae a cada 30 vendas? Bem, agora nosso problema poderia ser um pouco mais complicado de calcular, mas não é, pois usaremos o teste Binomial no R Studio.O teste Binomial é muito simples de executar. Você pode usar a função a seguir, onde o primeiro parâmetro será o número de sucessos que você está medindo (x); o size aqui é o número de vezes que o evento vai acontecer, o número de tentativas (não deve ser confundido com o tamanho da amostra); e a probabilidade de sucesso que você tem.dbinom(x= número de sucessos,       size = número de eventos ou tentativas,       prob = probabilidade de sucesso)Então, resumindo:Problema 1: Qual é a probabilidade de 5 em 30 clientes escolherem Sundae no menu?Método: Teste binomial = Escolha Sundae ou NÃO Sundae.Probabilidade de sucesso: escolha 1 entre 15 opções de menu.Quantidade de eventos: 30 clientes = 30 transações de vendas.Teste de sucesso: 5 pessoas escolheram sundae no menu.dbinom(x= 5, size= 30, prob= 1/15)[1] 0.03  # 3% de chance.E tem mais. Se quisermos testar a probabilidade acumulada de 5 ou mais pessoas de escolher um Sundae (5, 6, 7,… .30), há uma função para isso também. Podemos usar pbinom, que é bastante semelhante ao dbinom, mas nos traz o parâmetro lower.tail, usado como TRUE quando você deseja verificar um determinado número de sucessos ou menos (q inclusivo) e como FALSE quando você deseja mais de um certo números de sucessos (q exclusivo).# Informação: 5 pessoas ou mais, 30 vendas, prob 1/15pbinom(q=4, size= 30, prob= 1/15, lower.tail= F)[1] 0.0464 # 4.6% de chance.Nota: eu sei que você provavelmente está pensando agora “Mas a escolha de um produto por um cliente é muito mais complexa do que um simples teste estatístico”. E realmente é. Envolve preço, promoção, valor, ponto de vendas e muitas outras coisas. Mas a ideia neste artigo é apenas mostrar como realizar os testes e tê-los como uma nova ferramenta para sua análise.Distribuição de PoissonA Distribuição de Poisson (descoberta por Siméon Denis Poisson) está relacionada a eventos em um período de tempo.Você usa a distribuição de Poisson quando deseja saber qual é a chance de algo acontecer ’n’ vezes durante um período de tempo.Para usar esse teste, você pode digitar dpois no R Studio. No entanto, você precisará ter as seguintes informações para continuar:dpois(x= número a ser testado,      lambda = taxa média em que o evento ocorre)Mais uma vez, trazendo para o nosso doce exemplo do sorvete, no conjunto de dados apresentado no início deste artigo, vemos as colunas mês, vendas e clientes. Portanto, sabemos que nosso período de tempo é de um mês. E se chamarmos a função summary em nossa coluna de vendas, teremos a taxa média de vendas por um mês, correto?summary(ice_cream$sales)  Min.   1st Qu.  Median  Mean    3rd Qu.    Max.   103.0   123.5   189.5   245.2   374.2     477.0Nosso lambda é, portanto, 245.2 vendas por mês. Agora só precisamos saber o que queremos testar.Quero aumentar em 5% minha média de vendas. Qual a probabilidade de isso acontecer, apenas ao acaso?Problema 2: Aumente a média de vendas / mês em 5%, para aprox. 257Método: teste de Poisson = 12 vendas a mais por mêsMédia atual: 245.2 (lambda)dpois(x= 257, lambda)[1] 0.0188  # 1.8% de chanceÉ.. melhor eu começar a trabalhar mais em ações de marketing, certo? Porque se eu deixar tudo ao acaso, terei que contar com minúsculos 1,8% de probabilidade de que os clientes comecem a aparecer na minha loja e comprar mais.Da mesma forma que os outros testes de distribuição, o Poisson também traz o ppois que calcula a probabilidade acumulada. A diferença é apenas o nome da função começando com a letra p a inclusão do parâmetro lower.tail.Agora vou calcular a chance acumulada de aumentar minhas vendas em qualquer número entre 1% e 5%.# Calculando a prob acumulada de 5% de aumento ou menos e subtraindo a prob. de 1% ou menos. Dessa forma, obtenho apenas o intervalo exato entre 1% e 5%, nada acima ou abaixo dele.ppois(257, lambda = 245.2) - ppois(247, lambda = 245.2)[1] 0.2226  # 22% de chance!Lembre-se de que essa é a soma das chances. Portanto, aumentar 5% contém a soma das chances de aumentar 1% + 2% + 3% + 4% + 5% ou quaisquer casas decimais entre eles. Dessa forma, você deve ter muito cuidado ao plotar e ler um gráfico como o mostrado abaixo. Veja que ele mostra 56% de chance de aumentar 1% de nossas vendas. Tá brincando?! O que isso significa?Onde a imagem mostra 247 (ou aumento de aproximadamente 1%), estamos na verdade calculando a probabilidade acumulada (somando probabilidades) das vendas irem da média de 245.2 para qualquer número até 247 — ou seja, variar entre 0 a 1%. Mesmo uma pequena alteração como 245.2 para 245.21 é considerada. Deste modo, olhando para a primeira barra, não é correto dizer que você ficará sentado o dia todo e é 56% provável que suas vendas aumentem 1%.É 56% provável que suas vendas aumentem para qualquer valor entre 245.2 e 247 se você continuar fazendo o que faz. Pode aumentar 0.1% ou 0.45% ou 0.87%… Existe 58% de chance do número se mover dentro da faixa de 245.2 a 248 e por aí vai.Portanto, tenha cuidado ao interpretar este gráfico!Distribuição acumulada Poisson para probabilidade de aumento de vendas de sorvetes ao acaso.Distribuição normalFinalmente, a distribuição normal é o tipo mais comum que existe. Muitos conceitos e teorias estatísticas baseiam-se nesta distribuição.A distribuição normal é a famosa ‘curva em forma de sino’ onde os dados são distribuídos em torno da média. Se você plotar os valores em um gráfico, a média será o centro da curva.Conhecer as qualidades dessa curva nos permite fazer muitas suposições sobre os dados que são normalmente distribuídos e calcular as probabilidades para muitas coisas de nossa vida diária. Extrair uma amostra de uma população e usar as estatísticas dessa amostra para entender o todo é uma das vantagens surpreendentes da distribuição normal.Acredito que a melhor analogia que conheço para isso é com a comida. Quando você está explorando um novo alimento ou sabor, geralmente você não vai lá e dá aquela baita mordida. Primeiro você pega um pedacinho e experimenta para saber o sabor. Isso porque você assume que o todo terá o mesmo sabor daquele pedacinho. O mesmo é verdadeiro para distribuições normais e você pode aprender mais sobre isso pesquisando sobre o Teorema do Limite Central.Além disso, a área sob a curva em forma de sino terá 100% dos valores. Como as Distribuições Normais estão centradas em sua média, é correto dizer que 50% será maior e 50% será menor que a média, assim como a maior parte dos valores está concentrada em torno da média. Se você calcular quanto os valores podem estar distantes do centro e depois dividir a curva em 6 partes iguais chamadas de desvio padrão — 3 abaixo da média e 3 acima da média -, cada unidade de desvio padrão adicionada conterá mais valores que explicam os seus dados.Agora fica mais fácil dividir a amostra em intervalos de probabilidade. Isso é conhecido como a regra 68–95–99. Continue comigo: olhando para a curva normal abaixo, fica fácil ver que minha média é o centro, os valores estão a 3 pontos do centro e sabemos que a área sob a curva compreende 100% dos valores da minha amostra. Portanto, se eu pegar um desvio padrão para mais e um para menos do que a média, terei cerca de 68% dos valores de um determinado atributo. Se eu pegar dois desvios padrão, terei 95% dos valores. E três me dá 99% dos valores. E isso explica o intervalo de confiança que você deve ter ouvido muitas vezes, principalmente em períodos eleitorais. Saiba mais neste ótimo vídeo do Simple Learning Pro.O quanto você pode explicar usando 1, 2 ou 3 desvios padrão: regra 68–95–99.Seguindo adiante e trazendo o problema para o último exemplo de sorvete. Vamos pegar o mês de março para nosso teste. Aqui está a distribuição das 427 vendas daquele mês.Distribuição normal das vendas. Média = 14, desvio padrão = 1427 vendas em 30 dias nos dá aproximadamente 14 vendas por dia. O desvio padrão é 1 (por exemplo, poderia ter acontecido 13 ou 15 vendas em vez disso).Queremos saber a probabilidade de termos 16 vendas em um dia. Estou multiplicando a operação por 100, então já vemos a porcentagem final. Logo, vemos 5% de chance de ter 16 vendas em um dia. Isso cai para apenas 0,44% se testarmos 17 e 0,01% para o teste de 18 vendas.# 16 vendas em um dia dnorm(16, mean=14, sd=1)*100[1] 5.399097# 17 vendas em um diadnorm(17, mean=14, sd=1)*100[1] 0.4431848# 18 vendas em um diadnorm(18, mean=14, sd=1)*100[1] 0.01338302ConclusãoOs testes estatísticos são muito úteis para negócios e ciência de dados se soubermos como aplicá-los.Devemos ter cuidado ao mostrar os números e as probabilidades aos tomadores de decisão, uma vez que podem ser facilmente mal interpretados. Certifique-se de sempre incluir explicações detalhadas para cada probabilidade e gráficos.É fácil cometer um erro e colocar a culpa em estatísticas “ruins”. Mas o problema não está nos números, o problema está nas pessoas que interpretam esses números.if data:   data.science()GusData HackersBlog oficial da comunidade Data HackersFollow7 EstatisticaRCiencia De DadosData Science7 claps7 clapsWritten byGustavo SantosFollowFinancial Data Analyst | Data Scientist. I extract insights from data to help people and companies to make better and data driven decisions.FollowData HackersFollowBlog oficial da comunidade Data HackersFollowWritten byGustavo SantosFollowFinancial Data Analyst | Data Scientist. I extract insights from data to help people and companies to make better and data driven decisions.Data HackersFollowBlog oficial da comunidade Data HackersMore From MediumSete princípios para pipelines de dados confiáveisRicardo Pinto in Data HackersTomada de decisão com Teste A/BGuilherme Reis Mendes in Data HackersData Visualization: Uma Arte ObjetivaLauro Oliveira in Data HackersComo categorizar textos usando o LDAGustavo Santos in Data HackersDados, Design e UX Research — Data Hackers Podcast 31Paulo Vasconcellos in Data HackersGradientes Descendentes na prática — melhor jeito de entenderArthur Lamblet Vaz in Data HackersA maneira eficiente de filtrar um data frame — PandasArthur Lamblet Vaz in Data HackersEu li 3942 textos da New Order! Parte02 — Topic ModellingGiovani Ferreira in Data HackersLearn more.Medium is an open platform where 170 million readers come to find insightful and dynamic thinking."
Let it shiny: Um sorteador (útil de verdade),https://medium.com/rladiesbh/let-it-shiny-um-sorteador-%C3%BAtil-de-verdade-1bbb5e8b5b25?source=tag_archive---------1-----------------------,"R,Rladies,R Shiny","Dentre as muitas coisas que aprendi quando comecei a trabalhar com a Inteligência para produtos digitais é focar na resolução de um problema real do cliente final. É a dor a ser resolvida. Por mais óbvio que pareça, o processo de descobrir essa demanda e de ater-se a ela com foco no seu processo de desenvolvimento é super desafiador.Vi que em diversas comunidades, inclusive na minha favorita, costumam existir (com a benção de patrocinadores) sorteios. E que nesses momentos tão esperados, é comum usar sorteadores que não tem o melhor fit com o problema. Muitas vezes os sorteios ideais querem excluir pessoas já sorteadas para dar chance de mais participantes ganharem algo de um modo incremental mas com alguns sorteios especiais que todes podem participar.Adaptando a sabedoria popular: se a sua comunidade quiser um sorteio… Faça um sorteador (em R, claro!)Se você quiser usar o sorteador que o Rladies Belo Horizonte vai adotar para os seus sorteios (dedos cruzados para que sejam muitos!) basta acessar: https://larissasayurifcs.shinyapps.io/rladiesBHsampler .A solução descrita nesse post é a versão basicona do sorteador:Input: uma lista com os nomes das pessoas a serem sorteadas e o tamanho da amostra.Output: a lista de pessoas sorteadas sendo o sorteio aleatório simples sem reposição (cada vez que aperta-se o botão run um novo sorteio é feito excluindo os indivíduos amostrados anteiormente).Feito com o pacote shiny que permite interação do usuário e pacote dplyr para manipulação de dados.Sem os detalhes escritos em HTML.Com os códigos descritos aqui fazemos o shiny app que permite um sorteio tipo esse:Suponha que você como organizador do sorteio tem uma lista de 300 participantes da comunidade, descritos em uma planilha (esses nomes eu tirei do pacote babynames):A ideia é que você copie a lista de nomes e cole no shiny app do sorteador, caixa a esquerda da tela:Suponha que são 05 premiações diferentes e que vamos começar com o sorteio de 05 camisetas do Rladies BH:Em seguida, sorteiam-se 04 canecas do Rladies BH (apenas para quem não ganhou a camiseta):Depois, sorteiam-se 03 ingressos para trilhas do TDC online (apenas para quem não ganhou a camiseta ou a caneca):Segue o sorteio de 02 ingressos para trilhas do She’s tech (apenas para quem não foi sorteado até esse momento):E existe um sorteio ultra especial: Um ingresso com passagem e hospedagem para o próximo rstudio::Conf. Nesse todos os participantes podem concorrer de modo que o sorteador é reiniciado:Para permitir que o sorteador fosse usado pela maior quantidade de usuários diferentes possível tentei conceber a experiência de uso mais simples.Como eu queria uma tela com as funcionalidades do R rodando “por trás” eu optei por usar o pacote shiny. Para os que não são familiariados com ele eu sugiro fortemente assistir os vídeos em: https://shiny.rstudio.com/tutorial/.Comecei criando três scripts básicos de Shiny app (nomeados exatamente como ui.R, server.R e global.R e todos no mesmo diretório do projeto ):ui.R: destinado ao layout do app, para coletar dados que servem como input e como mostrar os objetos output para o usuário começando com:ui <- fluidPage(  )server.R: onde ficam os trechos de código destinados a manipulação de dados input para gerar os objetos output começando com:server <- function(input, output){  }global.R: script auxiliar que para o contexto desse sorteador, vai ter uma lista inicial de indivíduos a serem sorteados. Nas vezes que o usuário colar uma lista própria de indivíduos a serem sorteados o objeto lista desse script não será usado. Mas ainda assim ele será necessário para carregar o pacote dplyr:library(dplyr)lista <- paste(c(""Layla Comparin"",                 ""Numiá Gomes"",                 ""Ana Carolina Dias"",                 ""Larissa Sayuri Santos"",                 ""Numiá Comparin"",                 ""Ana Carolina Comparin"",                 ""Larissa Comparin"",                 ""Layla Gomes"",                 ""Ana Carolina Gomes"",                 ""Larissa Sayuri Gomes"",                 ""Layla Dias"",                 ""Numiá Dias"",                 ""Larissa Sayuri Dias"",                 ""Layla Santos"",                 ""Numiá Santos"",                 ""Ana Carolina Santos""                 ), collapse = ""\n"")A interface com o usuário (“as the top of the iceberg”)Começando pela parte visível do app:No script ui.R eu dividi a tela em duas partes de mesmo tamanho. A tela toda de um app shiny tem 12 como unidade de medida. Logo, é como se eu tivesse criado duas colunas, cada uma com 06 unidades de medida. Acrescentei a função wellPanel que cria um painel com borda e fundo cinza claro só para deixar mais bonitinho.ui <- fluidPage(  column(width = 6,         wellPanel(                    )  ),  column(width = 6,         wellPanel(                    )  ))A metade à esquerda da tela foi pensada para que o usuário cole a lista de indivíduos para amostrar.A metade à direita foi pensada para ter alguns botões de input na parte superior e a tabela com a relação dos indivíduos amostrados.No script ui.R vamos sub-dividir a tela da direita. Apesar de usar a função column estou criando linhas:A 1ª e 3ª linhas terão toda a extensão horizontal da coluna a direita (width = 12) para receber o botão do tamanho amostral e a tabela com os indivíduos amostrados. A 2ª linha ficará no meio da coluna direita, com dois botões: (01) que usa a função de amostragem sem reposição no indivíduos disponíveis para amostragem e (02) que reinicia o processo considerando todos os indivíduos como disponíveis para amostragem.ui <- fluidPage(  column(width = 6,         wellPanel(                    )        ),  column(width = 6,         wellPanel(           column(width = 12,                   # Sample size button           ),           column(width = 4, offset = 4,                   #  Run or Restart button           ),           column(width = 12,                  # Table output             )          )  ))Cola os nomes dos indivíduos e ‘printa’No script ui.R vamos colocar na coluna da esquerda uma caixa para receber texto como input usando a função textAreaInput.No script ui.R vamos colocar na coluna da direita uma tabela com o resultado das manipulações e instruções dadas no server.R usando a função tableOutput.ui <- fluidPage(  column(width = 6,         wellPanel(           textAreaInput(inputId = ""list"",                          label = ""Enter your list"",                           height = ""80vh"",                          value = lista)         )        ),  column(width = 6,         wellPanel(           column(width = 12,                   # Sample size button           ),           column(width = 4, offset = 4,                   #  Run or Restart button           ),           column(width = 12,                  # Table output                    tableOutput(""sample"")           )          )  ))No script server.R vou criar um objeto chamado dataInput que corresponderá à lista de nomes de indivíduos a serem amostrados associado a um id sequencial.dataInput <- eventReactive(input$list, {    input_names <- input$list    input_names_split <- strsplit(input_names, ""\n"")        if(length(input_names_split) > 0){      df <- as.data.frame(x = input_names_split)      names(df) <- ""name""      df <- df %>% mutate(id = seq(nrow(df))) %>%        select(id, name)    }  })Note que o objeto dataInput será atualizado a toda alteração feita em input$list. Em geral, shiny apps são concebidos de modo que as funcionalidades descritas em server.R são acionadas a toda e qualquer alterações de inputs (dizemos que são reativos, de reactive). A função eventReactive ‘isola’ a obtenção do dataInput para que dependa apenas das alterações feitas no input$list.Incluí também uma condição lógica de modo que manipula-se o dado para chegar no tibble/data.frame inicial apenas se o usuário tiver colado uma lista de caracteres efetivamente (ou se estivermos usando o default do app, a lista de global.R).Também no script server.R vou criar o objeto que o usuário vai ver, o output$sample, obtido com a função renderTable. Para começar, ele vai retornar as primeiras linhas da tabela (função head()) com os indivíduos a serem amostrados.output$sample <- renderTable({    data <- dataInput()    if(nrow(data) > 0) {      return(head(data))      } else {      return(NULL)    }  })Em resumo, o script server.R fica assim, por enquanto:server <- function(input, output){    dataInput <- eventReactive(input$list, {    input_names <- input$list    input_names_split <- strsplit(input_names, ""\n"")        if(length(input_names_split) > 0){      df <- as.data.frame(x = input_names_split)      names(df) <- ""name""      df <- df %>% mutate(id = seq(nrow(df))) %>%        select(id, name)    }  })    output$sample <- renderTable({    data <- dataInput()    if(nrow(data) > 0) {      return(head(data))      } else {      return(NULL)    }  })  }Clicando em Runapp vemos o app desse jeito:Criando botões (tamanho da amostra e Run) e amostrando efetivamenteNo script ui.R vamos criar os inputs sample_size e run. Os nomes são auto-explicativos. Só vou reforçar que o shiny app a princípio é atualizado a cada alteração de qualquer um dos parâmetros input (input$list e input$sample_size). Como nos passos futuros a amostragem será sem reposição eu optei por criar o botão run que acionará o processo de amostragem. Usando o actionButton a ideia é que o server.R só será acionado com a intervenção explícita do usuário.ui <- fluidPage(  column(width = 6,         wellPanel(           textAreaInput(inputId = ""list"",                          label = ""Enter your list"",                           height = ""80vh"",                          value = lista)         )        ),  column(width = 6,         wellPanel(           column(width = 12,                   # Sample size button                  numericInput(inputId = ""sample_size"",                                label = ""Sample size"",                                value = 3,                               min = 1,                               max = 1000000,                                width = ""33%"")           ),           column(width = 4, offset = 4,                   #  Run or Restart button                  actionButton(inputId = ""run"",                                label = ""Run!"",                                icon(""play-circle-o""))           ),           column(width = 12,                  # Table output                    tableOutput(""sample"")           )          )  ))Para fazer a amostragem propriamente dita vamos adicionar duas funções no script server.R: reactiveValues e observeEvent. Nesse post estamos construindo o shiny app juntes então pode parecer pouco intuitivo mas é o uso das duas funções (com eventReactive e renderTable já apresentadas) que flexibilizam o código para permitir a amostragem sem reposição.Em linhas gerais vamos:1. Criar um objeto no shiny para guardar os ids dos indivíduos amostrados com a função reactiveValues. 2. Quando apertarmos o botão run uma nova amostra será coletada (se possível) e o objeto que guarda os ids dos indivíduos amostrados será ATUALIZADO com a coleção mais recente de ids. Essa atualização acontece com a função observeEvent.No começo do script server.R eu adicionei:all_ids_sampled <- reactiveValues(ids = c())É como se eu estivesse criando um objeto global no shiny app chamado all_ids_sampled, com uma entrada chamada ids e inicializada com vazio.Abaixo do chunk para leitura de dados (objeto dataInput) adicionei o chunk do observeEvent:observeEvent(input$run, {    data <- dataInput()        ids_sampled <- sample(x = data$id,                          size = input$sample_size,                          replace = FALSE)        all_ids_sampled[[""ids""]] <- ids_sampled  })Note que:1. esse é um chunk que será executado se o botão run for acionado. 2. observeEvent é uma função que não produz output, ou seja, não pode ser usada para atribuir uma quantidade a uma variável no shiny app. 3. Vamos usar o observeEvent para atualizar a lista com os ids amostrados, a última linha do chunk. Logo, no observeEvent estamos atualizando o objeto global do shiny app criado com a função reactiveValues no passo anterior.Umas dicas do fundo do coração de quem apanhou para entender:Eu associo o observeEvent ao escopo local de uma função enquanto o eventReactive corresponderia a uma função no escopo global do shiny app.Quando é útil usar o observeEvent? Quando a atividade a ser feita é de atualização. Um uso comum dessa função pode ser no script ui.R quando um input está condicionado ao outro. Por exemplo: se o usuário precisa entrar com o estado e cidade de residência o app pode pedir primeiro o estado e dada a resposta do usuário apresentar apenas os municípios pertencentes ao estado reportado.Ao atualizar o output final incluindo a relação de ids amostrados no chunk de outputTable o script server.R fica:server <- function(input, output){    all_ids_sampled <- reactiveValues(ids = c())      dataInput <- eventReactive(input$list, {    input_names <- input$list    input_names_split <- strsplit(input_names, ""\n"")        if(length(input_names_split) > 0){      df <- as.data.frame(x = input_names_split)      names(df) <- ""name""      df <- df %>% mutate(id = seq(nrow(df))) %>%        select(id, name)    }  })      observeEvent(input$run, {    data <- dataInput()        ids_sampled <- sample(x = data$id,                          size = input$sample_size,                          replace = FALSE)        all_ids_sampled[[""ids""]] <- ids_sampled  })      output$sample <- renderTable({    data <- dataInput()    sampled_ids <- all_ids_sampled[[""ids""]]        if(length(sampled_ids) > 0) {      names_sampled <- data[sampled_ids, ]      return(names_sampled)      } else {      return(NULL)    }  })  }Execute o app nessa etapa! O meu tá do jeitinho abaixo, com um sorteio de tamanho 3.Alterando o código para uma amostragem sem reposiçãoAté esse momento a amostragem toma os nomes dos invidíduos e o tamanho de amostra como input. Queremos que as pessoas sorteadas para ganhar camisetas, por exemplo, saiam da lista dos indivíduos disponíveis para fazer um novo sorteio de canecas. Basicamente nesse passo vamos trabalhar para incrementar o objeto all_ids_sampled. Essa é uma mudança no script server.R mais especificamente no chunk observeEvent, aquele que atualiza o objeto all_ids_sampled.observeEvent(input$run, {    already_sampled <- all_ids_sampled[[""ids""]]        if(is.null(already_sampled)){      available <- dataInput()    }else{      available <- dataInput() %>%        filter(!(id %in% already_sampled))    }        ids_sampled <- sample(x = available$id,                          size = input$sample_size,                          replace = FALSE)        all_ids_sampled[[""ids""]] <- c(all_ids_sampled[[""ids""]], ids_sampled)      })Note que:1. No escopo local de observeEvent cria-se um objeto already_sampled que vai receber a lista de ids que já foram amostrados e, portanto, estão no objeto all_ids_sampled, output da função reactiveValues.2. Criamos um objeto com os indivíduos disponíveis para serem amostrados, chamado de available, nas linhas do if/else. No contexto desse sorteador, already_sampled e available são conjuntos complementares.3. A amostragem é feita com a função sample para o conjunto de ids do objeto available, com tamanho especificado pelo usuário e sem reposição.4. Ao término do amostragem o objeto all_ids_sampled é atualizado de modo a receber o conjunto de ids que acabou de ser sorteado (ids_sampled).E fica assim:Note como a lista de pessoas sorteadas vai aumentando. Mas temos um problema! Olha esse sorteio simplificado (10 nomes e tamanho 5) abaixo:Com 10 pessoas é possível fazer dois sorteios de 05 pessoas sem reposição. Depois dos dois primeiros sorteios… Na ausência de pessoas disponíveis para mais um sorteio o shiny app “quebra” aparecendo sombreado para o usuário. Vamos criar caixas de diálogo para o usuário de modo a orientá-lo melhor.Comunicando que o tamanho de amostra é maior que o número de pessoas disponíveis e/ou que todos foram sorteadosNo script server.R, chunk observeEvent, vamos adicionar testes lógicos com relação a quantidade de ids disponíveis para sorteio (objeto available):Se NÃO há elementos disponíveis para amostragem (length(available$id) == 0)Vamos usar as funções showModal e modalDialog para comunicar ao usuário que todas as pessoas cujos nomes foram colados na tela da esquerda já foram sorteadas (estão na tela da direita).showModal(modalDialog(        title = ""Stop!"",        ""There are no available elements to be sampled!"",         footer = modalButton(label = ""Ok! I'm done!""),        easyClose = TRUE      ))2. Caso contrário (length(available$id) != 0):2.1) Se o tamanho da amostra é MAIOR ou IGUAL que o número de pessoas disponíveis para sorteio (tem mais brinde que gente que ganhou nada)Todas as pessoas disponíveis para sorteios serão obrigatoriamente sorteadas.Na verdade, vai sobrar brinde! O que possivelmente implica o recomeço dos sorteios, com todo mundo apto a ser amostrado.Parachamar a atenção do organizador para isso vamos usar as funções showModal e modalDialog para comunicar o ocorrido.if(input$sample_size >= length(available$id)) {                ids_sampled <- available$id        all_ids_sampled[[""ids""]] <- c(all_ids_sampled[[""ids""]], ids_sampled)                showModal(modalDialog(          title = ""Attention!"",          ""There were fewer elements than the desired sample size. You have sampled everyone."",           footer = modalButton(label = ""Ok""),          easyClose = TRUE        ))              }2.2) Caso contrário (tem mais gente pé frio do que brindes — vida real)Nesse caso, o tamanho da amostra é MENOR que o número de pessoas disponíveis para sorteio. Ou seja, é o caso mais comum dos sorteios… sendo que o trecho de código não vai ter mensagem para o organizador:ids_sampled <- sample(x = available$id,                              size = input$sample_size,                              replace = FALSE)                all_ids_sampled[[""ids""]] <- c(all_ids_sampled[[""ids""]], ids_sampled)A essa altura o script server.R está beeem mais complexo:server <- function(input, output){    all_ids_sampled <- reactiveValues(ids = c())      dataInput <- eventReactive(input$list, {    input_names <- input$list    input_names_split <- strsplit(input_names, ""\n"")        if(length(input_names_split) > 0){      df <- as.data.frame(x = input_names_split)      names(df) <- ""name""      df <- df %>% mutate(id = seq(nrow(df))) %>%        select(id, name)    }  })      observeEvent(input$run, {    already_sampled <- all_ids_sampled[[""ids""]]        if(is.null(already_sampled)){      available <- dataInput()    }else{      available <- dataInput() %>%        filter(!(id %in% already_sampled))    }            if(length(available$id) == 0){            showModal(modalDialog(        title = ""Stop!"",        ""There are no available elements to be sampled!"",         footer = modalButton(label = ""Ok! I'm done!""),        easyClose = TRUE      ))          }else{      if(input$sample_size >= length(available$id)) {                ids_sampled <- available$id        all_ids_sampled[[""ids""]] <- c(all_ids_sampled[[""ids""]], ids_sampled)                showModal(modalDialog(          title = ""Attention!"",          ""There were fewer elements than the desired sample size. You have sampled everyone."",           footer = modalButton(label = ""Ok""),          easyClose = TRUE        ))              } else {        ids_sampled <- sample(x = available$id,                              size = input$sample_size,                              replace = FALSE)                all_ids_sampled[[""ids""]] <- c(all_ids_sampled[[""ids""]], ids_sampled)      }    }      })      output$sample <- renderTable({    data <- dataInput()    sampled_ids <- all_ids_sampled[[""ids""]]        if(length(sampled_ids) > 0) {      names_sampled <- data[sampled_ids, ]      return(names_sampled)      } else {      return(NULL)    }  })  }Olha como essa nova implementação ficou, considerando que são 11 pessoas e amostra de tamanho 5 nos gifs abaixo. Fazendo dois sorteios aleatórios, 10 das 11 pessoas iniciais foram sorteadas. Com atenção você notará que só falta a pessoa 8 na lista:Seguindo com o sorteio (o gif compreende todas as etapas) note como no terceiro “Run!” aparece a mensagem (tela sombreada) que deixa claro para o usuário que a amostra basicamente retornou todo mundo que não tinha sido sorteado. No último frame podemos ver que a pessoa 08 aparece na lista de indivíduos sorteados (última linha).Se a gente continuasse o sorteio acima clicando em Run mais uma vez apareceria, por fim a seguinte tela:Adicionando um botão para recomeçar os sorteiosÉ possível que o organizador tenha interesse em recomeçar o sorteio. Vamos adicionar o botão restart, abaixo do botão run no script ui.R:column(width = 4, offset = 4,                   #  Run or Restart button                  actionButton(inputId = ""run"",                                label = ""Run!"",                                icon(""play-circle-o"")),                  actionButton(inputId = ""restart"",                                label = ""Restart!"",                                icon(""play-circle-o""))           )E, claro, atualizar o server.R adicionando um trecho de código com a função observeEvent de modo que o objeto all_ids_sampled é apagado/ reinicializado se o usuário aperta no botão restart (input$restart):  observeEvent(input$restart, {    all_ids_sampled[[""ids""]] <- c()  })A essa altura o script ui.R está assim:ui <- fluidPage(  column(width = 6,         wellPanel(           textAreaInput(inputId = ""list"",                          label = ""Enter your list"",                           height = ""80vh"",                          value = lista)         )        ),  column(width = 6,         wellPanel(           column(width = 12,                   # Sample size button                  numericInput(inputId = ""sample_size"",                                label = ""Sample size"",                                value = 5,                               min = 1,                               max = 1000000,                                width = ""33%"")           ),           column(width = 4, offset = 4,                   #  Run or Restart button                  actionButton(inputId = ""run"",                                label = ""Run!"",                                icon(""play-circle-o"")),                  actionButton(inputId = ""restart"",                                label = ""Restart!"",                                icon(""play-circle-o""))           ),           column(width = 12,                  # Table output                    tableOutput(""sample"")           )          )  ))E o script server.R (grandão):server <- function(input, output){    all_ids_sampled <- reactiveValues(ids = c())      dataInput <- eventReactive(input$list, {    input_names <- input$list    input_names_split <- strsplit(input_names, ""\n"")        if(length(input_names_split) > 0){      df <- as.data.frame(x = input_names_split)      names(df) <- ""name""      df <- df %>% mutate(id = seq(nrow(df))) %>%        select(id, name)    }  })      observeEvent(input$run, {    already_sampled <- all_ids_sampled[[""ids""]]        if(is.null(already_sampled)){      available <- dataInput()    }else{      available <- dataInput() %>%        filter(!(id %in% already_sampled))    }            if(length(available$id) == 0){            showModal(modalDialog(        title = ""Stop!"",        ""There are no available elements to be sampled!"",         footer = modalButton(label = ""Ok! I'm done!""),        easyClose = TRUE      ))          }else{      if(input$sample_size >= length(available$id)) {                ids_sampled <- available$id        all_ids_sampled[[""ids""]] <- c(all_ids_sampled[[""ids""]], ids_sampled)                showModal(modalDialog(          title = ""Attention!"",          ""There were fewer elements than the desired sample size. You have sampled everyone."",           footer = modalButton(label = ""Ok""),          easyClose = TRUE        ))              } else {        ids_sampled <- sample(x = available$id,                              size = input$sample_size,                              replace = FALSE)                all_ids_sampled[[""ids""]] <- c(all_ids_sampled[[""ids""]], ids_sampled)      }    }      })      observeEvent(input$restart, {    all_ids_sampled[[""ids""]] <- c()  })      output$sample <- renderTable({    data <- dataInput()    sampled_ids <- all_ids_sampled[[""ids""]]        if(length(sampled_ids) > 0) {      names_sampled <- data[sampled_ids, ]      return(names_sampled)      } else {      return(NULL)    }  })  }Tadam! Chegamos então em um shiny app que faz a seguinte sequência de sorteios:Como eu disse lá no começo essa é a versão basicona, sem os códigos HTML que deixam a interface linde. Os scripts ui.R, server.R, global. R e a base babynames.csv do app descrito nesse post estão nesse repositório.Perfeito é impossívelMesmo a versão com ajustes em HTML pode e deve passar por melhorias tipo:permitir que o usuário do shiny app nomeie as etapas do sorteio deixando claro, por exemplo, que as primeiras cinco linhas da tabela descrevem os ganhadores das camisetas no exemplo do começo do post.permitir que o usuário do shiny app (o organizador do sorteio) faça download de uma planilha final com o nome das pessoas sorteadas, seguida da expressão que identifica o seu prêmio.ajustar a função sampler para que amostre o mínimo de 1 elemento e o máximo de elementos como sendo o número de nomes distintos colados na tela da esquerda.permitir que o usuário do shiny app (o organizador do sorteio) especifique a semente para o sorteio aleatório.E você? Sugere alguma funcionalidade para tornar esse shiny app ainda mais útil? Tem alguma coisa do app https://larissasayurifcs.shinyapps.io/rladiesBHsampler que não te parece intuitivo ou bem explicado? Sugestões e comentários são muito bem-vindos!!!Esse foi um dos projetos pessoais mais desafiadores que eu já peguei. Eu ainda sou imatura em programar um shiny app por exigir a compreensão clara dos 03 scripts ao mesmo tempo. Mas certamente foi o projeto que eu mais aprendi recentemente. Sobretudo na tarefa de escrever esse post, porque me fazer colocar definições e raciocínios nas minhas próprias palavras costuma ser a melhor forma de me fazer aprender.ReferênciasPara esse post foram usados os seguintes pacotes:babynames: Hadley Wickham (2019). babynames: US Baby Names 1880–2017. R package version 1.0.0. https://CRAN.R-project.org/package=babynamesdplyr: Hadley Wickham, Romain François, Lionel Henry and Kirill Müller (2020). dplyr: A Grammar of Data Manipulation. R package version 1.0.0. https://CRAN.R-project.org/package=dplyrshiny: Winston Chang, Joe Cheng, JJ Allaire, Yihui Xie and Jonathan McPherson (2020). shiny: Web Application Framework for R. R package version 1.5.0. https://CRAN.R-project.org/package=shinyE como não poderia faltar:O shiny app está disponível na plataforma self-service https://www.shinyapps.io/, específica para aplicações do shiny.rladiesbhUma comunidade de mulheres interessadas em R e Data Science. :)Follow6 Sign up for We R Ladies - BlogBy rladiesbhUma comunidade de mulheres interessadas em R e Data Science. :) Take a lookGet this newsletterBy signing up, you will create a Medium account if you don’t already have one. Review our Privacy Policy for more information about our privacy practices.Check your inboxMedium sent you an email at  to complete your subscription.RRladiesR Shiny6 claps6 clapsWritten byLarissa Sayuri Futino Castro Dos SantosFollowStatistician, Data Scientist, stationery lover, cooker wannabe.FollowrladiesbhFollowUma comunidade de mulheres interessadas em R e Data Science. :)FollowWritten byLarissa Sayuri Futino Castro Dos SantosFollowStatistician, Data Scientist, stationery lover, cooker wannabe.rladiesbhFollowUma comunidade de mulheres interessadas em R e Data Science. :)More From MediumInterpretação de Modelos de Machine Learning no R — Parte 2Jéssica Ramos in rladiesbhViolência contra a Mulher: dados do Governo Federal, formattable e leafletLarissa Sayuri Futino Castro Dos Santos in rladiesbhInterpretação de Modelos de Machine Learning no RJéssica Ramos in rladiesbhO que é Sports Analytics e por que você pode se interessar pela áreaMariana Pasqualini in rladiesbhComo contribuir no blog do R-Ladies Beagá?RLadies Belo Horizonte in rladiesbhMachine Learning — Bibliotecas úteis para análise no R.Gisele G Brito in rladiesbhComo fazer um mapa animado com dados do IBGE ❤Marília Melo Favalesso in rladiesbhRMarkdown: o mínimo que você precisa saberLarissa Sayuri Futino Castro Dos Santos in rladiesbhLearn more.Medium is an open platform where 170 million readers come to find insightful and dynamic thinking."
Transformando colunas em linhas e outras loucuras com tidyr,https://medium.com/psicodata/transformando-colunas-em-linhas-com-tidyr-a649f287a238?source=tag_archive---------2-----------------------,"Ci,R,Psicodata,Tutorial,Psicometria","Cada célula é um valor único.Cada coluna é uma variável.Cada linha é uma observação.O pacote tidyr foi criado com esse propósito.Quando temos um banco de dados, uma das primeiras preocupações deve estar relacionada à qualidade desses dados. Eles precisam estar organizados e limpos para que as análises sejam feitas.Já falamos aqui sobre a manipulação de um banco de dados em Python e em R (com o pacote dplyr). Além de manipular dados, é importante verificar se eles estão organizados. A equipe do tidyverse.org preparou o checklist acima para entender se um banco de dados está organizado.Esse é um tutorial para te ensinar a fazer as seguintes transformações:1. Separar uma coluna em duas ou mais colunas2. Transformar colunas em linhas3. Transformar linhas em colunas4. Unir colunasA gente vai usar esse banco de dados aqui. Esse banco contém dados fictícios do número de telefonemas e cartas que passaram por essas cidades pequenas.Banco de dados a ser utilizado.Atualmente o banco tem 20 linhas e 10 colunas.Ao final, ele vai ter 70 linhas e 6 colunas.A coluna id representa o código de identificação da cidade. A coluna cidade contém o nome das cidades. A informação sobre as cartas são sempre pegas no mesmo dia do ano, já as colunas hora_coleta e minutos_coleta representa o horário em que as informações sobre a cidade foram coletadas naquele ano — sempre entre 11h da manhã e 13h da tarde.Os anos estão separados por colunas, sendo que as últimas colunas, infelizmente, tiveram seu valor colocado em uma mesma coluna… Uma pena, embora seja conveniente para o que queremos aprender.Transformação inicialAntes de começar, vamos apenas carregar o banco de dados e retirar a primeira coluna que carrega com ele.data_url <- ""https://raw.githubusercontent.com/GabrielReisR/R/master/estrutura%20de%20dados/dados/untidy.csv""untidy <- read.csv(data_url)untidy <- untidy[,-c(1)] # retirando primeira colunaVamos começar?Separar uma coluna em duas ou mais colunasVamos lembrar da primeira regra: cada célula deve ser um valor único.Para ter a certeza de que nosso banco cumpra isso, utilizaremos a função separate() do tidyr.O primeiro grande erro do banco, como já citado, está na computação da coluna ano_1994_1995_1996.Vamos usar a função separate() para separar os valores da coluna ano_1994_1995_1996.separate() é bem simples, então vamos exemplificar fazendoBasta adicionar o nome da coluna atual (""ano_1994_1995_1996""), a separação que existe entre as variáveis daquela coluna (nesse caso, um espaço sep = "" ""), e quais as novas colunas que devem ser criadas (into = c(""ano_1994"", ""ano_1995"", ""ano_1996"")). Pronto!O que antes estava contido em uma só coluna agora está separado por coluna. Vários valores que estavam em uma só célula agora estão em outras.Transformar colunas em linhasAgora a gente vai entrar nas funções pivot. Vamos lembrar: Cada coluna deve ser uma variável.Para ter a certeza de que nosso banco cumpra isso, utilizaremos pivot_longer(). Nos livraremos de colunas repetitivas e alongaremos o banco de dados ao acrescentar mais linhas nele.A parte mais fácil passou. Era óbvio que os dados dos anos precisavam ser separados.Entretanto, nem tudo é óbvio quando se trata de dados organizados. Às vezes pode ser difícil enxergar que precisamos deixar apenas uma coluna para cada variável.Todos os anos de coleta dizem respeito a apenas uma variável, a variável ano. Como sabemos disso? Pois isso é algo que varia em cada observação. Ao invés de termos uma coluna para cada um dos anos, o ideal seria criar uma coluna ano para armazenar todos esses anos.Quais colunas entrarão na nova coluna ano?A nossa nova coluna ano armazenará todos os nomes das colunas que representam o ano. Já os valores dessas colunas serão armazenados em uma nova coluna chamada de casos.Como escrever isso? Usaremos a função pivot_longer().Transformar linhas em colunasOlhando o banco acima podemos perceber que existem duas observações para cada um dos 6 anos coletados. Isso porque há uma divisão entre o tipo “cartas” e “telefonemas”.Isso quebra nossa terceira regra. Vamos lembrá-la: cada linha é uma observação só.Para ter a certeza de que nosso banco cumpra isso, utilizaremos pivot_wider(). Nos livraremos de linhas repetitivas e alargaremos o banco de dados ao acrescentar mais colunas nele.Agora que temos o nosso banco quase pronto, vamos refletir sobre duas colunas importantes: tipo e casos.tipo informa qual meio de comunicação essa cidade usou.casos informa quantas vezes essa cidade usou esse meio.O problema aqui é que temos duas linhas para uma observação só! Em 1990, Butte, às 11h27, desde o ano anterior, havia enviado 4275 cartas e feito 3450 telefonemas.A questão é que “cartas” e “telefonemas” são coisas diferentes, elas não são variáveis contidas em “tipo” (assim como os anos estão contidos ano, telefonemas e cartas não estão contidas em tipo). Estamos alongando o banco sem necessidade, já que estamos falando de coisas diferentes e poderíamos ter duas colunas para cada uma dessas variáveis.Torna-se interessante alargar esses dados (essa é uma escolha, não uma regra) — ou seja, criar colunas para cartas e telefonemas. Para isso, usamos pivot_wider().Agora sim!Perceba que agora temos apenas uma linha para cada ano, ao invés de 2 linhas para cada ano.Da mesma forma, ‘casos’ não é mais necessário, e nosso banco se tornou mais fácil de ser lido.Unir colunasUma coluna para cada variável e uma célula para um caso único. Aqui, a gente vai se preocupar com duas colunas, utilizando unite() para resolver um probleminha que temos.Algo que ainda não conversamos sobre é a redundância das colunas hora_coleta e minutos_coleta. Essa pode ser apenas uma coluna, não precisa ser duas. Vamos criar uma nova coluna que vai se chamar horario_coleta.Vamos juntar as duas com unite().Vamos entender:Iniciamos com a primeira coluna que queremos. O valor dessa coluna irá na frente (hora_coleta).A segunda coluna informa o valor que irá em segundo lugar (minutos_coleta).Definimos uma separação para nossas variáveis. Nesse caso, tratando-se de horários, preferi sep = "":"".É possível definir sem separação: nesse caso, sep = """".Em col = ""horario_coleta"", definimos o nome da nossa coluna como horario_coleta.Como prometido, passamos de 20 linhas para 70 linhas, de 10 colunas para 6 colunas.Espero que tenha ajudado, de alguma forma, a entender um pouco mais sobre dados organizados usando tidyr.Como procurar ajuda?O tutorial apresentado nesse post também pode ser encontrado aqui.Fóruns: https://pt.stackoverflow.com/Documentação: https://cran.r-project.org/web/packages/tidyr/tidyr.pdfPágina no tidyverse: https://tidyr.tidyverse.org/index.htmlCheatsheet (folha de códigos): https://github.com/rstudio/cheatsheets/blob/master/data-import.pdfContatoQualquer dúvida, observação ou comentário são muito bem-vindos!Fique à vontade para se manifestar e vamos aprender juntos :)Fale comigo pelo LinkedIn ou pelo email reisrgabriel@gmail.com 😁PsicoDatapsicodataFollow2 Sign up for PsicoData NewsletterBy PsicoDataFique por dentro das nossas publicações! Take a lookGet this newsletterBy signing up, you will create a Medium account if you don’t already have one. Review our Privacy Policy for more information about our privacy practices.Check your inboxMedium sent you an email at  to complete your subscription.Ciência De DadosRPsicodataTutorialPsicometria2 claps2 clapsWritten byGabriel RodriguesFollowSou um psicólogo que adora Ciência de Dados — busco criar e compartilhar conteúdo sobre esses dois assuntos. linktr.ee/gabrielrrFollowPsicoDataFollowUma plataforma colaborativa em português sobre ciência de dados e psicologia.FollowWritten byGabriel RodriguesFollowSou um psicólogo que adora Ciência de Dados — busco criar e compartilhar conteúdo sobre esses dois assuntos. linktr.ee/gabrielrrPsicoDataFollowUma plataforma colaborativa em português sobre ciência de dados e psicologia.More From MediumUsando R para avaliar a normalidadeGabriel Rodrigues in PsicoDataTá tudo junto: média, variância e desvio-padrãoGabriel Rodrigues in PsicoDataDados e Psico: 5 lições que eu gostaria de ter ouvido no inícioPaula Costa in PsicoDataO que é uma variável latente e como medi-la? — Parte 2Gabriel Rodrigues in PsicoDataSimples e Direto: um guia de visualização de dados com PythonDalton Costa in PsicoDataBaixando e processando dados do DATASUS sobre suicídio com PythonDalton Costa in PsicoDataO que é uma variável latente e como medi-la? — Parte 1Gabriel Rodrigues in PsicoDataUm gráfico vale mais que mil palavras!Marcela Alves Sanseverino in PsicoDataLearn more.Medium is an open platform where 170 million readers come to find insightful and dynamic thinking."
Predicting Adoption Speed for PetFinder,https://towardsdatascience.com/predicting-adoption-speed-for-petfinder-bb4d5befb78c?source=tag_archive---------0-----------------------,"R,Pets,Data Modeling,Classification","Our team is currently acting as a consulting agency who works on behalf of PetFinder, a non-profit organization, contains a database of animals and aims to improve the animal welfare through collaborations with related parties. The core task of this project is to predict how long it will take for a pet to be adopted.Business UnderstandingCredit: Petfinder FoundationAccording to PetFinder’s 2018 Financial Report, approximately 70% of its total public support gained and revenues earned will be spent on its public welfare programs, including this animal adoption program. Our defined business problem is “When new pets come in, what would be the estimated time for new pets to be adopted.” Based on different traits each pet has, we could estimate how long it takes for the pet to be adopted according to the model we created. Therefore, by adopting our model, the company can guide shelters and rescuers who post information about animals through the channel on estimating the adoption speed for new animals. Once the pet adoption speed is predicted well, more efficient resource allocation can be implemented to improve the overall adoption performance, and subsequently, reduce the costs of sheltering and fostering. Eventually PetFinder will receive more financial support and make a greater contribution to the global animal benefit.Data PreparationThis dataset originally consists of 14,993 observations and 24 variables. All the data are based in the Malaysia area so our analysis is country-specific and culture-oriented. We removed the variable Name since we assume it is irrelevant to the pet’s adoption speed. Other than Name, we removed State, PetID, and RescuerID for the same reason. In addition, some columns such as Breed1 and Type are coded as numbers with an additional reference spreadsheet provided. For clarification reasons, we convert Type and Gender into strings: we coded “1” as “Dog” and “2” as “Cat”; For Gender, we coded “1” as “Male”, “2” as “Female”, and “3” as “Mixed”. The mixed represents the gender of a profile of pets. We also changed categorical variables into dummy variables. Besides, we conduct a chi-squared test to test the correlation among Color1, Color 2, and Color3, the result shows that they are dependent on each other. Therefore, we plan to remain Color1 and remove the other two. Finally, considering only 0.38% of records have Breed2, we remove Breed2 for simplicity reasons.Besides, considering that the dataset contains a Description column, we applied text analytics to this column. Based on the analytics, we select the 10 most frequently occurring words and create an additional 10 columns based on these hot words. We check if each pet’s description contains these 10 words individually. If any word is matched in the description, we assign “1” under that column. We assign “0” if it does not. These 10 words are: love, kitten, puppi, rescu, healthi, activ, cute, train, mother, kind. Finally, we all agree that the length of a description may be an important factor as well. Since the more detailed the description is, the less time it takes adopters to pick the pet. Logically, adopters tend to make decisions quicker if the available information is richer. Therefore, our final dataset consists of 14,993 records and 29 variables.Data ExplorationText AnalysisBased on the text analytics, we first created a text cloud to capture the most frequent word in the Description column. It turns out that “adopt” appears most frequently, followed by words “kitten”, “puppi” and “cute”. From the word cloud, we can summarize the major characteristics of those pets in our dataset.Next, we created bigrams to capture more patterns in the text. The most frequent bigram is “month old”, and we pick 10 most intriguing words and plot them in a bar chart.As we can see from the two graphs, rescuers and shelters tend to use a lot of descriptive words in the description such as active, good, cute etc. As mentioned above, to test the effectiveness of these words, we picked 10 hot words and converted them into 10 categorical variables. The reason that we did not choose bigram was that bigrams were made of stemming words, so it may be hard for the software to detect these bigrams in description.Breed vs. AdoptionSpeedAdditionally, from Perro Market Website, one of the most well-known pet websites in Malaysia, we learned the 10 most popular cat and dog breeds in the country. We picked Maine and Persian for Cat, and Poodle for Dog for the purpose of data exploration.The mean of AdoptionSpeed for cats is 2.40 with a standard deviation of 1.21. By comparison, the average Adoption Speed for Maine is 1.97 and the average Adoption Speed for Persian is 1.95. Even though their mean is lower than the average, they are still in the first standard deviation range. Based on this fact, we could not conclude that popular cat breeds such as Maine and Persian tend to have shorter adoption period than other breeds.On the other hand, dogs have an average AdoptionSpeed of 2.62 with a standard deviation of 1.14. Noticeably, Poodle has an average adoption speed of 1.97. Again, even though the average adoption speed is smaller than the average of the whole group, it still falls within 1 standard deviation range, so we could not conclude that popular dog breeds lead to shorter adoption period.However, from the graph below, we find that the number of pets under popular breeds is much higher than the rest. Breed 179 is Poodle, and Breed 285 is Persian while Breed 276 is Maine.Based on this preliminary analysis, our conclusion is that popular breed determines the number of pets under this breed but do not necessarily mean lower adoption speed.Vaccinated vs. AdoptionSpeedVaccinated as “1” means the pet is vaccinated, “2” means the pet is not vaccinated, and “3” means not sure. The pet who has been vaccinated counterintuitively shows higher median of time span than the one not got vaccinated, and for those reported unknown they have higher time span waiting to be adopted. There’s also no clear correlation (-0.06) between the AdoptionSpeed and Vaccinated across the species.ModelingSince the target variable AdoptionSpeed is a categorical variable with 4 levels, we decided the core task of this project to be classification: when a new pet comes in, the model would determine how long it will be adopted based on which level of adoption speed it is classified into. Because of its multi-class characteristic, we choose three models to achieve the goal, including Random Forest, SVM, and Multinomial logistic regression. All three models help solve our business performance by predicting the adoption speed for new animals based on their provided attributes. By sharing our predictions with the company, the company can have a better plan about the allocation of current resources, especially allocating more resources to the animals with longer predicted adoption period to accelerate their adoption speed.Random ForestRandom Forest consists of a large number of individual decision trees that operate as an ensemble. Each individual tree in the random forest spits out a class prediction and the class with the most votes becomes our model’s prediction. One pro with Random Forest is that its decorrelation when dealing with highly correlated variables. One con is its limitation on the number of categorical variables could be included in the predictors. In our project, we excluded the Description since there are more than 14000 levels in one variable and it has already segmented into words with high frequency and description length.Support Vector Machines (SVM)Another model we used is the Support Vector Machines (SVM) model, to create the optimal hyperplane to separate classes and classify new records in our test dataset. We include all the variables from the cleaned training dataset, except Description. SVM is easily adopted in solving multiclass classification problems by transforming complex data into high dimensional analysis and thus having classes easily separable. The model itself has good computational properties, but it does not attempt to model probability. Comparing with Random Forest, SVM can only be used to predict the class instead of estimating the probability of different classes. From this point of view, Random Forest may offer us a more comprehensive understanding of the prediction of classes of the test dataset.Multinomial Logistic Regression with Lasso and Post LassoWe also used multinomial logistic regression to predict our target variable. However, because of the limitations on the number of variables could be included in multinomial logistic regression, we used Lasso to select related variables for each class of adoption speed. We built 4 models of multinomial logistic regression with Lasso and post-Lasso based on minimum λ and 1 standard error λ. After selecting the variables, the models eliminated overfitting problems and may make better predictions. However, the variables selected by Lasso may differ in different bootstrapped models, and they are hard to be interpreted.EvaluationIn order to evaluate the performance of each model, we conducted a 3-fold cross-validation and selected the Confusion Matrix Balanced Accuracy as our Out of Sample accuracy measurement. Based on the chart above, all of our six models have a similar level of accuracy performance, ranging from 0.5 to 0.6. Random forest achieved the highest level of OOS accuracy of 59%. Multinomial Logistic Regression with Lasso using minimum λ is the second highest with 56% OOS accuracy. Considering OOS accuracy as an important indicator for the classification models, we chose Random Forest as the optimal model to predict a newly coming pet’s adoption speed. To investigate specific classification accuracy, we further evaluated the confusion matrix of Random Forest and SVM, one has the highest and one has the lowest OOS accuracy.From the confusion matrix, we can conclude that Random Forest predicts more accurately when pets are actually adopted within 1 month (AdoptionSpeed = 2) and no adoption after 100 days being listed (AdoptionSpeed = 4). Random Forest does not show a tendency towards one specific class. In contrast, SVM is leaning toward classifying pets into AdoptionSpeed level of 2.One possible limitation with the model is that there are too few data points with 0 AdoptionSpeed and therefore it’s hard to evaluate its classification accuracy. On the other hand, with the above-mentioned limitation of Random Forest, if the PetFinder adds a new categorical variable that has more than 100 levels, Multinomial Logistic Regression with Lasso using minimum λ, instead of Random Forest, would be a better choice to classify newly coming pet’s adoption speed.When we try to apply our model to real-life business, we should develop a business case to identify if the prediction is accurate and if the business can utilize the prediction to optimize its resource planning and thus improve the adoption performance. The business can compare its previous adoption performance with its future adoption performance by having resources allocated based on our prediction model. The improvement in adoption performance can be proved by the increase in the number of animals adopted within the same period of time. In addition, the business should analyze if the animals with long predicted adoption time can be adopted within a shorter period of time, to see if the change in resource allocation is efficient.DeploymentBy evaluating each model with its OOS accuracy, we reached a conclusion that Random Forest, which generated the highest OOS accuracy, would be the optimal choice for PetFinder. Whenever a new pet comes in, PetFinder is capable to predict how long it will take for a pet to be adopted by simply inputting its features. Estimating the adoption time more accurately will enable us to improve the animal’s profile, which is crucial to expedite the adoption speed, and subsequently save the costs of sheltering.In order to have more insights on what specific features have a significant impact on pets that take a longer time to be adopted, we examined the variables selected by Multinomial Logistic Regression with Lasso using minimum λ. Since the variable selection differs in each fold of data, we took the intersection of the variable selections in 3 folds and found that across the four levels of adoption speed, Age, Breed, Amount of photos uploaded, and Amount of Video Uploaded are important. Specifically, for pets with slower adoption speed, people pay more attention to their health: whether they are vaccinated, dewormed, and sterilized. Also, the length of the description plays a more important role in the profile of pets with slower adoption speed. If pets are classified as possibly taking a longer time to be adopted, PetFinder should encourage the shelters to prioritize the health of the pets and increase the length of the description on profile.One important ethical consideration is that if the pets should be treated differently based on their predicted adoption speed. Without the prediction model, we assume that all pets are receiving an equal level of treatment. However, if we suggest the business to allocate more resources to animals with lower predicted adoption speed, we are resulting in an unequal distribution of resources. If our prediction is not accurate, some pets will be negatively influenced and more unlikely to be adopted. From this perspective, the business’ goal of improving global animal benefits may not be fully achieved.One possible risk for our model is that we are currently having an inaccurate prediction for pets in the actual adoption speed class of 0. If those pets have predicted class of 3 or 4, the business will allocate more resources to promote them and thus results in a waste of limited resources. To alleviate the risk, we should always update and improve our model as we have more records, especially records with 0 actual adoption speed.Written byEileen Z.Follow35 1 Sign up for The Daily PickBy Towards Data ScienceHands-on real-world examples, research,  tutorials, and cutting-edge techniques delivered Monday to Thursday. Make learning your daily ritual. Take a lookGet this newsletterBy signing up, you will create a Medium account if you don’t already have one. Review our Privacy Policy for more information about our privacy practices.Check your inboxMedium sent you an email at  to complete your subscription.35 35 1 RPetsData ModelingClassificationMore from Towards Data ScienceFollowA Medium publication sharing concepts, ideas, and codes.Read more from Towards Data ScienceMore From Medium5 YouTubers Data Scientists And ML Engineers Should Subscribe ToRichmond Alake in Towards Data Science7 Must-Haves in your Data Science CVElad Cohen in Towards Data Science30 Examples to Master PandasSoner Yıldırım in Towards Data Science21 amazing Youtube channels for you to learn AI, Machine Learning, and Data Science for freeJair Ribeiro in Towards Data ScienceThe Roadmap of Mathematics for Deep LearningTivadar Danka in Towards Data Science4 Types of Projects You Must Have in Your Data Science PortfolioSara A. Metwalli in Towards Data ScienceAn Ultimate Cheat Sheet for Data Visualization in PandasRashida Nasrin Sucky in Towards Data ScienceHow to Get Into Data Science Without a DegreeTerence S in Towards Data ScienceAboutHelpLegalGet the Medium app"
"<strong class=""cc"">KNN Algorithm in R</strong>",https://medium.com/nerd-for-tech/knn-algorithm-in-r-30dff0c8d094?source=tag_archive---------1-----------------------,"Machine Learning,Algorithms,Knn Algorithm,R,Supervised Learning","The huge amount of data that we’re generating every day, has led to an increase in the need for advanced Machine Learning Algorithms. One such well-performed algorithm is the K Nearest Neighbour algorithm.In this blog on KNN Algorithm In R, we will understand what is KNN algorithm in Machine Learning and its unique features including the pros and cons, how the KNN algorithm works, an essay example of it, and finally moving to its Implementation of KNN using the R Language.It is quite essential to know Machine Learning basics. Here’s a brief introductory section on what is Machine Learning and its types.Machine learning is a subset of Artificial Intelligence that provides machines the power to find out automatically and improve from their gained experience without being explicitly programmed.There are mainly three types of Machine Learning discussed briefly below:1. Supervised Learning: It is that part of Machine Learning in which the data provided for teaching or training the machine is well labeled and so it becomes easy to work with it.2. Unsupervised Learning: It is the training of information using a machine that is unlabelled and allowing the algorithm to act on that information without guidance.3. Reinforcement Learning: It is that part of Machine Learning where an agent is put in an environment and he learns to behave by performing certain actions and observing the various possible outcomes which it gets from those actions.Now, moving to our main blog topic what is KNN Algorithm?KNN which stands for K Nearest Neighbour is a Supervised Machine Learning algorithm that classifies a new data point into the target class, counting on the features of its neighboring data points.Let’s attempt to understand the KNN algorithm with an essay example. Let’s say we want a machine to distinguish between the sentiment of tweets posted by various users. To do this we must input a dataset of users’ sentiment (comments). And now, we have to train our model to detect the sentiments based on certain features. For example, features such as labeled tweet sentiment i.e., as positive or negative tweets accordingly. If a tweet is positive, it is labeled as 1 and if negative, then labeled as 0.Features of KNN algorithm:KNN is a supervised learning algorithm, based on feature similarity.Unlike most algorithms, KNN is a non-parametric model which means it does not make any assumptions about the data set. This makes the algorithm simpler and effective since it can handle realistic data.KNN is considered to be a lazy algorithm, i.e., it suggests that it memorizes the training data set rather than learning a discriminative function from the training data.KNN is often used for solving both classification and regression problems.Disadvantages of the KNN algorithm:After multiple implementations, it has been observed that the KNN algorithm does not work with good accuracy on taking large datasets because the cost of calculating the distance between the new point and each existing point is huge, and in turn, it degrades the performance of the algorithm.It has also been noticed that working on high dimensional data is quite difficult with this algorithm because the calculation of the distance in each dimension is not correct.It is quite needful to perform feature scaling i.e., standardization and normalization before actually implementing the KNN algorithm to any dataset. Eliminating these steps may lead to wrong predictions by the KNN algorithm.Sensitive to noisy data, missing values and outliers: KNN is sensitive to noise in the dataset. We need to manually impute missing values and remove outliers.KNN Algorithm ExampleIn order to make understand how KNN algorithm works, let’s consider the following scenario:In the image, we have two classes of data, namely class A and Class B representing squares and triangles respectively. The problem statement is to assign the new input data point to one of the two classes by using the KNN algorithmThe first step in the KNN algorithm is to define the value of ‘K’ which stands for the number of Nearest Neighbors.In this image, let’s consider ‘K’ = 3 which means that the algorithm will consider the three neighbors that are the closest to the new data point. The closeness between the data points is calculated either by using measures such as Euclidean or Manhattan distance. Now, at ‘K’ = 3, two squares and 1 triangle are seen to be the nearest neighbors. So, to classify the new data point based on ‘K’ = 3, it would be assigned to Class A (squares).Ways to measure the new data point and nearest data points:Euclidean Distance: It always gives the shortest distance between the two points.Manhattan Distance: To measure the similarity, we simply calculate the difference for each feature and add them up.Practical Implementation of KNN Algorithm in RProblem Statement: To study a bank credit dataset and build a Machine Learning model that predicts whether an applicant’s loan can be approved or not based on his socio-economic profile.Step 1: Import the dataset and then look at the structure of the dataset:loan <- read.csv(“C:/Users/zulaikha/Desktop/DATASETS/knn dataset/credit_data.csv”)str(loan)Step 2: Data CleaningFrom the structure of the dataset, we can see that there are 21 predictor variables but some of these variables are not essential in predicting the loan. So, it’s better to filter down the predictor variables by narrowing down 21 variables to 8 predictor variables.loan.subset <- loan[c(‘Creditability’, ‘Age..years.’, ‘Sex…Marital.Status’, ‘Occupation’, ‘Account Balance’, ‘Credit.Amount’, Length.of.current.employment’)]head(loan.subset)Step 3: Data NormalizationIt’s very essential to always normalize the data set so that the output remains unbiased. In the below code snippet, we’re storing the normalized data set in the ‘loan.subset.n’ variable and also we’re removing the ‘Credibility’ variable since it’s the response variable that needs to be predicted.normalize <- function(x){return ((x — min(x)) / (max(x) — min(x)))}loan.subset.n <- as.data.frame(lapply(loan.subset[,2:8], normalize))Step 4: Data SplicingIt basically involves splitting the data set into training and testing data set. Then, it is needful to create a separate data frame for the ‘Creditability’ variable so that our final outcome can be compared with the actual value.set.seed(123)dat.d <- sample(1:nrow(loan.subset.n),size=nrow(loan.subset.n)*0.7,replace = FALSE) #random selection of 70% data.train.loan <- loan.subset[dat.d,] # 70% training datatest.loan <- loan.subset[-dat.d,] # remaining 30% testing datatrain.loan_labels <- loan.subset[dat.d,1]test.loan_labels <-loan.subset[-dat.d,1]Step 5: Building a Machine Learning modelAt this stage, we have to build a model by using the training data set. Since we’re using the KNN algorithm to build the model, we must first install the ‘class’ package provided by R. Next, we’re going to calculate the number of observations in the training data set.install.packages(‘class’)library(class)NROW(train.loan_labels)knn.26 <- knn(train=train.loan, test=test.loan, cl=train.loan_labels, k=26)knn.27 <- knn(train=train.loan, test=test.loan, cl=train.loan_labels, k=27)Step 6: Model EvaluationAfter building the model, it is time to calculate the accuracy of the created models:ACC.26 <- 100 * sum(test.loan_labels == knn.26)/NROW(test.loan_labels)ACC.27 <- 100 * sum(test.loan_labels == knn.27)/NROW(test.loan_labels)As shown above, the accuracy for K = 26 is 67.66 and for K = 27 it is 67.33. So, from the output, we can see that our model predicts the outcome with an accuracy of 67.67% which is good since we worked with a small data set.The summarizing words…KNN proves to be a useful algorithm at a lot many areas such as in banking sectors to predict whether a loan will be approved by a manager for an individual or not, in credit rate calculation of an individual by comparing the rate with a person bearing similar traits, and also in politics to classify a potential voter. Other areas in which the KNN algorithm can be used are Speech Recognition, Handwriting Detection, Image Recognition, and Video Recognition.Check out more articles:https://medium.com/nerd-for-tech/keras-608df20e88fdPlease share this Article with all your friends and hit that 👏 button below to spread it around even more. Also, add any points or maybe your valuable suggestions that you want to convey below in the comments 💬!I would love to hear from you. Stay in touch by following me…Check out more articles @ https://medium.com/@Eshita_NandyNerd For TechFrom Confusion to ClarificationFollow51 Machine LearningAlgorithmsKnn AlgorithmRSupervised Learning51 claps51 clapsWritten byEshita NandyFollowTechnical Content Writer || Data Science Enthusiast || Intern @ IIT BHU || B Tech — IT ||FollowNerd For TechFollowWe are tech nerds because we believe in reinventing the world with the power of Technology. Our articles talk about some of the most disruptive ideas, technology, and innovation.FollowWritten byEshita NandyFollowTechnical Content Writer || Data Science Enthusiast || Intern @ IIT BHU || B Tech — IT ||Nerd For TechFollowWe are tech nerds because we believe in reinventing the world with the power of Technology. Our articles talk about some of the most disruptive ideas, technology, and innovation.More From Medium(My) Machine Learning WorkflowArian Prabowo in Data Driven InvestorEXAM — State-of-The-Art Method for Text ClassificationNeuroHiveLearning a XOR Function with Feedforward Neural NetworksTojo Batsuuri in The StartupAnti-Patterns in Machine LearningAndrew Lamb in firemind.K-Means Algorithm: Dealing with Unlabeled DataSrijarko Roy in SRM MICRecommendation System: An IntroductionRuben Winastwan in The StartupNearest Neighbors with Keras and CoreMLSøren L KristiansenPre-trained language model in any languagerohan.arora in AI n ULearn more.Medium is an open platform where 170 million readers come to find insightful and dynamic thinking."
Integrate R & JIRA using JIRA REST API,https://medium.com/@gowthamrajaram/integrate-r-jira-using-jira-rest-api-a3e4617b60e8?source=tag_archive---------2-----------------------,"Jira,Rest Api,API,R,Programming","This article provides introduction on how to integrate R and JIRA using httr package (https://cran.r-project.org/web/packages/httr/httr.pdf)Photo by Chris Ried on UnsplashBefore we get started a small introduction on REST API .A RESTful API is an application program interface (API) that uses HTTP requests to GET, PUT, POST and DELETE data. An API for a website is code that allows two software programs to communicate with each other.JIRA does provide some documentation on how to access their API.But it not specific to R.JIRA 7.13.0docs.atlassian.comwithin your .renviron file create two new variablesJIRA_USER: email address of JIRA associated accountJIRA_APIKEY: can be generated within your profile->security->API tokenHaving this within environmental variables ensures that you have access to these while running your R sessionLets dive into a sample R snippet that returns list of projects available:library(httr)library(jsonlite)url<-“http://host:port/rest/api/2/project""params <- list(httr::accept_json())params<-c(params,httr::authenticate(Sys.getenv(‘JIRA_USER’),  Sys.getenv(‘JIRA_API_KEY’)))json <-httr::GET(url =url,config = params)project_dataframe<-fromJSON(rawToChar(json$content))project_dataframe$nameparams -> to specify the username and jira api key as parameterproject_dataframe$name -> gives out list of projectsWe can do much more by integrating JIRA API with R .Including but not limited to create projects , Upload file , add comments,filters and so on .If you want to see more code snippets on how to read/write (GET/POST) data to/from JIRA using R let me know in comments belowWritten byGowtham RajaramI am a Data Science enthusiast and love working with data :) Because data is the footprint of our existenceFollow1 1 1 1 1 JiraRest ApiAPIRProgrammingMore from Gowtham RajaramFollowI am a Data Science enthusiast and love working with data :) Because data is the footprint of our existenceMore From MediumObject-Oriented JavaScript — Errors and IterablesJohn Au-Yeung in JavaScript In Plain EnglishCopying to the clipboard in ReactFeargal WalshHow I made my portfolio website blazing fast with GatsbyMaribel Duran in freeCodeCamp.orgUnderstanding Cypress.io: An Automated Testing LibraryRahul Gupta in DSC RNGPITGetting Gatsby WrongChris Vibert in Better ProgrammingMy “Whoa, I didn’t know that!” moments with JestBriwa in The StartupHide files and folders in FinderDaniel RotärmelMaterial-UI and React — Theme customizationCyclic BarrierAboutHelpLegalGet the Medium app"
Essential R packages for data science projects,https://medium.com/@ericbonucci/essential-r-packages-for-data-science-projects-d79cb5698b96?source=tag_archive---------3-----------------------,"R,Data Science,Productivity,Community,Rstudio","Leverage genius work from R community in your projects!There are more than 16 000 packages on the Comprehensive R Archive Network (CRAN) that gather a lot of commonly used methods in data science projects. Time runs fast, and it may takes days to code functionalities for sometimes basic tasks… Fortunately, we can leverage many packages to focus on what is essential for projects to be successful!Quick reminder: install and use packagesThe most common way is to install a package directly from CRAN using the following R command:# this command installs tidyr package from CRANinstall.packages(""tidyr"")Once the package is installed on your local machine, you don’t need to run this command again, unless you want to update the package with its latest version! If you want to check the version of a package you installed, you may use:# returns tidyr package versionpackageVersion(""tidyr"")RStudio IDE also provides a convenient way to check if any update is available for installed packages in Tools/Check for packages updates…Update all your packages in a few clicks using RStudioLast but not least: how to use a package now it is installed :) You may either specify the package name in front of its included method:stringr::str_replace(""Hello world!"", ""Hello"", ""Hi"")Or run the following command to load all the package’s functions at once:# load a package: it will throw an error if package is not installedlibrary(stringr)Now you’re ready to go!If you want to learn basically everything about R packages development, I highly recommend Hadley Wickham R packages book (free online version).Fetching dataFetching data is often the starting point of a data science project: data can be located in a database, an Excel spreadsheet, a comma-separated values (csv) file… it is essential to be able to read it regardless of its format, and avoid headaches before even starting to work with the data!When data is located in a .csv files or any delimited-values fileThe readr package provides functions that are up to 10 times faster than base R functions to read rectangular data.Great R packages usually have a dedicated hex sticker: https://github.com/rstudio/hex-stickersConvenient methods exist for reading and writing standard .csv files as well as custom files with a custom values separation symbol:# read csv data delimited using comma (,)input_data <- readr::read_csv(""./input_data.csv"")# read csv data delimited using semi-colon (;)input_data <- readr::read_csv2(""./input_data.csv"")# read txt data delimited using whatever symbols (||)input_data <- readr::read_delim(""./input_data.txt"", delim = ""||"")In addition to good looking stickers, great R packages also have cheat sheets you can refer to!When data is located in an Excel fileMicrosoft Excel has its own file formats (.xls and .xlsx) and is very commonly used to store and edit data. The package readxl enables efficient reading of these files into R, you can even only read a specific spreadsheet:# read Excel spreadsheetsinput_data <- readxl::read_excel(""input_data.xlsx"", sheet = ""page2"")When data is located in a database or in the cloudWhen it comes to fetching data from databases, DBI makes it possible to connect to any server, as long as you provide the required credentials, and run SQL queries to fetch data. Because there are many different databases and ways to connect depending on your technical stack, I suggest that you refer to the complete documentation provided by RStudio to find the steps that suit your needs: Databases using R.Make sure to check if a package exists to connect to your favorite cloud services provider! For example, bigrquery enables fetching data from Google BigQuery platform.Wrangling dataYou may have noticed a lot of the previously mentioned packages are part of the tidyverse. This collection of packages forms a powerful toolbox that you can leverage throughout your data science projects. Mastering these packages is key to become super efficient with R.The pipe operator shipped with the magrittr package is a game changer https://github.com/tidyverse/magrittrData wrangling is made easy using the pipe operator, which goal is simply to pipe left-hand values into right-hand expressions:# without pipe operatorpaste(""Hello"", ""world!"")# with pipe operator""Hello"" %>% paste(""world!)It may not seem obvious in this example, but this is a life-changing trick when you need to perform several sequential operations to a given object, typically a data frame.Data frames usually contains your input data, making it the R object you probably work the most with. dplyr is a package that provides useful functions to edit, filter, rearrange or join data frames.library(dplyr)# mtcars is a toy data set shipped with base R# create a column mtcars <- mtcars %>% mutate(vehicle = ""car"")# filter on a columnmtcars <- mtcars %>% filter(cyl >= 6)# create a column AND filter on a columnmtcars <- mtcars %>%   mutate(vehicle = ""car"") %>%  filter(cyl >= 6)Now you should understand my point about the power of the pipe operator :)There is so much more to say about data wrangling that you can find entire books discussing the topic, such as Data Wrangling with R. In addition, a key work on leveraging tidyr functionalities is R for Data Science. A free online version of the latter can be found here. Please notice that these are Amazon affiliated links so I will receive a commission if you decide to buy the books.VisualizationOne of the main reason R is a very good choice for data science projects may be ggplot2. This package makes it easy and eventually fun to build visualizations that looks good and gather a lot of informations.You may find inspirations from this Top 50 ggplot2 visualisation article : http://r-statistics.co/Top50-Ggplot2-Visualizations-MasterList-R-Code.htmlggplot2 is also part of the tidyverse collection, that’s why it works perfectly with shapes of data you typically obtain after tidyr or dplyr data wrangling operations. Managing to plot histograms and scatter plots is rather quick. Then many additional elements can be used to enhance your plots.Machine learningAnother very convenient package is caret, that wraps up a lot of methods typically used in machine learning processes. From data preparation, to model training and performances assessment, you will find everything you need when working on predictive analytics tasks.I recommend reading the caret chapter about model training where this key task is discussed. Here is a very simple example of how to train a logistic regression:library(dplyr)# say we want to predict iris having a big petal widthobservations <- iris %>%   mutate(y = ifelse(Petal.Width >= 1.5, ""big"", ""small"")) %>%   select(-Petal.Width)# set up a a 10-fold cross-validationtrain_control <- caret::trainControl(method = ""cv"",                                      number = 10,                                      savePredictions = TRUE,                                      classProbs = TRUE)# make it reproducible and train the modelset.seed(123)model <- caret::train(y ~ .,                      data = observations,                      method = ""glm"",                      trControl = train_control,                      metric = ""Accuracy"")Final wordsThanks a lot for reading my very first article on Medium! I feel like there is so much more to say in each section, as I did not talk about other super useful packages such as boot, shiny, shinydashboard, pbapply… Please share your thoughts in the comments, I am very interested in feedbacks on what you are willing to explore in future articles.Useful documentations and referencesreadr https://readr.tidyverse.org/index.htmlreadxl https://readxl.tidyverse.org/bigrquery https://github.com/r-dbi/bigrquerycaret http://topepo.github.io/caret/index.htmltidyr https://tidyr.tidyverse.org/All RStudio cheatsheets https://rstudio.com/resources/cheatsheets/Written byEric BonucciData scientist based in 📍Paris, FranceFollow48 48 48 RData ScienceProductivityCommunityRstudioMore from Eric BonucciFollowData scientist based in 📍Paris, FranceMore From Medium5 Simple Ways to Write Clean CodeShubham Pathania in Better ProgrammingA Practical Step-by-Step Guide to Understanding KubernetesRam Rai in Better ProgrammingIntroduction to Functional ProgrammingSerokell in The StartupApache Kafka Producer/Consumer using Spring BootOshan Fernando in The StartupRefactoring a Flutter Project — a Story About Progression and DecisionsGonçalo Palma in Flutter CommunityHow to use Java variables like a proTom HenricksenIntroduction to StanfordNLP: An NLP Library for 53 Languages (with Python code)Mohd Sanad Zaki Rizvi in Analytics VidhyaModeling Bowling in ElixirIan GrubbAboutHelpLegalGet the Medium app"
Análise de Imóveis com Estatística Descritiva,https://medium.com/@erick.vinicius81/an%C3%A1lise-de-im%C3%B3veis-com-estat%C3%ADstica-descritiva-b4271befd10b?source=tag_archive---------4-----------------------,"Estatistica,Ciencia De Dados,An,R","Esse texto tem objetivo apresentativo, os atributos técnicos, tais como os códigos gerados, podem ser acessados no meu perfil do GitHub:Er1ckAraujo/House-RocketProjeto proposto pelo Meigarom - Data Scientist que tem como objetivo mostrar o poder de uma boa Análise Exploratória…github.comImagine o seguinte cenário: Uma empresa do ramo imobiliário chamada House Rocket deseja aumentar seus lucros, para isso, precisa de uma análise para identificar quais são as melhores oportunidades de negócio. Essa empresa compra e vende imóveis, o seu lucro é a diferença entre o valor de venda, e o valor de compra. Como Analista, o nosso papel é responder as seguintes perguntas:1. Quais casas o CEO da House Rocket deveria comprar e por qual preço de compra?2. Uma vez a casa em posse da empresa, qual o melhor momento para vendê-las e qual seria o preço da venda?3. A House Rocket deveria fazer uma reforma para aumentar o preço da venda? Quais seriam as sugestões de mudanças? Qual o incremento no preço dado por cada opção de reforma?Esse projeto foi proposto pelo Meigarom — Data Scientist, seu objetivo é demonstrar como uma boa Análise Exploratória pode gerar bons insights e propor soluções para um negócio. Para saber mais, o desafio pode ser conferido clicando aqui.Sem mais delongas, vamos entender esse banco de dados, e extrair o que ele tem a nos dizer…O House Sales King County, USA é um banco de dados que contém informações sobre a venda de imóveis em King County entre 02/05/2014 até 27/05/2015, possui 21613 observações e 21 variáveis sendo elas:id — Um número único de identificação para cada casa vendida;date — Data de venda do imóvel no intervalo de um ano, como comentado acima;price — Preço do imóvel vendido, que varia de $75,000 até $7,700,000, essas casas valem em média $540,088;bedrooms — Número de quartos. A maioria das casas possuem de 1 a 6 quartos;bathrooms — Número de banheiros, onde números inteiros representam a quantidade de banheiros completos, e números fracionados representam banheiros incompletos. “O que é um banheiro incompleto?” — Foi a pergunta que eu me fiz quando tentava entender essa variável. Banheiros incompletos são como aqueles banheiros externos que temos no Brasil, geralmente encontrados em áreas de churrasqueira, que possuem apenas vaso sanitário, sem área para banho. É mais comum encontrar casas que possuem 2.5 banheiros, ou seja, dois banheiros completos, e um incompleto;sqft_living — É o espaço interior da casa em m². Essas casas possuem entre 290m² e 13540m², com média: 2080m²;sqft_lot — Essa variável representa o tamanho do terreno onde o imóvel está localizado e varia de 520m² a 1651359m², com média: 15107m²;floors — Número de andares. A maior parte das propriedades possuem 1 andar. Essa variável, assim como, a variável bathrooms, recebe uma nota: 1, 1.5, 2, 2.5… Aqui no Brasil, não é usual dizer ou descrever: “Meio andar”, ou “dois andares e meio”. Um andar completo aqui na análise é considerado um andar que ocupa toda área de base ocupada pelo primeiro;Waterfront — Uma variável binária que diz se uma casa possui vista pro mar, ou não. Apenas 0.75% das casas possuem vista pro mar;View — Um índice de 0 a 4 que avalia o quão boa é a vista da propriedade;Condition — Um índice de 1 a 5 da condição do apartamento;grade — Um índice de 1 a 13, onde 1–3 fica aquém da construção e design dos edifícios, 7 tem um nível médio de construção e design e 11–13 têm um alto nível de qualidade de construção e design.yr_built — Ano de construção do imóvel. Varia entre 1900 e 2015;Há ainda algumas variáveis que foram omitidas por mim nessa apresentação inicial por dois motivos, primeiro, para que a leitura fique menos densa, e segundo que dado o nosso objetivo de encontrar oportunidades de compra e venda de imóveis, algumas variáveis não são tão relevantes para o nosso propósito, como por exemplo, sqft_living15. Essa variável, descreve a metragem quadrada do espaço habitacional para os 15 vizinhos mais próximos. Imagine você na posse de um imóvel que deseja vender, antes de vender, você pretende fazer algumas reformas e modificações para que o seu imóvel fique mais atrativo para a venda, ainda que a localização dessa propriedade tenha influência sobre seu preço, não é viável pensar em melhorias que englobem uma fração do seu bairro. O que eu estou tentando dizer? Dado uma propriedade, algumas variáveis estão fora do nosso poder de modificação, como CEP por exemplo. Não há reforma que mude um imóvel de lugar. Voltando alguns passos, é interessante observar também que, as propriedades possuem, em média, 2080 m² de construção, e 15107m² de terreno, ou seja, estamos lidando com imóveis com grande potencial construtivo, visto que há espaço disponível.É importante lembrar do objetivo da nossa análise para que ela não se perca no meio de tantas informações.O meu processo investigativo tem como objetivo encontrar variáveis que influenciam o preço da propriedade. Sendo assim, vamos avaliar a correlação de algumas variáveis numéricas:Maiores correlações: Price vs sqft_living (0.702); sqft_living vs sqft_above (0.877); sqft_above vs Price (0.606)A variável sqft_above é uma das variáveis que foram omitidas na introdução, ela descreve a metragem quadrada da habitação acima do nível do solo, ou seja, o tamanho total da propriedade. É natural que essa variável apresente uma relação forte com o preço, e mais ainda com o tamanho da casa propriamente dito.Podemos concluir, a partir dessa tabela, que o tamanho da casa influencia fortemente seu preço.Agora, vamos analisar as variáveis categóricas, porém, gostaria de lembrar que os nossos dados serão analisados sob duas perspectivas: Em um primeiro momento com o objetivo de compra, e posteriormente com o objetivo de venda.Sendo assim, nada melhor pra guiar a nossa análise do que as perguntas propostas inicialmente:1- Quais casas o CEO House Rocket deveria comprar e por qual preço?Vamos analisar algumas características dos imóveis, e no fim, definir um perfil de compra.QuartosMinha hipótese inicial era que o preço da casa aumentava a medida que o número de quartos aumentava, porém, no gráfico acima vemos que não necessariamente. Os maiores preços são observados em propriedades com 5 e 6 quartos.Os menores preços, por sua vez, estão localizados em imóveis que possuem de 0 a 3 quartos. Apliquei um filtro sob o banco de dados e descobri que essas propriedades custam em média: $449,873.9. Adquirir imóveis com poucos ou nenhum quarto pode ser interessante.BanheirosAs casas aumentam de preço à medida que o número que banheiros aumenta. Aparentemente banheiros são itens valiosos em um imóvel.Para selecionar imóveis com um preço mais atrativo, filtrei propriedades que possuem até 2.5 banheiros. Casas com essa característica custam em média $466,453.9.AndaresSe observarmos o gráfico, veremos que imóveis com 2.5 andares estão levemente acima dos outros, eles possuem uma amplitude maior, e são sim, em média, mais caros que os outros. Um dos fatores que elevou essa média, é o fato da propriedade mais cara da tabela possuir dois andares e meio. O ponto isolado na coluna 2.5 é o imóvel mais caro do nosso banco de dados. Sem distrair do nosso objetivo, casas com 1 andar são as mais baratas e custam em média: $442,180.6.Vista pro marÉ intuitivo presumir que propriedades com vista pro mar são mais valiosas. Uma casa com vista pro mar é, em média 212.63% mais cara que uma casa sem vista pro mar, e custam em média $1,661,876. É um preço bem alto em comparação aos valores que viemos observando anteriormente. Uma casa com vista pro mar, pode até ganhar uma valorização generosa por causa de sua localização, porém, podemos reparar que a propriedade mais valiosa não possui vista para o mar. Ou seja, da casa mais barata até a mais cara, nenhuma possui vista para o mar, e essa distância entre as valorizações é justamente a brecha que procuramos para maximizar os lucros da House Rocket. Portanto, casas sem vista pro mar, são melhores investimentos, pois o preço de compra é mais atrativo, custando em média $531,563.6.VistaImóveis que possuem uma melhor avaliação no quesito vista, também possuem uma valorização maior, essa variável está relacionada com a localização do imóvel, e assim como a vista pro mar, essa é uma variável que está fora do nosso alcance de modificação, então é importante que a escolha nesse quesito seja feita com atenção. Se observarmos, os maiores outiliers (pontos fora da curva), as duas propriedades mais caras, possuem nota 2 e 3, que podem ser consideradas notas media e boa respectivamente. Considerando imóveis, nota 2 e 3, encontraremos imóveis que custam em média $854,571.9.CondiçãoA condição dos apartamentos não parece ser um fator muito definitivo na sua valorização, já que a média dos valores entre as notas parece não variar muito. Logo, adquirir imóveis em condições 1 e 2, ou seja, em péssimas condições não parece ser um bom negócio.Imóveis em condições 3 e 4, consideradas média e boa, possuem as maiores valorizações fora da curva, e custam em média $536,016.NotasImóveis com nota de 1 a 8, considerados de ruim a bom, custam em média $437,284. A partir daí, a valorização começa a aumentar quase de maneira exponencial. Imóveis acima de nota 8 custam em média $959,962.4Tendo essas considerações em mente, vamos traçar um perfil de casa ideais para compra:-Casas que possuem de 0 a 3 quartos;-0 a 2.5 banheiros;-1 andar;-sem vista pro mar;-nota da vista entre 2 e 3;-condições 3 e 4;-nota geral de 1 a 8;Imóveis com essas características possuem valores entre $154,000.0 e $1,500,000.0. Com média $502,808.0.Visando valorização futura, esses são os melhores imóveis para compra.2- Uma vez a casa em posse da empresa, qual o melhor momento para vendê-las, e qual seria o preço da venda?A variável “date”, nos diz respeito a data de venda de um determinado imóvel. Para que a visualização desses dados ficasse mais amigável, classifiquei os meses em 12 fatores, que variam de 1 a 12, e cada número representa o mês de mesma ordem. 1 para janeiro, 2 para fevereiro, e assim recursivamente…DataPodemos observar que existe um grande volume de negociações no mês de maio e nos meses vizinhos. Porém, se avaliarmos os preços, veremos que maio não foi o mês onde ouveram as negociações mais valiosas. Abril, Junho, Agosto, Setembro e Outubro foram os meses mais aquecidos no que diz respeito ao preço das negociações.Vale a pena dar uma olhada detalhada nesses imóveis para observar quais características os deixaram tão valiosos. Como o objetivo da House Rocket é compra e venda, a melhor maneira de agregar valor a essas propriedades é por meio de uma reforma, sendo assim, não vale a pena focar em atributos imutáveis, como CEP, vista pro mar…Nosso objetivo é observar atributos que possam ser modificados, e consequentemente agregar valor, como: Quantidade de quartos, banheiros, andares, e o tamanho geral da propriedade.Na tabela mostrada, podemos observar o perfil das propriedades mais valiosas referente aos meses citados. Na primeira linha, por exemplo, temos uma casa com 5 quartos, 5 banheiros, 8 mil m², 2 andares, avaliada em $5,350,000 e vendida no mês de Abril.A tabela é um catalogo enxuto das principais características dos imóveis mais valiosos desse período.Usando esse perfil como referência, podemos ter uma noção de qual será o preço de revenda quando os imóveis em posse da House Rocket alcançarem um padrão de qualidade fora da curva. Ou seja, casas que possuam de 4 a 5 quartos, 4 a 8 banheiros completos, dois andares, e que tenha no mínimo 5 mil m². Aplicando um filtro com essas especificações no nosso banco de dados podemos observar que casas com esse perfil possuem um preço de venda entre $784,500 e $7,700,000, com média $2,346,534. Comprando imóveis com a média de compra que estimamos na primeira pergunta ($502,808.0), e vendendo pela média de venda estimada, temos uma possibilidade de retorno maior que 300%. Uau! Parece um ganho astronômico, porém, essa simulação não leva em conta os gastos com as reformas e todas as despesas geradas em todo o contexto de um empreendimento imobiliário real.3- A House Rocket deveria fazer uma reforma para aumentar o preço da venda? Quais seriam as sugestões de mudanças e qual o incremento no preço, dado por cada opção de reforma?Sim. Como vimos na pergunta número 2, as reformas são o ponto chave no ganho da valorização de um imóvel. Vamos conferir logo abaixo algumas sugestões de reformas:BanheirosOs banheiros são um belo atrativo em termos de valorização imobiliária. Cada banheiro completo, gera, em média, uma valorização de 59% para o imóvel.QuartosCada quarto gera uma valorização, em média, de 22% no preço total do imóvel.AndaresCada andar gera uma valorização, em média, de 47% no preço total do imóvel.Se for parar pra pensar, o valor agregado de um andar, é menor que o valor de um banheiro novo.ConclusãoÉ importante lembrar que as variáveis avaliadas não são independentes. Portanto, o tamanho do imóvel influencia na quantidade de banheiros, de quartos, andares e vice-versa. A análise também não leva em consideração despesas de nenhuma espécie. É um projeto de estudo, e agora o CEO da House Rocket já sabe onde investir!Written byErick AraújoGraduando em Estatística pela Universidade Federal de Ouro Preto (UFOP)FollowEstatisticaCiencia De DadosAnálise ExploratóriaRMore from Erick AraújoFollowGraduando em Estatística pela Universidade Federal de Ouro Preto (UFOP)More From MediumFast transition between dplyr and data.tableNata Berishvili in Towards Data ScienceVanilla Neural Networks in RChris Mahoney in Towards Data ScienceInterpret Regression Analysis Results using R: Biomedical DataSwayanshu Shanti Pragnya in Analytics VidhyaDonald Trump Won, No Matter What Happens NextJessica Wildfire in The Apeiron Blog(Why) There Was no Biden Landslideumair haque in Eudaimonia and CoThe Election Should Never Have Been This Closeumair haque in Eudaimonia and Co20 Things Most People Learn Too Late In LifeNicolas Cole in Better AdviceThis Is ‘I Wish a Motherf*cker Would’ Week for Black PeopleMarley K. in ZORAAboutHelpLegalGet the Medium app"
Hierarchical Clustering (Bagian 1): Contoh Sederhana Hierarchical Clustering dengan R,https://medium.com/@ikaokay/hierarchical-clustering-bagian-1-contoh-sederhana-hierarchical-clustering-dengan-r-76bf489474a9?source=tag_archive---------5-----------------------,"Hierarchical Clustering,Clustering Analysis,R","Source: Module Doc — Cluster MapAsslamu’alaikum sobat data..Pada kesempatan kali ini kita akan belajar terkait suatu metode pengelompokan yaitu hierarchical clustering yang juga digunakan sebagai salah satu teknik dalam data mining.Tapi sebelumnya, mari kita ketahui bersama apa yang dimaksud hierarchical clustering!Pengertian Hierarchical ClusteringHierarchical Clustering atau Pengelompokan hirarki adalah suatu metode pengelompokan data yang dimulai dengan mengelompokkan dua atau lebih objek yang memiliki kesamaan paling dekat. Kemudian proses diteruskan ke objek lain yang memiliki kedekatan kedua. Demikian seterusnya sehingga cluster akan membentuk semacam pohon dimana ada 6 hierarki (tingkatan) yang jelas antar objek, dari yang paling mirip sampai yang paling tidak mirip. Secara logika semua objek pada akhirnya hanya akan membentuk sebuah cluster. Dendogram biasanya digunakan untuk membantu memperjelas proses hierarki tersebut.Penerapan Hierarchical ClusteringBaiklah, sekarang kita akan membuat contoh sederhana dari pengelompokan hirarki. Berikut adalah contoh data yang digunakan:Source: Chapter 15 — Cluster analysisLangkah pertama adalah meng-install package, tetapi jika di aplikasi R yang sobat miliki sudah tersedia package berikut, maka cukup panggil saja dengan perintah “library()”:install.packages(""cluster"")install.packages(""dendextend"")install.packages(""factoextra"")library(cluster)library(dendextend)library(factoextra)Selanjutnya meng-input data seperti berikut:X1=c(3,4,2,5,1,4)X2=c(2,1,5,2,6,2)data1=data.frame(X1,X2)data1Maka hasil data framenya adalah,karena dari data frame di atas belum memuat nama subjeknya maka akan kita tambahkan dengan cara berikut:colnames(data1)=c(“X1”,”X2"")rownames(data1)=c(“a”,”b”,”c”,”d”,”e”,”f”)data1Sehingga hasilnya adalah,Tahap selanjutnya, kita coba untuk membuat plot data menggunakan perintah:plot(data1)text(data1,rownames(data1))Dan hasilnya adalah,Berdasarkan plot di atas diketahui bahwa data menyebar dan membagi menjadi dua kelompok besar. Dimana kelompok satu yang terdiri dari empat kelompok observasi yaitu a, b, d, dan f. Sedangkan, kelompok dua terdiri dari dua kelompok observasi yaitu e dan c.Berikutnya dalam pengelompokan ini, kita perlu untuk menghitung jarak antar data. Perintahnya adalah sebagi berikut:dist(data1)Maka hasilnya adalah,Setelah mengetahui jaraknya, bisa kita lakuka peng-cluster-an dengan salah satu metode hirarki yaitu single:data1.hc=hclust(dist(data1), “single”)plot(data1.hc,hang=-1)Sehingga hasilnya adalah sebagai berikut,Setelah diperoleh ouput dendogram, maka kita dapat membuat garis cluster-nya sehingga kita bisa melihat anggota dari masing-masing kelompok serta.Perintahnya adalah sebagi berikut:rect.hclust(data1.hc,k=2,border=2:3)data1.cut<-cutree(data1.hc,2)rownames(data1)[data1.cut==1]rownames(data1)[data1.cut==2]Maka hasilnya adalah,Dari dendogram di atas dapat diketahui bahwa dari data yang ada telah dibentuk 2 kluster dengan masing-masing anggota adalah a, b, d, dan f untuk kelompok 1, sedangkan untuk kelompok 2 anggotanya adalah c dan e.Dan berikut adalah hasil dari kelompok yang telah di cut pada dendogram yang menunjukkan output anggota yang sama:Demikian artikel kali ini terkaitcontoh sederhana hierarchical clustering.Sampai jumpa di artikel berikutnya.Terima Kasih dan semoga bermanfaat ya sobat! :)Wassalamu’alaikum Warrahmatullahi Wabarakatuh…Referensi:http://eprints.umpo.ac.id/3039/3/BAB%20II.pdfhttp://www.yorku.ca/ptryfos/f1500.pdfWritten byRizka KhairunnisaFollowHierarchical ClusteringClustering AnalysisRMore from Rizka KhairunnisaFollowMore From MediumClustering Techniques: Hierarchical and Non-HierarchicalM Bharathwaj in Towards Data ScienceFast transition between dplyr and data.tableNata Berishvili in Towards Data ScienceEconomics for Tech People — Equilibrium (Part 3)Tyler Harris in Towards Data ScienceA Study of the Hierarchical Clustering: Unsupervised Machine LearningElias Hossain in Analytics VidhyaHierarchy ClusteringHamza Issa in AI In Plain EnglishHow to use R in Google ColabEd Adityawarman in Towards Data ScienceThree Different Lessons from Three Different Clustering Analyses: Data Science CapstoneMichio SuginooTiming matters in imposing measures for COVID-19Catherine Lopes Ph.D. in Towards Data ScienceAboutHelpLegalGet the Medium app"
N/A,https://medium.com/@h88129/r-sql%E5%B0%87data-frame-insert-ioto%E5%88%B0sql-653f733f01a4?source=tag_archive---------6-----------------------,"R,Sql,Dataframes","紀錄不使用PASTE來將data.frame直接匯入mariadb中基本連接mariadblibrary(DBI);library(RMariaDB)con <- dbConnect(RMariaDB::MariaDB(),username=”root”,password=”toyo123"",group=”my-db”,dbname=”test2"",host=”127.0.0.1"",port=3303)2.建立data.framedf=data.frame(var1=1,var2=2)3.建立多筆資料df=rbind(df,df)4.寫入DBdbWriteTable(con,”test_r”, df, append=TRUE, row.names=FALSE)con：DB的連接口""test_r"":DB表格名稱df:data.frameWritten byAllen個人筆記(https://tw.linkedin.com/in/%E9%A7%BF%E5%AE%8F-%E5%BC%B5-10a847177)FollowRSqlDataframesMore from AllenFollow個人筆記(https://tw.linkedin.com/in/%E9%A7%BF%E5%AE%8F-%E5%BC%B5-10a847177)More From Medium40 Best Ruby Gems We Can’t Live WithoutCodica Team in Codica JournalThe Knights of Functional Programming fight the Imperative Dragon.Sam FareAn Introduction to the Java Memory ModelPrashant Pandey in The StartupA Principled Approach to GraphQL Query Cost AnalysisJames Davis in Dev GeniusUnderstanding Window FunctionsGarrett Edel in The StartupA Combination of TimerTask and TimerHank Lee👨🏻‍💻🇹🇼🇦🇺What Happens After Prisoners Learn to Code?The Atlantic in The AtlanticAWS Key Management Service: All You Need to KnowShilpi Gupta in Better ProgrammingAboutHelpLegalGet the Medium app"
Back-linking of terms in R scripts for use in Obsidian,https://medium.com/@kee.adrian/back-linking-of-terms-in-r-scripts-for-use-in-obsidian-99d43505b6a2?source=tag_archive---------0-----------------------,"Obsidian,Backlink,R","Problem: Although I was able to load my R scripts as markdown files for use in Obsidian (see my previous post), there was no back-linking tags and thus the power of Obsidian, which is in the linkage graphs, is not harnessed. It would be nice if all the terms of my interest in the related R scripts can be linked up in Obsidian fashion.Solution: Just loop through each markdown file and change the terms of interest to include the back-linked syntax. E.g., if the term of interest is mindfulness, changing all occurrences of it to [[mindfulness]] will do the trick. The result is that I can get a node called mindfulness (as seen below), and can check out the identified co-occurrences more easily.The R codes for achieving this is pretty simple.First, create a vector of the file names in the directory containing the Obsidian markdown files.all_files<-list.files(pattern = ‘\\.md$’)Next, define the terms to change (term_to_change) and define how the edited term (edited_term) would look like. The edited_term has the extra square brackets that will make back-linking work. I am interested in the term mindfulness in the example below.term_to_change<-“mindfulness”edited_term<-“[[mindfulness]]”Then, we are ready to loop through each markdown file, grab its content, replace the term_to_change to edited_term accordingly, before finally saving the markdown file with the new contents. Since all_files contains the markdown file names, the write function uses this information directly.for (i in 1:length(all_files)){ data<-readLines(all_files[i]) updated<-str_replace(data, term_to_change, edited_term) write(updated, all_files[i])}The result is that you now have an updated markdown file which has the square brackets added to the term of interest, as shown below. This will facilitate the back-linking in Obsidian.The entire script is shown below.all_files<-list.files(pattern = ‘\\.md$’)term_to_change<-“mindfulness”edited_term<-“[[mindfulness]]”for (i in 1:length(all_files)){ data<-readLines(all_files[i]) updated<-str_replace(data, term_to_change, edited_term) write(updated, all_files[i])}To use this script, place it in the folder where you keep the markdown files for Obsidian. As usual, take extra and necessary CAUTION, such as backing up your files before trying out this script. It changes your files content as the write function is used.Written byAdrian KeeResearcher on mindfulness, interested in coding. I tweet @keefellow. https://orcid.org/0000-0003-2839-0461FollowObsidianBacklinkRMore from Adrian KeeFollowResearcher on mindfulness, interested in coding. I tweet @keefellow. https://orcid.org/0000-0003-2839-0461More From MediumReversing an n-bit number in O(log n) timeEhud Tamir in HackerNoon.comIntro to Computer Science — In 5 Minutes (with python)Tyler WalkerSolve Async Callbacks with FutureBuilder!Sayan Mondal in Flutter CommunityStoring & retrieving PostgreSQL JSON data using GorpAlvin RizkiSet Up Varnish-Cache 4 for WordPressEni Sinanaj in Better Programming3 Ways to Explore a Python ObjectYang Zhou in TechToFreedomWhat’s coming to Rails 6.0?Guy Maliar in Ruby InsideReady, Steady, Connect. Help Your Organization to Appreciate KafkaSPOUD in The StartupAboutHelpLegalGet the Medium app"
Running backs importam? Analisando dados da NFL com R,https://medium.com/@riquecardoso_/running-backs-importam-analisando-dados-da-nfl-com-r-5398bc22456f?source=tag_archive---------0-----------------------,"Data Science,R,NFL,Football,Statistics","A análise de dados é uma tendência que veio pra ficar no mundo dos esportes, em especial no futebol americano. Nas últimas décadas, vimos um crescimento do uso das estatísticas, não apenas nos programas de TV como também com os fans, que utilizam esses dados para chegar em suas próprias conclusões. Fan-pages e também serviços focados em oferecer e tratar esses dados surgiram para aquecer a paixão do fã de esportes.Com o tempo fomos percebendo que algumas dessas métricas de performance não faziam muito sentido e acabavam distorcendo muito a real importância e performance dos jogadores. Os torcedores se dividiram entre a turma que seguia rigorosamente esses dados para formar opinião, e os que preferiam confiar no “teste do olho”: assistir o jogo e julgar criticamente sem mergulhar tanto nos stats.Quem seguia os dados cegamente caia na armadilha de acreditar que “running backs não importam”, que é a ideia de que todos os running backs tem mais ou menos o mesmo valor, como um commodity, pois grande parte da performance dos corredores é por conta do trabalho da linha ofensiva, esquema do ataque, alinhamento da defesa e outros fatores. É verdade que tudo isso importa muito! Porém, quem for realizar a análise pelo vídeo do jogo, observando cada jogada, percebe com facilidade que existe diferença de qualidade entre os RBs, e você ter um jogador de elite nessa posição pode aumentar a sua chance de vencer o jogo.Então os dados mentem?Como o futebol americano é um esporte coletivo, infinitos fatores influenciam no resultado de uma jogada. Os dados simples (públicos) de uma jogada podem te informar se uma corrida deu certo para o ataque, mas não é possível mensurar o quanto disso é responsabilidade do corredor. E simplesmente presumir que todos os corredores são igualmente bons para não ter que lidar com esse problema, é esconder uma dimensão enorme do jogo.A partir de 2018 a NFL passou a realizar o Big Data Bowl, que é uma competição de analytics no Kaggle, para a criação de novas métricas e novas conclusões serem tiradas do esporte, tendo em vista a quantidade enorme de dados que conseguimos extrair hoje em dia. O vencedor de 2019, Matt Ploenzke, acabou sendo contratado para o departamento de Data Science do San Francisco 49ers. A competição liberou dados avançados, como a localização exata, velocidade e aceleração de todos os jogadores no campo, de uma determinada amostra de jogos. O tema para 2019 era criar um modelo preditivo do resultado de uma corrida, dado um retrato do exato momento do snap, e 1 segundo após o snap.O trabalho de Ploenzke primeiro determinou a relevância de cada feature para o resultado da jogada. A distância entre o corredor e o primeiro defensor desbloqueado foi considerado o fator de maior relevância para o sucesso ou fracasso da jogada, assim como a aceleração do corredor ainda no backfield.Por conta de limitações nas regras da competição (não poderiam haver features individuais da performance de cada jogador), o modelo alcançou boa predição nas corridas que resultavam em poucas jardas (até +/- 10j), e perdeu vertiginosamente a capacidade preditiva nas maiores corridas. Isso me fez pensar se existe alguma forma coerente de avaliar running backs com os dados públicos, até mais simples, por meio de pacotes como o nflscrapR.Conhecendo o esporte, a conclusão que eu pude tirar “de olho” é que a corrida depende muito mais de fatores externos ao corredor nas primeiras jardas. Ou seja, do momento do snap, até ultrapassar o box defensivo, a jogada depende muito de fatores como o trabalho de bloqueios da linha ofensiva, as formações do ataque e da defesa, sorte, etc. Já no momento em que o running back ultrapassou o box defensivo e tem apenas o último nível da defesa a sua frente, é nesse momento, acredito eu, que conseguimos mensurar melhor a capacidade de um corredor em formar big plays (jogadas para enorme ganho de jardas). A capacidade do corredor então, se mensura não nas estatísticas da sua produção total, mas na capacidade de transformar ganhos de 8 jardas, em ganhos de 20 ou mais jardas, por exemplo.É lógico que existe sim muito mérito dos corredores no processo de ultrapassar o box defensivo. Nas primeiras jardas, são importantes fundamentos como as leituras que um corredor faz, sua capacidade de quebrar tackles, ganhar velocidade, mudar de direção com rapidez, dentre outros. No entanto, esses fundamentos não acabam sendo refletidos nas estatísticas simples, que foram o meu objeto de estudo. Se eu fosse levar esses importantes fundamentos em consideração, seria preciso avaliar cada jogada minuciosamente e ir adicionando essas features. Minha proposta era tentar chegar numa conclusão eficiente com dados abertos ao público, então realizei algo mais simples.Para avaliar as estatísticas simples, utilizei a linguagem R com o pacote nflscrapR, que retorna um play-by-play com literalmente centenas de informações públicas sobre cada jogada. Irei utilizar a temporada 2019 para essa análise. No final desse artigo estará um link com um baita tutorial para brincar com essas ferramentas.library(tidyverse)library(dplyr)library(na.tools)library(ggimage)pbp <- read_csv(url(“https://github.com/ryurko/nflscrapR-data/raw/master/play_by_play_data/regular_season/reg_pbp_2019.csv""))Primeiro importamos algumas importantes bibliotecas e criamos uma variável “pbp” com todas as jogadas de 2019. Em seguida, filtramos e criamos uma variável “corridas” com apenas as corridas e um número limitado de colunas. Nós vamos solicitar um summary e um histograma das corridas para analisar esses números melhor:corridas <- pbp %>% filter(rush_attempt==1, play_type==""run"") %>% select(desc, rush_attempt, yards_gained, rusher_player_name, rush_touchdown, epa, posteam, td_prob, rusher_player_id)summary(corridas[""yards_gained""])hist(corridas$yards_gained, breaks= 60)#summary output:  yards_gained     Min.   :-15.000   1st Qu.:  1.000   Median :  3.000   Mean   :  4.472   3rd Qu.:  6.000   Max.   : 91.000Histograma da ocorrência do ganho de jardas em todas as corridas da temporada 2019Como pude perceber, apenas 25% das corridas são para 6 jardas ou mais.A seguir, crio uma variavel “corridasMaisDe8” com apenas as corridas que resultaram em mais de 8 jardas. Uso dois tipos diferentes de summary, e ploto um histograma com essas corridas:corridasMaisDe8 <- corridas %>% filter(yards_gained >= 9) summary(corridasMaisDe8[""yards_gained""])hist(corridasMaisDe8$yards_gained, breaks = 60)corridasMaisDe8 %>% group_by(yards_gained) %>% summarize(Corridas = n()) %>% arrange(desc(Corridas))#output summary 1:  yards_gained   Min.   : 9.00   1st Qu.:10.00   Median :12.00   Mean   :15.56   3rd Qu.:17.00   Max.   :91.00Sumário 2: A quantidade de corridas que resultou num determinado número de jardas. (2019)Histograma contendo apenas as corridas que resultaram em pelo menos 9 jardas (2019).Em seguida, crio a minha primeira métrica. Estou rankeando os corredores pelo EPA Médio que eles conseguiram, apenas nas jogadas que resultaram em no mínimo 9 jardas, e considerando apenas os corredores que tiveram pelo menos 20 dessas corridas. EPA significa “Expected Points Added”. Basicamente é uma métrica da eficiência do ataque, que mensura o quanto uma jogada contribuiu para aumentar as chances do ataque de pontuar em uma campanha. Como estamos presumindo que após o passar o box defensivo a qualidade individual do corredor passa a ser um grande diferencial, podemos dizer que o EPA Médio nessas jogadas é mérito do corredor.Se fossemos julgar o EPA Médio considerando todas as jogadas, inclusive aquelas que resultaram em poucas jardas, iriamos regredir todos os jogadores à média, pois haveria uma interferência muito maior daqueles outros fatores que discutimos. Seria menos influenciado pelo mérito individual, e mais pelo mérito coletivo dos dois times, e do acaso.Obs: Nesse ponto do estudo, ainda não filtramos para retirar os Quarterbacks da lista. No último gráfico, plotaremos apenas os números dos running backs.rank <- corridasMaisDe8 %>% group_by(rusher_player_id) %>% mutate(EpaMedio = mean(epa, na.rm = TRUE), TotalEpa = sum(epa, na.rm = TRUE), Time = posteam, TDs = sum(rush_touchdown), ""Jardas Por Carregada""=mean(yards_gained), Corridas=n()) %>% select (Time, rusher_player_name, EpaMedio, TotalEpa, Corridas,  TDs) %>% distinct %>% arrange(desc(EpaMedio),  na.rm = TRUE) %>% filter(Corridas >= 20)rankRanking por EpaMedio, página 1.Ranking por EpaMedio, página 2.Com raras surpresas, vemos uma lista com corredores bem conceituados, o que pode indicar que a minha premissa inicial estava correta. Vamos seguir com a análise dos dados.O EpaMedio parece bem realista, mas também criei uma métrica que pode contribuir mais ainda ao debate: O OpenFieldRatio.O OFR é divisão que tenta responder a seguinte pergunta:Dessas corridas para pelo menos 9 jardas, quantos % delas resultaram em big plays?Para definir um conceito objetivo de big play, utilizei:Corridas que resultaram em no mínimo 20 jardas, OU que resultaram em no mínimo 9 jardas e foram touchdowns.corridas <- corridas %>%  mutate(    CL = if_else(yards_gained >= 9, 1 , 0), #clutch line    BPS = if_else(yards_gained >= 20 | (yards_gained >= 9 & rush_touchdown == 1), 1, 0) #sucesso na big play  )rank2 <- corridas %>% filter(yards_gained >= 9) %>% group_by(rusher_player_id) %>% mutate(EpaMedio = mean(epa, na.rm = TRUE), TotalEpa = sum(epa, na.rm = TRUE), Time = posteam, TDs = sum(rush_touchdown), ""Jardas Por Carregada""=mean(yards_gained), Corridas=n(), OpenFieldRatio = sum(BPS) / sum(CL), TotalBPS = sum(BPS), TotalCL = sum(CL)) %>% select (Time, rusher_player_name, OpenFieldRatio, TotalBPS, TotalCL, EpaMedio, TotalEpa, Corridas,  TDs) %>% distinct %>% arrange(desc(OpenFieldRatio),  na.rm = TRUE) %>% filter(Corridas >= 20)rank2OFR. Página 1.OFR. Página 2.Alguns nomes já conhecidos, com leves mudanças em relação à métrica anterior, mas uma coisa chama bastante atenção:É inadmissível um ranking de Rb que tenha Cristian McCaffrey, discutivelmente o melhor running back da liga, apenas na 3ª página. Isso pode ser explicado pelo fato de ele ter muitas jogadas em que correu mais do que 9 jardas, mas ter ganho menos de 20 jardas na maioria delas.Para entender, olhem a linha do Josh Jacobs. TotalCL seria a quantidade de vezes em que ele correu pelo menos 9 jardas (32 vezes). TotalBPS seria a quantidade dessas vezes que o ganho foi de pelo menos 20 jardas ou um TD (10 vezes). OpenFieldRatio: 10 / 32 = 0.31.Números do McCaffrey: 7/38 = 0.18.É, ficou bastante questionável se o OFR retrata a realidade. O ranking anterior por EPAMedio pareceu mais verossímil.Resolvi então plotar um gráfico com essas duas métricas, para que possamos visualizar melhor e tirar melhores conclusões. Antes disso, criei também uma coluna que tenta “somar” essas duas métricas. Chamei de SuperTrunfo (faltou criatividade, e eu estava com sono). Essa coluna priorizou os running backs que foram bem nas duas métricas, mas não corrigiu as distorções como a do Cristian McCaffrey. Ah, dessa vez retiramos os nomes dos Qbs, pois a intenção é avaliar o trabalho dos running backs. Os Qbs atléticos tem uma vantagem natural para ganhar jardas corridas, pois são um jogador a mais para a defesa se preocupar no jogo terrestre. Muitas vezes acabam conseguindo mis-matches táticos e arrumando jardas com facilidade. Bom, se você está lendo até aqui, provavelmente já sabe disso.rank3 <- corridas %>% filter(yards_gained >= 9) %>% group_by(rusher_player_id) %>% mutate(EpaMedio = mean(epa, na.rm = TRUE), TotalEpa = sum(epa, na.rm = TRUE), Time = posteam, TDs = sum(rush_touchdown), ""Jardas Por Carregada""=mean(yards_gained), Corridas=n(), SuperTrunfo = (sum(BPS) / sum(CL)) + (EpaMedio / 10), OpenFieldRatio = sum(BPS) / sum(CL), TotalBPS = sum(BPS), TotalCL = sum(CL)) %>% select (Time, rusher_player_name, SuperTrunfo, OpenFieldRatio, TotalBPS, TotalCL, EpaMedio, TotalEpa, Corridas,  TDs) %>% distinct %>% arrange(desc(OpenFieldRatio),  na.rm = TRUE) %>% filter(Corridas >= 20, !(rusher_player_name %in% c(""K.Murray"", ""M.Ingram"", ""L.Jackson"", ""D.Watson"", ""J.Allen"", ""Jos.Allen"")), !(rusher_player_name == ""K.Drake"" & Time == ""MIA""))rank3Considerando as duas métricas. Página 1.Considerando as duas métricas. Página 2.Para plotar o gráfico:ggplot(rank3, aes(x=EpaMedio, y=OpenFieldRatio)) +   geom_text(aes(label=rusher_player_name), size= 3) +  labs(x = ""EPA Médio (corridas acima de 8 jardas)"",  y = ""Open Field Ratio"",  caption = ""Dados de nflscrapR (Feito por Henrique Cardoso @5adescida)"",  title = ""Rbs nas jogadas acima de 8 jardas"",  subtitle = ""2019"")  ggsave('GraficoRBs.png', dpi=3000)Quais as maiores surpresas? Podemos ver alguns jogadores como Gus Edwards em boa posição nas duas métricas! Eu não esperava. Vou olhar com atenção para a temporada do Baltimore Ravens esse ano. A excelente posição do Miles Sanders reflete bem o quão subestimado ele realmente é.No entando, é estranho ver um excelente jogador como Ezequiel Elliott sendo o pior running back do grupo. Zec é elite. Quebra tackles e luta por jardas adicionais com muita facilidade. Olhando os números, ele foi o jogador que mais conseguiu ter ganhos de pelo menos 9 jardas: 49! Dessas 49 vezes, apenas 5 ele conseguiu um TD ou correu pra mais de 20 jardas. A conclusão que eu chego é: Zec é elite, mas talvez não nessas big plays. Ele é muito elite correndo as primeiras jardas das corridas (aquelas que essas métricas simples jamais irão conseguir mensurar). De todos os running backs da liga, ele foi o que mais teve corridas de pelo menos 9 jardas.A conclusão que podemos chegar é que os números mostram muita coisa, mas é preciso contextualizar e entender o que houve. É preciso entender como são calculadas as métricas, entender quais são os seus pontos cegos, quais os fundamentos mais importantes do esporte e quais podemos mensurar com os dados.Running backs importam sim! Possuem caracteristicas muito diferentes, e são qualitativamente diferentes. Essas métricas que eu produzi estão longe de decretar qualquer veredicto ou de serem verdades absolutas, mas já são bem melhores do que simplesmente considerar o número total de jardas (o que acaba sendo um stat completamente enviezado) ou o próprio EPA Médio, mas considerando todas as jogadas, inclusive as que resultaram em poucas jardas.Se você quiser aprender mais sobre o nflscrapR, e como realizar esses estudos, esse tutorial é ótimo. Se quiser aprender mais sobre o que fazer, e principalmente o que NÃO fazer com stats de futebol americano, o NoFlags tem três artigos excelentes sobre o tema (1, 2 e 3).Você consegue baixar todos os arquivos que precisa para reproduzir esse estudo no meu GitHub.Seria incrível debater sobre essas métricas! Discorde de mim mandando uma mensagem no meu Linkedin. Se existir uma forma melhor de calcular tudo isso, eu adoraria saber.Continue em casa, use máscara e sempre tenha álcool em gel.Forever Faithfull.Written byHenrique CardosoEstudante de Machine Learning e Inteligência ArtificialFollow1 1 1 1 1 Data ScienceRNFLFootballStatisticsMore from Henrique CardosoFollowEstudante de Machine Learning e Inteligência ArtificialMore From MediumThe Military Is Back in BrazilForeign Policy in Foreign PolicyLewagon — Learning how to code in Rio de JaneiroStuart BoyleWhat Does a Developer Look Like?Allison Chow in Code Like A GirlLab 2— Javascript IntroductionChristian Grewell in applab 2.04 Habits That Make You an Inefficient DeveloperDaan in Better ProgrammingVanilla Neural Networks in RChris Mahoney in Towards Data ScienceFast transition between dplyr and data.tableNata Berishvili in Towards Data ScienceFake News Classification with Recurrent Convolutional Neural NetworksAmol Mavuduru in Towards Data ScienceAboutHelpLegalGet the Medium app"
Build Stock Price Database With R Website Crawling & MYSQL,https://medium.com/data-room/build-stock-price-database-with-r-website-crawling-mysql-2948dec80c5a?source=tag_archive---------1-----------------------,"Stocks,Database,Data Analytics,Crawling,R","Why build my own stock price database?Limitations of the environmentCompared with American stock price research, there are no R packages to get Taiwanese stock price data. If you want to do research without many open data sources, this article might provide you an idea.On the other hand, we query the stock price data from the TWSE website (Taiwan Stock Exchange Corporation) which treats website crawling strictly. Collecting historical data on local is necessary because few requests per minute and per day are allowed.2. The historical stock price for backtestingBacktesting is the most important thing for quantitative financial analysis.Backtesting requires lots of historical data to test your model performances and make further predictions. The database is the best choice to store and sort the huge amount of data.3. Combine with other data to do deep analysisMost investors analyze stock prices in three perspectives, technical, fundamental, and chip analysis respectively.Multiple data sources can optimize the model performances. Doing fundamental analysis requires monthly revenue, but chip analysis demands the institutional investors’ trading records. The database can store multiple data sources at the same place.The structure of my databaseThe current database contains six tables.Stocks_Categories: Including stock Id and their industries categories.History_Prices: Data from Yahoo Finance API. Including open, high, low, close (OHLC), adjusted close prices, and volume.Monthly_Revenue: Including the monthly revenue of each stock.History_Prices_TWSE: The OHLC, last offered prices(which didn’t make a deal), and the amounts of deals.Institutional_Investor_Trading_TWSE: The institutional investors’ trading records of each stockHistory_Index_TWSE: Including Taiwanese industrial indexes, and indexes of top tier Taiwanese companies’ performances which share similar concepts with S&P 500.There are two stock prices from Yahoo Finance API and TWSE site respectively. Although there is huge homogeneity between two data sources, they are complementary to each other. It’ difficult to get the whole historical prices from the TWSE site, due to the restricted limit of requests. On the other hand, yahoo finance lacks the price data of some stocks, but the TWSE site has all stocks’ today prices.There are rooms for improvements for this database. I kept almost all the columns from the original sources, but some calculated metrics are redundant in the database. Besides, adding more different data about stocks is important to consummate models.Using R crawls the data & writes into the databaseData Roomdata analysisFollowStocksDatabaseData AnalyticsCrawlingRWritten bylalaLyn TheaterFollow介紹世界各地的舞台劇與電影。最近主題為歌舞伎。FollowData RoomFollowLearning on data and sharing my projectsFollowWritten bylalaLyn TheaterFollow介紹世界各地的舞台劇與電影。最近主題為歌舞伎。Data RoomFollowLearning on data and sharing my projectsMore From MediumLevels of MeasurementsRiteshpratap A. Singh in The StartupHow people talk about marijuana on Reddit: a natural language analysisSara Robinson in HackerNoon.comSave a Neural Net, Use a Linear ModelODSC - Open Data Science in PredictCheck out My MixtapeHarmit Sampat in VisUMDMinimally Sufficient PandasTed Petrou in Dunder DataWish your team paid more attention to data?Atlantic 57 in Insights from Atlantic 57How to Manage Big Data With 5 Python LibrariesSeattleDataGuy in Better ProgrammingMismatch Between Academic and Real-world Data Science ProjectsBenjamin Obi Tayo Ph.D. in Towards AILearn more.Medium is an open platform where 170 million readers come to find insightful and dynamic thinking."
Creating Visualization with R ggplot2,https://medium.com/@putik.dhira/creating-visualization-with-r-ggplot2-f6584b400774?source=tag_archive---------2-----------------------,"R,Ggplot,Ggplot2,Charts,Visualization","Photo by Markus Winkler on UnsplashIn the previous article about automating your report with R, you can add some charts on your report to make even more comprehensive visualization. This article will give some examples on how to use the ggplot2 package to create some useful charts. First thing first, you need to load ggplot2 package to your workspace. Similar to previous articles, we will use the financial sample dataset.Library(ggplot2)Library(dplyr)Library(scales)Line Chart / Time SeriesFirst, we need to summarize the data to become a daily data view.sales_day <- financial_sample %>% group_by(Date) %>% summarise(daily_sales = sum(Sales))Then, we can use below script to plot the line chart.theme_set(theme_classic())ggplot(sales_day, aes(x=Date)) + geom_line(aes(y=daily_sales)) + labs(title=”Daily Sales Chart”, subtitle=”Total of Daily Sales”, caption=”Source: Financial Sample”, y=”Sales”) + scale_y_continuous(name=”Sales”, labels = comma)2. Bar ChartFirst, we need to summarize the data to become a daily data view per product.sales_day_p <- financial_sample %>% group_by(Date, Product) %>% summarise(daily_sales = sum(Sales))Then, we can use below script to plot the stacked bar chart.product_plot <- ggplot(sales_day_p, aes(Date,Product))product_plot + geom_bar(aes(fill=Product,x = Date, y = daily_sales), stat=”identity”, width = 20) + labs(title=”Financial Sample”, subtitle=”Daily Sales of Products”, caption=”Source: Financial Sample”) + theme(axis.text.x = element_text(angle=65, vjust=0.6)) + scale_y_continuous(name=”Sales”, labels = comma)3. Pie ChartWhen we try to visualize a categorical product share, a pie chart is one of the best ways to visualize it. First, we need to summarize the data to total profit per product, and add a column in order to put them as label.profit_product <- financial_sample %>% group_by(Product) %>% summarise(total_profit = sum(Profit))profit_product$Label <-paste(round(profit_product$total_profit/1000000,2),”M”)Then, we can use below script to plot the pie chart.p <- ggplot(profit_product, aes(x = 1, y = total_profit, fill = Product)) + geom_bar(stat = “identity”)p + coord_polar(theta = ‘y’) + theme_void() + geom_text(aes(label = Label), position = position_stack(vjust = 0.5)) + labs(fill=”Product”,title=”Pie Chart of Profit by Product”,caption=”Source: Financial Sample”)Written byPutik DhiraramantiBusiness Intelligence — Product ManagementFollowRGgplotGgplot2ChartsVisualizationMore from Putik DhiraramantiFollowBusiness Intelligence — Product ManagementMore From MediumHow to use Iterrows in PandasHamilton Chang in Python In Plain EnglishHEARTCOUNT Quick GuidelineSidney @HEARTCOUNT in HEARTCOUNTPrinciples of PlotlyJenny Dcruz in The StartupAutomated e-Learning Content Creation with Web Scraping and NLPErdem IsbilenThe Data Visualization PolarityMike Raper in NightingaleYour Microsoft Excel Skills Need WorkPendora in High FinanceHow Did I Get Started With Machine Learning?Moeedlodhi in The StartupFast.ai Practical Data Ethics lesson 5.1 notes-The problem with metricsRisto Hinno in The InnovationAboutHelpLegalGet the Medium app"
Predicting the Nominal GDP using Economic Indicators: A Data Science Approach,https://towardsdatascience.com/predicting-the-nominal-gdp-using-economic-indicators-a-data-science-approach-7c56cded782?source=tag_archive---------0-----------------------,"Data Science,Economics,Machine Learning,Artificial Intelligence,R","Photo by Kevin Ku on UnsplashThis article talks about forecasting nominal GDP (Gross Domestic Product) using data present over the web. The problem statement was to gather data from authentic sources, perform an Exploratory Data Analysis (EDA), train a model and predict the Nominal GDP (Canada). As an IT person, I had little knowledge about these economics-associated terms. That is why the first thing to do was to get familiar with these concepts. Next was to learn and execute this project in R programming language and get acquainted with popular R packages such as tidyverse, ggplot, caret and others. In practice, good use of online resources makes it easy to get accustomed to R and its syntax.Stepping into the task, I came across various economic statistics aka indicators, classified as lagging and leading indicators. Economic indicators are something which may have a direct impact on the Nominal GDP of a country. Our approach here was to use these indicators to predict the Nominal GDP of Canada. After researching these indicators from various online blogs, articles and papers, the following indicators were finalized:PopulationRefugee PopulationReal Interest RateNumber of Domestic CompaniesTravel Services ( % of Import Services -BoP)Tax RevenueHousing MarketLabor ProductivityGovernment Bond Yield 10yr ratePersonal Remittances (Received & Paid in USD)Passengers carried by RailwaysPassengers carried by Air TransportsInflationIncome GrowthUnemployment RateGovernment DeficitConsumer Price IndexCommodity Price IndexEUR to CAD conversion rateUSD to CAD conversion rateToronto Stock Exchange traded valueToronto Stock Exchange traded volumeIncoming International TouristsData was fetched from sources like Statistics Canada, World Bank, Statista and others. These indicators were assumed as features and data was collected for these features between from year 2009 to 2018. The data set was divided into two parts, a traning set and a testing set.Training Set — 2009 to 2016esting Set — 2017 and 2018Exploratory Data AnalysisAfter the dataset was assembled, cleaned and neatly arranged, exploratory data analysis for the data set was started. Data normalization was done using the probability density dnorm function for a standard normal distribution. To check whether the data is normal, Shapiro-Wilk Normality Test was applied on each feature. All variables except 3 were having their p-value greater than 0.05, which indicated that they are normal. In order to analyze the correlation between the dataset features, findCorrelation function of caret package was utilized and highly correlated attributes were removed. As the data points were less i.e. 8 (2009–2016), the dimensionality of the data had to be reduced. This was done by applying Principal Component Analysis (PCA) using prcomp function. At this point, EDA was concluded. Let's proceed with modelling now! Refer the GitHub link…Click here!Linear Regression ModellingIn this type of modelling, PCA values were used to train the simple linear regression model using the lm() function. The resultant model was summarized using the summary function. The key observation was that the p-value was less than the significance level (< 0.05). This means that I could safely reject the null hypothesis as co-efficient β of the predictor is zero. Furthermore, the F-statistic value of the model was 428.6 (more the better). Hence, it was concluded that the model was statistically significant. To validate the model, I tested it on the values of 2017 and 2018. The overall accuracy of the model was 98.31%. A table at the end of the article provides more details about the experiments.Random Forest ModellingFor this modelling, randomForest library of R language makes it easier to select the features and create a random forest model. The importance() function finds and displays the importance of each feature according to the data leaf node impurity. These were stored in a new dataset which was the sorted on basis of the importance values. The trick here is not to select the features whose importance values are only higher or lower, but to select values to neutralize the spectrum i.e. combination of higher, lower and mid importance values to maintain the balance. Six features were picked using trial and error method.Personal Remittances (Received) Real Interest Rate Travel Services Imports  Government Bond Yield 10yr rateGovernment DeficitNumber of Domestic CompaniesThe model was trained by using the randomForest function. mtry parameter was set to 6 (no of features) and number of trees (ntree) to 1000. The model forecasted the 2017 and 2019 NGDP with an average accuracy of 95.68%.Support Vector Machine ModellingThis was the final one on our experiment list. In order to utilize the SVM modelling in our use, we used the e1071 package of R that provides the svn function to model. As our use-case was not a classification one, the regression option had to be applied. For this, the svm() function provides a type parameter which enables us to opt in our choice. Accordingly, the eps-regression type was opted and kernel parameter was set to radial. For more details on kernels, click here. The average accuracy forecasted here was 98.18%. The graph and table below provide more details about the experiment.Chart displaying the actual vs predicted valuesTo conclude, despite having a small data set, all models predict with 95% + accuracy. As the linear model is trained using the PCA values, the resulting output is better than other. Whereas, the Random Forest can be considered to be least reliable as it uses minimal features of the dataset. If not the accuracy or reliability, the results at least advocate the significance of economic indicators in the prediction of Nominal GDP.PS: This experiment was a part of the submission for Statistics Canada Business Data Scientist Challenge 2019/2020.Written byDamian Diago D’monteMaster's in Computer Science - University of New BrunswickFollowSign up for The Daily PickBy Towards Data ScienceHands-on real-world examples, research,  tutorials, and cutting-edge techniques delivered Monday to Thursday. Make learning your daily ritual. Take a lookGet this newsletterBy signing up, you will create a Medium account if you don’t already have one. Review our Privacy Policy for more information about our privacy practices.Check your inboxMedium sent you an email at  to complete your subscription.Data ScienceEconomicsMachine LearningArtificial IntelligenceRMore from Towards Data ScienceFollowA Medium publication sharing concepts, ideas, and codes.Read more from Towards Data ScienceMore From Medium5 YouTubers Data Scientists And ML Engineers Should Subscribe ToRichmond Alake in Towards Data Science7 Must-Haves in your Data Science CVElad Cohen in Towards Data Science21 amazing Youtube channels for you to learn AI, Machine Learning, and Data Science for freeJair Ribeiro in Towards Data Science30 Examples to Master PandasSoner Yıldırım in Towards Data ScienceThe Roadmap of Mathematics for Deep LearningTivadar Danka in Towards Data ScienceAn Ultimate Cheat Sheet for Data Visualization in PandasRashida Nasrin Sucky in Towards Data ScienceHow to Get Into Data Science Without a DegreeTerence S in Towards Data Science4 Types of Projects You Must Have in Your Data Science PortfolioSara A. Metwalli in Towards Data ScienceAboutHelpLegalGet the Medium app"
Merubah Tema di RStudio,https://medium.com/@sumadireja/merubah-tema-di-rstudio-dca8acf1435?source=tag_archive---------1-----------------------,"Rstudio,R Shiny,R,2020,Indonesia","Assalamualaikum..Hallo temen-temen semua, apa kabar nih kalian semua? Saya harap di situasi sekarang ini temen-temen selalu dalam keadaan baik,ya! Tetap jaga kesehatan. Jika harus keluar rumah tetap gunakan masker, ya, jangan lupa untuk selalu cuci tangan juga setelah berpergian.Kali ini saya akan membuat tutorial sederhana tentang bagaimana merubah tema pada R Studio.Source From rstudio(dot)comRstudio itu sendiri merupakan Integrated Development Environment (IDE) open souce dan gratis untuk R, bahasa pemrograman untuk komputasi statistik dan grafik. Rstudio didirikan oleh JJ Allaire, pencipta bahasa pemrograman ColdFusion.Rstudio tersedia dalam dua versi: Rstudio desktop, dimana program dapat dijalankan secara lokal sebagai aplikasi desktop biasa; dan Rstudio server, yang memungkinkan mengakses Rstudio menggunakan browser web saat sedang berjalan di server Linux jarak jauh.Rstudio tersedia dalam edisi open source dan komersial serta dapat berjalan pada OS (Windows, macOS, dan Linux) atau di browser yang terhubung ke Rstudio Server atau Rstudio Server Pro. Rstudio sebagian ditulis dalam bahasa pemrograman C++.Kali ini saya akan menjelaskan Rstudio dengan versi open source yang berjalan pada OS Windows 10. Berikut tutorialnya..Secara default Rstudio memiliki tema dengan cenderung berwarna putih, bagi sebagain orang termasuk saya kurang suka dengan tampilan yang cenderung berwarna putih, karena tidak begitu nyaman untuk mata. Maka dari itu saya merubahnya menjadi warna yang enak untuk dipandang dan tidak membuat perih mata. Hal pertama yang perlu dilakukan ialah buka Rstudio kamu. Setelah itu klik bagian Tools > Global Options…Setelah itu klik bagian Appearance dan pilih Editor theme. Disana banyak pilihan tema yang dapat kamu gunakan. Namun kali ini saya ingin menggunakan tema night owl. Untuk caranya kita pergi ke link berikut ini.Setelah itu download yang bagian night-owlish.rstheme. Jika sudah balik lagi pada Rstudio. Lalu, klik button add dan pilih file night-ownlish yang telah kamu download tadi.Jika sudah langkah terakhir tinggal pilih apply > ok. Dan tadaaa sudah terganti temanya menjadi yang lebih nyaman dipandang.Cukup sekian, terimakasih temen-temen sudah mampir. Semoga lebih semangat ya ngoding menggunakan Rstudionya.😊Jika menurutmu ini bermanfaat tolong share ke temen-temen kamu juga, ya. Semoga membantu!Written byImron SumadirejaHallo! Terimakasih sudah mampir.😊Follow2 2 2 RstudioR ShinyR2020IndonesiaMore from Imron SumadirejaFollowHallo! Terimakasih sudah mampir.😊More From MediumExploring Undernourishment: Part 6 — Research Area 3: Surprising TrendsChris Mahoney in The StartupExploring Undernourishment: Part 7 — Research Area 4: Most Influential IndicatorChris Mahoney in The StartupFast transition between dplyr and data.tableNata Berishvili in Towards Data SciencePowerBI vs. R Shiny: Two Popular Excel Alternatives ComparedDario Radečić in Towards Data ScienceThere is an R in ReproducibilityDr Andreas Ochs in Towards Data ScienceWeb Scraping With R — Easier Than PythonDario Radečić in Towards Data ScienceInterpret Regression Analysis Results using R: Biomedical DataSwayanshu Shanti Pragnya in Analytics VidhyaTableau vs. R Shiny: Which Excel Alternative Is Right For You?Dario Radečić in Towards Data ScienceAboutHelpLegalGet the Medium app"
N/A,https://medium.com/psicodata/saia-do-spss-e-venha-para-o-r-7cdfdeac8a24?source=tag_archive---------2-----------------------,"R,Newsletter,Psicodata","No mês de agosto a linguagem R dominou o PsicoData!Você perdeu alguma coisa?! Não fique triste, eu vou resumir para você todos os conteúdos:Se você não sabe por onde começar, respira e vem conferir comigo esse post sobre como iniciar no R!Se você é daqueles mais adiantados e quer aprender a manipular dados com R, também temos conteúdos para você:Transformando colunas em linhas e outras loucuras com tidyrValores missing — Parte 1Valores missing — Parte 2Ganhe liberdade aprendendo a ler funçõesOk, você já sabe muitas coisas que dá para fazer com o R, vamos tentar algum mais avançado? Que tal corrigirmos uma escala psicométrica e obter insights incríveis sobre os seus dados? 😍Preparamos um tutorial de como analisar uma escala psicométrica utilizando o R, clique aqui.Esse mês de agosto foi incrível! E o que será que vem por ai no PsicoData? 👀Convide seus amigos a assinar a nossa newsletter pelo link abaixo:PsicoDatapsicodataFollow3 Sign up for PsicoData NewsletterBy PsicoDataFique por dentro das nossas publicações! Take a lookGet this newsletterBy signing up, you will create a Medium account if you don’t already have one. Review our Privacy Policy for more information about our privacy practices.Check your inboxMedium sent you an email at  to complete your subscription.Thanks to Gabriel Rodrigues. RNewsletterPsicodata3 claps3 clapsWritten byDalton CostaFollowEstudante de psicologia e ciência de dadosFollowPsicoDataFollowUma plataforma colaborativa em português sobre ciência de dados e psicologia.FollowWritten byDalton CostaFollowEstudante de psicologia e ciência de dadosPsicoDataFollowUma plataforma colaborativa em português sobre ciência de dados e psicologia.More From MediumTá tudo junto: média, variância e desvio-padrãoGabriel Rodrigues in PsicoDataDados e Psico: 5 lições que eu gostaria de ter ouvido no inícioPaula Costa in PsicoDataO que é uma variável latente e como medi-la? — Parte 2Gabriel Rodrigues in PsicoDataSimples e Direto: um guia de visualização de dados com PythonDalton Costa in PsicoDataBaixando e processando dados do DATASUS sobre suicídio com PythonDalton Costa in PsicoDataO que é uma variável latente e como medi-la? — Parte 1Gabriel Rodrigues in PsicoDataUm gráfico vale mais que mil palavras!Marcela Alves Sanseverino in PsicoDataValores missing — Parte 2Gabriel Rodrigues in PsicoDataLearn more.Medium is an open platform where 170 million readers come to find insightful and dynamic thinking."
N/A,https://medium.com/nuances-of-programming/%D0%BF%D1%80%D0%B8%D0%BA%D0%BB%D1%8E%D1%87%D0%B5%D0%BD%D0%B8%D1%8F-%D0%B0%D0%BD%D0%B0%D0%BB%D0%B8%D1%82%D0%B8%D0%BA%D0%B0-%D0%B2-%D1%81%D1%82%D1%80%D0%B0%D0%BD%D0%B5-%D0%BA%D0%BE%D0%B4%D0%B0-%D0%BF%D1%80%D0%BE%D0%B1%D1%83%D0%B6%D0%B4%D0%B5%D0%BD%D0%B8%D0%B5-%D1%81%D0%B8%D0%BB%D1%8B-415488c5ce04?source=tag_archive---------0-----------------------,"Learn To Code,R,Sql,Spreadsheets,Nuances Of Programming","Программы для электронных таблиц, такие как Microsoft Excel и Google Sheets, превосходны. Они вне конкуренции, когда дело касается выполнения простых вычислений или построения финансовых моделей.Однако наступает момент, когда вы должны отказаться от электронных таблиц и найти другое решение. Вы не представляете, сколько разных отвратительных отчетов по электронным таблицам попадалось мне в свое время! Как только вы начинаете использовать таблицу в качестве отчета, базы данных и инструмента для их преобразования, это сигнал — надо остановиться! Вы зашли слишком далеко. Такие электронные таблицы не что иное, как страшный сон для обслуживания и отладки.А есть ли альтернатива? Конечно — научиться писать код для анализа данных. Нет никаких причин бояться кода. Если вы уже в курсе, как использовать программу для таблицы, то не составит труда провести аналогии и понять суть этого процесса. Я уже курировал нескольких аналитиков на начальных этапах их разработок и убедился, что подход из этой статьи работает стабильно. Начнем!Предварительное условие — изучить электронные таблицы для анализа данныхЕсли вы еще не знаете, как использовать программу для электронных таблиц, то это надо исправить. Именно на данном этапе вы познакомитесь с понятиями, связанными с данными, и безболезненно вольетесь в процесс написания кода.Старт с нулевым опытомЕсли у вас нулевой опыт в этой области, то советую пройти бесплатный курс по электронным таблицам в YouTube. Учитывая их разнообразие, просто выберите один и посмотрите полностью.Старт с позиций опытаЕсли у вас есть какой-либо опыт, тогда нужно практиковаться до тех пор, пока не обретете уверенность в работе с электронными таблицами. В интернете доступно множество наборов данных с открытым исходным кодом. Придумайте 5 действий с данными и выполните их, используя эти наборы. Примеры возможных действий:“Какое среднее значение в столбце X?”“Сколько пустых клеток/содержат пропущенные значения в столбце X?”“Отсортируйте информацию в столбце X в порядке убывания”.“Создайте линейную диаграмму столбца X”.“Какое значение столбца X для каждого значения в столбце Y?”Изучаемые понятияНезависимо от имеющегося опыта, ваши приключения в стране электронных таблиц будут сопровождаться важными понятиями, относящимися к процессу написания кода для данных.Формулы могут применяться к отдельным ячейкам или к группе ячеек. Это подобно понятию “векторизации”, когда мы применяем операции ко всем массивам данных.Данные могут обобщаться в процессе агрегирования. Сводные таблицы научат нас, что значит подсчитывать или суммировать столбцы по группам, определенным в других столбцах.Визуализации — мощный инструмент. Сводные диаграммы позволяют вам понять, какие типы диаграмм представляют какие типы данных.Два набора данных могут быть “объединены”. Это мы узнаем, используя силу формул, таких как ‘VLOOKUP’ (функция ВПР).Как только вы набили руку в работе с программами для электронных таблиц, значит пришла пора переместиться в страну кода.От электронной таблицы к кодуВыбираем язык программированияПользователи Microsoft Office могут подумать:“Я активно пользуюсь Excel, к тому же слышал про ‘VBA’. Говорят, его код можно использовать для автоматизированных отчетов Excel.Ни в коем случае! Забудьте про VBA. Поверьте на слово тому, кто когда-то написал на нем тонны строк кода. Лучше освоить “передаваемый навык”, т.е. навык, которым вы сможете заинтересовать следующего работодателя, независимо от того, пользовался ли он Microsoft Office или нет.Учитывая, что у вас нет опыта программирования, предлагаю сосредоточиться на SQL и R.Для изучения SQL не надо большого ума. Это широко распространенный язык реляционной базы данных. Вот его вы и должны выучить.А почему R? Почему не Python? Потому что он разработан для анализа данных! Так как этот язык проектировался с определенной целью, он позволяет больше времени уделять именно анализу данных, а не борьбе с абстрактными аспектами языков программирования. Например, R поставляется со встроенными наборами данных, которые сразу можно анализировать. CSV-файлы могут быть импортированы в одной строке при помощи ‘read.csv()’. Вы можете создавать (неприглядные) гистограммы, используя ‘hist()’, а также диаграммы рассеяния и многое другое при помощи ‘plot()’. Дело в том, что все эти встроенные возможности R позволяют приступить к анализу данных сразу же после его установки.Хочу, чтобы вы меня правильно поняли. Я не утверждаю, что вы не должны учить Python. Сам я его обожаю и когда-то сделал выбор в его пользу. И вам обязательно надо его выучить, только не сейчас. Я руководствуюсь тем, что, освоив сначала R, вы сразу же получите успешный опыт работы с кодом. А эти маленькие достижения будут поддерживать в вас решимость, мои “юные падаваны”, овладеть светлой стороной силы под названием написание кода для данных.По этим языкам так много информации. На каких аспектах сосредоточиться?Пойдем на поводу нашего прагматизма и сосредоточимся на изучении тех аспектов, которые пригодятся в работе аналитика. А чтобы их обозначить, будем следовать простой логике:1. Составьте список всех отчетов на основе электронных таблиц, которые вы регулярно обновляете. Добавьте к нему специальные фрагменты анализа, выполненного на основе таблиц за последние полгода.2. Отдохните 5 минут после такой утомительной работы!3. Откройте одну из таблиц в вашем списке.4. Установите таймер на 10 минут.5. Во втором списке сначала отметьте формулы, которые вы использовали в отчете, затем созданные вами визуализации, а также числа, полученные на основе сводных таблиц. Мыслите как можно шире! Не нужно конкретизировать, как вы выполняли ‘VLOOKUP’, используя точные соответствия в столбце B и возвращая значения в столбец F. Просто отметьте факт выполнения этой функции. Рядом с каждой формулой напишите, сколько раз она встречалась в таблицах. Продолжайте работу до истечения установленного времени или переходите к следующей таблице, если 10 минут истекли.6.Возьмите пятиминутный перерыв и переходите к следующей таблице в списке.Анализируйте таким способом все таблицы в списке до тех пор, пока не закончите подсчеты. Отсортируйте ваш список в порядке убывания в зависимости от того количества раз, которое каждая формула/визуализация/значение сводной таблицы появлялись в отчетах и анализах. Этот порядок обозначит главные направления, которыми вы будете руководствоваться при организации своего дальнейшего обучения!Как же организовать обучение?Давайте разделим процесс обучения на два двадцатиминутных занятия (две “помидорки” согласно терминологии практикующих метод “помидора”). Закончите эти два занятия перед работой! Ведь все мы устаем после тяжелого рабочего дня. Если постоянно переносить занятия на более позднее время в течение дня, то вероятность их выполнения будет стремиться к нулю.Первые 25 минут мы сосредотачиваемся на изучении R:Обратите внимание на первый объект в списке и изучите, как его реализовать в R. Сначала научитесь делать это при помощи дружественного нам пакета dplyr. Если не получилось, расширьте поиски и найдите способ реализовать этот объект, используя R в целом.Если у вас есть наборы данных с работы, которые можно использовать, то смелее — сделайте это. В противном случае обратитесь к встроенным наборам данных R или наборам данных с открытым исходным кодом в интернете.По истечении 25 минут спросите себя, можете ли вы свободно применять этот навык. Если ответ положительный, зачеркните первый пункт в списке, а завтра переходите к следующему. Если вы испытывали трудности в процессе, то дальнейшее освоение навыка можно отложить на завтра.Следующие 25 минут посвятим изучению SQL:Пройдите бесплатный курс SQLZOO.Окончив курс, импортируйте данные в онлайн среду SQL и продолжайте работу по агрегированию и объединению ваших таблиц. Ни в коем случае не загружайте туда данные, связанные с работой.Как только вы научились свободно агрегировать и объединять таблицы, приступайте к освоению оконных функций.А добившись свободного владения и этим навыком, вы можете прекратить изучение SQL и заменить 25 минутное занятие по SQL на R.Достигнув свободного владения R и SQL, подумайте, как вы можете применить эти знания в своей работе. Чувствуете мощь той силы, которую вы обрели над данными, научившись писать код?Работа со списком закончена! Что дальше?Продолжайте обучение и овладевайте новыми навыками! Развивайтесь! Жизнь не ограничивается лишь выталкиванием списков данных и выполнением отчетов!Откройте для себя ещё более удивительные знания. Например, прочитайте книгу “Язык R в задачах науки о данных” Гаррета Гролемунда и Хадли Уикмена, легенды R. Узнайте, что такое Kaggle. Подпишитесь на R-bloggers и учитесь у своих коллег-пользователей R.Изучайте R ежедневно на протяжении 6 месяцев. По мере освоения R переходите к изучению Python. Знание двух языков программирования станет хорошей стартовой площадкой для погружения в него.ЗаключениеНет лучшей награды для преподавателя, чем видеть, как его ученики прошли путь развития от аналитиков, работающих с данными электронных таблиц, до аналитиков с навыком написания кода. К сожалению, я не могу лично сопровождать вас в процессе вашей профессиональной трансформации! Но надеюсь, что материал статьи вдохновит вас на повышение уровня своих навыков, и вы станете более мощным аналитиком данных.У вас всё получится!Читайте также:7 шагов до уровня Моцарта кода5 недооценённых приложений в помощь программистуВыбираем шрифтЧитайте нас в Telegram, VK и Яндекс.ДзенПеревод статьи Justin: Learn to code for data: a pragmatist’s guideWritten byJenny VFollowLearn To CodeRSqlSpreadsheetsNuances Of ProgrammingMore from NOP::Nuances of ProgrammingFollowОбразовательные статьи и переводы — всё для программистаRead more from NOP::Nuances of ProgrammingMore From MediumАннотации для параллелизма в Java: наш подход к расцвечиванию потоковРудольф Коршун in NOP::Nuances of ProgrammingТОП — 5 советов, как улучшить свои UI навыкиTeya Manasherova in NOP::Nuances of ProgrammingСборка и запуск загрузчикаIuliia Averianova in NOP::Nuances of ProgrammingFake-объекты практичнее mock-объектовIuliia Averianova in NOP::Nuances of ProgrammingUX или UI — на что уделить внимание?Teya Manasherova in NOP::Nuances of Programming5 подводных камней нереляционных баз данныхАндрей Шагин in NOP::Nuances of Programming5 признаков того, что вы тратите свой потенциал разработчика впустуюElena Andenko in NOP::Nuances of ProgrammingКак развернуть пакет Cython в PyPIАндрей Шагин in NOP::Nuances of ProgrammingAboutHelpLegalGet the Medium app"
Pre-Processing Data with R,https://medium.com/@tenos.id/pre-processing-data-with-r-b50593e9163e?source=tag_archive---------1-----------------------,"Text Mining,Text Preprocessing,R","Haloo sahabat Tenos, selamat bermalam minggu.Akhir-akhir ini sedang hype tentang text-mining loh. Kira-kira apasih text mining?Text mining adalah proses penemuan akan informasi atau trend baru yang sebelumnya tidak terungkap dengan memproses dan menganalisa data dalam jumlah besar (Adiwijaya, 2006).“Data dalam jumlah besar”Wah pasti bukan hanya jumlah yang besar, biasanya data nya pun bertipe “unstructured data”Jadi sebelum text mining, kita perlu “membersihkan” unstructured data tersebut. Terkadang data yang diperoleh banyak mengandung “noise”, bahkan tidak memiliki struktur yang jelas, dan terkadang tidak mencerminkan makna yang sebenarnya.Nah proses tersebut dikenal dengan pre-processing data. Secara singkatnya mengubah unstructured data -> structured data untuk proses mining.Sebagai contoh di story medium Tenos yang berjudul “scraping tweets dengan R”, pasti tahapan pertama yang dilakukan ketika sudah mendapatkan data #Pilkada2020 ialah melakukan Pre-Processing atau cleaning data.Sebagai contoh, mimin akan melakukan pre-processing data tweets #Pilkada2020 dengan R.Menyiapkan data tweets #Pilkada2020Menginstall package3. Text SubbingFungsi sub, kependekan dari substitute yaitu mencari pola tertentu pada suatu text dan menggantinya. Misalnya menhapus/mengganti substring dengan text kosongMaka seluruh cuitan yang mengandung pola “\n” akan diganti dengan spasi4. Text ReplacementSerupa dengan text subbing, text replacement ini lebih spesifik. Dapat mengganti/menghapus html, url, emoji, tanggal, dan lain-lain.Sebagai contoh mimin akan menghapus html dan url pada data pilkada20205. Replacement -> Mention dan HastagFungsi replace ini memiliki banyak kegunaan selain menghapus html/url, kita dapat menghapus/mengganti hastag atau mention yang terdapat pada Pilkada2020 dengan fungsi replace_hash dan replace_tag.6. Slang WordSlang word biasa dikenal dengan “BAHASA GAUL”. Dalam menulis status di social media, acapkali ditemukan kata-kata gaul atau singkatan. Nah kata-kata seperti itu perlu kita ubah menjadi kata yang formal. Sehingga data yang kita miliki lebih tertata dan mudah untuk dimengerti.7. Text StrippingText stripping merupakan fungsi untuk menghapus pola atau symbol yang tidak relevan atau tidak bermakna. Seperti “…..” atau “,,,,,” pada text.StemmingStemming merupakan suatu proses untuk menemukan kata dasar dari sebuah kata. Dengan menghilangkan semua imbuhan (affixes) baik yang terdiri dari awalan (prefixes), akhiran(suffixes) dan confixes (kombinasi dari awalan dan akhiran) pada kata turunan. Stemming digunakan untuk mengganti bentuk dari suatu kata menjadi kata dasar. Sebagai contoh, saya ingin mengetahui kata dasar dari“membagikan” ->bagi“mengembangkan” -> kembangBerbeda dengan text cleaning yang sudah tersedia banyak fungsi bawaan, sedangkan untuk melakukan stemming kita perlu membuat fungsi stemming terlebih dahuluTOKENIZINGTokenize merupakan proses pembagian kalimat atau text menjadi bagian-bagian tertentu. Misal kalimat“saya memakan roti”Jika dilakukan tokinaizing maka akan terdapat 3 token yaitu,saya, memakan, rotiStopwordStop words merupakan common words atau kata umum yang biasanya muncul dalam jumlah yang banyak dan dianggap tidak memiliki makna. Sebagai contoh, the, and, of.Nah stopword yang tersedia pada R merupakan common word dalam Bahasa inggris.Sedangkan tweet #pilkada2020 merupakan tweet berbahasa Indonesia, jadi kita perlu data yg berisi stop word dalam Bahasa Indonesia sebagai contoh kata :yang, dan, tetapi, sedangkan, dllSetelah data sudah “bersih” maka kita dapat melanjutkan ke tahap selanjutnya text mining. Entah melakukan Sentiment Analysis atau yang lainnya.Selamat Mencoba!Written byIndekstat — TenosBig Data Analytic , IT Developer, and IT Consulting & ServicesFollowText MiningText PreprocessingRMore from Indekstat — TenosFollowBig Data Analytic , IT Developer, and IT Consulting & ServicesMore From MediumFast transition between dplyr and data.tableNata Berishvili in Towards Data ScienceBBC News Text ClassificationCigdem Tuncer in Analytics VidhyaVanilla Neural Networks in RChris Mahoney in Towards Data ScienceTwitter Text Analysis in RKieran Tan Kah Wang in Towards Data ScienceUsing Keras Tokenizer Class for Text Preprocessing Steps — 1st Presidential Debate Transcript 2020Kurt F. in An Idea (by Ingenious Piece)Natural Language Processing in Production: 27 Fast Text Pre-Processing MethodsBruce H. Cottman, Ph.D. in The StartupNLP- Text Preprocessing TechniquesPrassena Kannan in The StartupInterpret Regression Analysis Results using R: Biomedical DataSwayanshu Shanti Pragnya in Analytics VidhyaAboutHelpLegalGet the Medium app"
10 Best Data Science Books on R,https://medium.com/swlh/10-best-data-science-books-on-r-777dba4ec2fa?source=tag_archive---------0-----------------------,"Data Science,Data Analysis,R,Best Data Science Books,R Books","It is always complained that finding written sources in the R programming language is not as easy as in other current languages. Unfortunately, enough blog support and current question sources are not the address of the solution. However, these are not the only options for those who manage to work with books.Documentation literacy is perhaps the most difficult but necessary workload of software. Realizing this much earlier, I think, is the most important way to be permanent in the sector. However, not many people bother to do it either because they are used to it or because they think they have enough code knowledge — that’s exactly what it is. It’s also a reason why you can’t get deep. In other words, as the reading level increases, it is not difficult to increase the quality.Of course there are other things that I would like to mention in this post. First of all, I will introduce some resources to you to better understand the R language. For this, I would like to share with you the resources and links that we will discuss in some detail. If you’re ready, let’s start:1-John Chambers / Software for Data AnalysisYou can find books by John Chambers and many other data scientists at Springer. These are a very well designed resource with good examples for you to gain real programming skill in R with statistics.Image by Springer (1)2-Bill Venables / An Introduction to RThe Bill Venables book is indeed a good introduction for R. The guide looks pretty straightforward, but is still ideal for beginnersImage by Amazon (2)3-Brian Ripley-Bill Venables / S ProgrammingThis book is dense, but contains a lot of high-level information that you won’t find easily anywhere else. This is basically a programming reference for those with experience in R and possibly another programming language. For example, information about loops (for, while, again) and their endings (next, search) is found on a page.Image by Springer (3)4- Venables-Ripley / Modern Applied Statistics with SThis is a print run to have in S + / R, but the book is not for novices in Statistics. You will not learn how and why to apply such models correctly, but how to fit generalized linear models into the language. For this purpose, there are a large number of custom monographs, and most of them use R for examples. Also, this book assumes some basic programming knowledge.Image by Springer (4)5-Pinheiroand Base / Mixed-Effects Models in S and S-PlusIt includes the excellent depth and scope of security analysis. This text is very well written and insightful. Can be used as a helpful resource.Image by Springer (5)6-Paul Murrell / R GraphicsThis book provided a thorough treatment of the different systems for creating graphics in R. It particularly emphasizes grid graphics, and how to use low level plotting functions to create, extend, and enhance plots. The book is divided into four major sections and there are ample examples of graphs and code. Further, each chapter starts with a brief preview and ends with a short summary paragraph.Image by Amazon (6)7- Garret Grolemund and Hadley Wickham / R for Data ScienceIf you want to sharpen your R skills, R for Data Science is the perfect book. It covers the basics for new R users, such as data cleaning, but also gets into more advanced topics as well. Data scientists can spend up to 80% of their time cleaning data, so this is a reference you will definitely want to keep close by.Image by Amazon (7)8-Hadley Wickham / Advanced RIt covers everything from the foundations, including data structures, object oriented programming, and debugging, to functional programming and performance code.Image by Amazon (8)9-Nina Zumel / Practical Data Science with R PaperbackThis is a great book that artfully bridges the gap of data science as a process and data science as a practice. Really well written. It introduces R, version control, databases, a bit of visualization and some techniques that everyone doing data science should have on their toolbox.Image by Amazon (9)10-Jared P. Lander / R For EveryoneThis book gives an introduction into R programming and graphics, but also to many statistical and Machine Learning methods. It even comprises chapters about Markdown, Shiny and building R packages. Pertaining to R programming it provides a very good overview about base R, but also tidyverse and the data.table package. The description of statistical methods is focused on the coding aspects.Image by Amazon (10)And the R-bloggers website has a longer list of books.And of course it has free links for R Books:20 Free Online Books to Learn R and Data Science - Python and R TipsIf you are interested in learning Data Science with R, but not interested in spending money on books, you are…cmdlinetips.comReferences(1)https://www.springer.com/gp/book/9780387759357(2)https://www.amazon.com/Introduction-R-William-N-Venables/dp/0954612086(3)https://www.springer.com/gp/book/9780387989662(4)https://www.springer.com/gp/book/9780387954578(5)https://www.springer.com/de/book/9780387989570(6)https://www.amazon.com/Graphics-Chapman-Hall-CRC/dp/1439831769(7)https://www.amazon.de/-/en/Hadley-Wickham/dp/1491910399(8)https://www.amazon.com/Advanced-Chapman-Hall-Hadley-Wickham/dp/1466586966(9)https://www.amazon.com/Practical-Data-Science-Nina-Zumel/dp/1617291560(10)https://www.amazon.de/R-Everyone-Addison-Wesley-Data-Analytics/dp/0321888030The StartupMedium's largest active publication, followed by +723K people. Follow to join our community.Follow147 1 Data ScienceData AnalysisRBest Data Science BooksR Books147 claps147 claps1 responseWritten byKurt F.FollowPhysicist Analyst & always learner — writer because of his enthusiasm: Phython / R / Data Science / Computer Science / AI — ML — DLFollowThe StartupFollowMedium's largest active publication, followed by +723K people. Follow to join our community.FollowWritten byKurt F.FollowPhysicist Analyst & always learner — writer because of his enthusiasm: Phython / R / Data Science / Computer Science / AI — ML — DLThe StartupFollowMedium's largest active publication, followed by +723K people. Follow to join our community.More From MediumWho’s home and who isn’t? The challenges of conducting face-to-face interviews in JordanLaura Silver in Pew Research Center: DecodedData Science Must-KnowBoadzie Daniel in Analytics VidhyaThemes Don’t Just Emerge — Coding the Qualitative DataProjectUXAdopting Data Science Solutions for Business: Balancing Complexity, Accuracy and InterpretabilityThomas Gorin in BCG GAMMAPython — Read multiple SQL database tables into csvMukesh SinghGetting Your Data Ready for AIO'Reilly Media in oreillymediaMick Jagger & Circular Buffers in F#Chris White in The StartupML: Student’s, Two-Sample & Paired Sample T-tests. Don’t Use It Blindly.Jeheonpark in The StartupLearn more.Medium is an open platform where 170 million readers come to find insightful and dynamic thinking."
5-Minute Guide to Calling Functions from R Scripts,https://towardsdatascience.com/5-minute-guide-to-calling-functions-from-r-scripts-41c4a09db1eb?source=tag_archive---------0-----------------------,"R,R Tutorial,Code,Google Trends,Editors Pick","Photo by Christina Morillo from PexelsYou’ve likely heard the popular guideline that if you find yourself copying and pasting code more than 3 times, you should write it as a function. While you can write and store these functions at the top of your R Markdown files, this approach can get messy and is counterproductive if you end up copying and pasting the functions into multiple files. Often, the best way to stay organized is to write your functions in a script and to call them from any additional files where they’re needed.To demonstrate this process, I will use 3 functions to conduct a very simple change point analysis of Google searches containing the words “supreme court” over the past month.First, create your functionsIn an R script, we write three simple functions. The first plots the Google Trends data, the second performs a simple change point analysis using the bcp() function from the “bcp” package, and the third plots the results of this analysis.google_graph = function(data, date, observation, graph_title) {    data %>%     ggplot() +    geom_line(aes(x = date, y = observation),               color = ""#09557f"",              alpha = 0.6,              size = 0.6) +    labs(x = ""Date (Start of Week)"",          y = ""Relative Proportion"",         title = graph_title) +    theme_minimal() +    scale_x_date(date_breaks = ""1 week"") +    theme(axis.text.x = element_text(angle = 45))  }bcp_analysis = function(observation, data) {    set.seed(100)    bcp = bcp(observation)    prob = bcp$posterior.prob  prob = as.data.frame(prob)     bcp_dataframe = cbind(data, prob) %>%     select(date, prob)  }bcp_plot = function(dataframe){    dataframe %>%     ggplot() +    geom_line(aes(x = date, y = prob),              color = ""#09557f"",              alpha = 0.6,              size = 0.6) +    labs(x = """",         y = ""Posterior Probability"",         title = ""Changepoint Probabilities"") +    theme_minimal() +    ylim(0, 1) +    scale_x_date(date_breaks = ""1 week"") +    theme(axis.text.x = element_text(angle = 45))  }I’ve also checked this “Source on Save” box. If you check this box, then the file will be sourced automatically to the global environment when you save changes to your functions in the script.Connect to your functionsConnecting to functions stored in a script from an R Markdown file is very similar to connecting to functions stored in a package. Instead of using a library() statement, we use a source() statement and indicate the script’s path name. In this case, we use the following code:source(""./functions.R"")When we run this line of code, the functions contained within the script automatically appear in the Global Environment. The connection was successful!Use your functionsFirst, we’ll just use the gtrends() function from the “gtrendsR” package to pull Google search volume for searches containing the words “supreme court” in the United States over the past month. A mutate step is also used to convert the “date” variable to a date format:data = gtrends(keyword = ""supreme court"", geo = ""US"",                time = ""today 1-m"")$interest_over_time %>%   mutate(date = as.Date(date))Now we have data to use in our functions! We use the google_graph() function the same way we would use any other function, allowing us to easily plot the data:google_graph(data, data$date, data$hits, ‘Google Searches for “Supreme Court”’)The plot looks good! Unsurprisingly, there is a massive surge in searches containing “supreme court” following the death of Supreme Court Justice Ruth Bader Ginsburg. Let’s use the bcp_analysis() and bcp_plot() functions to see if this spike represents a significant change point in this time-series object. Again, we use the functions the same way we would if we were using functions from a loaded package:bcp_dataframe = bcp_analysis(data$hits, data)bcp_plot(bcp_dataframe)Several days following the death of Ruth Bader Ginsburg have posterior probabilities of 1.00, indicating that searches containing the words “supreme court” likely changed in a statistically meaningful way around this time.It’s as simple as that! If we wanted to plot and analyze several different sources of data in separate Rmd files, it would be as easy as connecting those files to our “functions” script with a source() statement and using our functions.Written byEmily A. HalfordI am currently a data analyst working in psychiatric epidemiology, and I am excited about the intersection of data science and mental health. Views are my own.Follow38 1 Sign up for The Daily PickBy Towards Data ScienceHands-on real-world examples, research,  tutorials, and cutting-edge techniques delivered Monday to Thursday. Make learning your daily ritual. Take a lookGet this newsletterBy signing up, you will create a Medium account if you don’t already have one. Review our Privacy Policy for more information about our privacy practices.Check your inboxMedium sent you an email at  to complete your subscription.38 38 1 RR TutorialCodeGoogle TrendsEditors PickMore from Towards Data ScienceFollowA Medium publication sharing concepts, ideas, and codes.Read more from Towards Data ScienceMore From Medium5 YouTubers Data Scientists And ML Engineers Should Subscribe ToRichmond Alake in Towards Data Science7 Must-Haves in your Data Science CVElad Cohen in Towards Data Science21 amazing Youtube channels for you to learn AI, Machine Learning, and Data Science for freeJair Ribeiro in Towards Data ScienceThe Roadmap of Mathematics for Deep LearningTivadar Danka in Towards Data Science30 Examples to Master PandasSoner Yıldırım in Towards Data ScienceAn Ultimate Cheat Sheet for Data Visualization in PandasRashida Nasrin Sucky in Towards Data ScienceHow to Get Into Data Science Without a DegreeTerence S in Towards Data ScienceHow To Build Your Own Chatbot Using Deep LearningAmila Viraj in Towards Data ScienceAboutHelpLegalGet the Medium app"
Machine Learning to Kaggle Caravan Insurance Challenge on R,https://medium.com/swlh/machine-learning-to-kaggle-caravan-insurance-challenge-on-r-f52790bc7669?source=tag_archive---------1-----------------------,"Machine Learning,R,Data Science,Data Analysis,Logistic Regression","Photo by Scott Graham on UnsplashRecapping from the previous two posts, this post will utilise machine learning algorithms to predict customers who are mostly likely to purchase caravan policy based on 85 historic socio-demographic and product-ownership data attributes. In the previous post, we talked about using several feature selection methods like forward/backward stepwise selection and lasso regularisation to reduce the number of attributes we should fit into our ML algorithms. We will discuss which set of attributes as identified by different feature selection methods, will give us the best prediction results.Logistic Regression model development 1Since the target attribute (i.e. whether the customer purchases or not purchase caravan policy) is binomial discrete, we can use the simplest logistic regression as our first ML algorithm usingglm function from glmnet package. We first fit all the variables in to see how does the model performs using V86~ which means all attributes as the predictor variables other than V86 which is our target variable.Observations from this model:At 5% significance level, V47 (Contribution car policies), V55 (Contribution life policies), V59 (Contribution fire policies), V76 (Number of life insurances), V82 (Number of boat policies) are significantResidual deviance is lesser than null deviance which means this model is more useful compared to one without any predictor variablesNote that the *** signifies the level of significance of this predictor variable.We can use the Chisq test to check how well is the logistic regression model in comparison with a null model.The p-value is really small which means this model is significantly better than the null model.Since our task is to predict if a specific customer will buy caravan policies, we create a confusion matrix by using confusionMatrix function from the caret library to show how many predictions this model has made correctly and incorrectly in our training dataset.This model predicted that 9+7=16 customers will buy mobile home policies and 5465+341=5806 will not buy mobile home policies. Of these observations, 5465 out of (5465+9) which is approximately 99.84% (sensitivity) were correctly predicted by the model that these customers will not buy mobile home policies and they indeed not buy it. Whereas, only 7 out of (7+341) which is approximately 2.01% (specificity) were correctly predicted by the model that these customers will buy mobile home policies and they indeed bought it. Hence, this model has high sensitivity but very low specificity.Note that even though the accuracy of the model is high at 93.99%, it is expected as since our dataset is highly unbalanced with only 348 (6%) of the customers having bought mobile home policies. If we were to predict that all customers will not purchase mobile home policies, we will still get an overall percentage of 5474/5822 of correct predictions which is approximately 94.02% (i.e. No Information Rate). As a result, what we are aiming for in terms of the training error is a model with high specificity since we are trying to identify the customers that will buy mobile home policies.We can illustrate the sensitivity and specificity of the model by plotting a Receiver Operating Characteristic (ROC) curve by using the library pROC.What will be ideal in a ROC curve for a good model is for both the sensitivity and specificity to be close to the left top edge of the curve which means area under the curve is 1. However as seen in this plot, the line is far away from the top left edge and area under the curve is only 0.509 which is also obtainable as Balanced Accuracy in the previous Confusion Matrix and Statistics section.Since we know that our data is highly imbalanced, we can use sampling method like K-Fold Cross Validation approach to get a more accurate value for the accuracy of our model. We will use 10 Fold Cross Validation which involved splitting our training data into 10 folds, and we train 10 models each of them using 9 of the fold as training data and 10th fold as the validation data to get the accuracy of the model. We then obtain the mean of the accuracy of all the 10 models.With 10-Fold Cross Validation, we noted the accuracy of this model is 0.9375.Logistic Regression model development 2We repeat the above using the set of attributes identified by Forward stepwise selection, Backward stepwise selection and Lasso Regularisation. From there, it can be shown that Lasso Regularisation identified attributes performed better in terms of specificity. Now, we can try to fit another model using 7 significant attributes as identified by the logistic regression trained with Lasso regularisation identified attributes.Observations from this model:At 5% significance level, V18 (Lower level education), V47 (Contribution car policies), V59 (Contribution fire policies), V82 (Number of boat policies), V83 (Number of bicycle policies) are significant.Residual deviance of 2376.7 which is higher than the 2nd model.We create a confusion matrix to show how many predictions this model has made correctly and incorrectly in our training dataset.The sensitivity increased slightly from 99.8% to 99.9%. The specificity decreased slightly from 1.44% to 0.8621%.We can run the anova() function to compare this model and the 2nd model and see if they are statistically different.The anova test shows that this model is statistically different compared to the 2nd model since p-value is low (i.e. <0.05). Again, we use 10-Fold Cross Validation Sampling Method to check our accuracy of the model.The cross-validated accuracy is the highest thus far, and is slightly higher than the 2nd model at 0.9385.We can consider the variance inflation factors to see if there are any variables that we can consider to remove to simplify our model.Variance Inflation Factor is a measure of how much the variance of the estimated regression coefficient is “inflated” by the existence of correlation among the predictor variables in the model.A VIF of 1 means that there is no correlation among the k-th predictor and the remaining predictor variables, and hence the variance of the k-th coefficient is not inflated at all. The general rule of thumb is that VIFs exceeding 4 warrant further investigation, while VIFs exceeding 10 are signs of serious multicollinearity requiring correction.We use the vif function from the car library to determine the VIF.As seen, all the 7 variables that were fitted in the 5th model have VIF of around 1 which means there is no correlation among the k-th predictor and the remaining predictor variables. Hence, we will stick to the same model parameters and keep it as the best model for our logistic regression.Linear Discriminant Analysis model development 1The 2nd type of model we can consider is Linear Discriminant Analysis (LDA) which is closely connected to Logistic Regression in which both produce linear decision boundaries that separate a class from another. The only difference is LDA will assume that the observations are drawn from Gaussian Distribution with a common covariance matrix in each class, and if this assumption is true, it will perform better than the Logisitc Regression.We first fit a LDA model with the full set of variables using lda function from the MASS library and show the confusion matrix to find out the specificity and sensitivity.We see that the sensitivity is 99.2% and specificity 7.8%. Comparing this full LDA model with the full Logistic Regression model, although the sensitivity of the LDA model is lower, it’s specificity is higher than 2.01% in the full Logisitic Regression model. If two or more variables are almost a linear combination to each other, their estimated coefficients will be close to 0 which makes it hard to interpret entirely their effects on the target variable. It is worthy to note that we should avoid variables that are highly correlated to each other in fitting LDA.Also note that there is a warning message of “variables are collinear” while fitting the LDA model with the full dataset which means some of our predictor variables are correlated to each other which was what was already explored in the EDA section.Let’s use 10-Fold Cross Validation Sampling method to get the accuracy of the this model.The accuracy of this model is 93.25%. If we were to compare this model to the models fitted in the logistic regression, this model is currently the best in terms of specificity as it is able to predict correctly 7.76% of the customers who indeed bought mobile home policies, which is the main aim for our prediction task, although the accuracy of the this model is lower than all the models in logistic regression.Linear Discriminant Analysis model development 2Next, we try to fit the 7 variables that were identified in Logistic Regression model development which performed the best in terms of model complexity and testing specificity.Sensitivity is 99.5% and specificity 4.02%. ROC (balanced accuracy) is 0.518. This model performs worse in terms of specificity and better in terms of sensitivity than the full LDA model.Note that we did not receive the warning message of “variables are collinear” in this model as the 7 variables fitted between them has non-significant correlation between them, which was explored in the Variable Inflation Factor (VIF).Let’s perform 10 Fold Cross Validation to get the mean accuracy of this model.Accuracy of this model is 0.9376 which is higher than the previous model at 0.9325, but still lower than all the models in logistic regression.Quadratic Discriminant Analysis model development 1The 3rd type of model we can consider is Quadratic Discriminant Analysis (QDA). In Linear Discriminant Analysis, we assume that the observations are drawn from Gaussian Distribution with a common covariance matrix in each class. However in QDA, this assumption does not hold as each class is allowed to have different covariance matrix.In both of our Logistic Regression and Linear Discriminant Analysis model development, we found out that the best model respectively is the one using the 7 variables that were deemed significant from Lasso regression, Forward Stepwise Selection and Backward Stepwise Selection. Hence, we first train our QDA model using this set of 7 variables.Sensitivity is 96.89% which is the lowest thus far out of all the models trained. Specificity is 9.20% which is the highest out of all models thus far.Let’s get the mean accuracy of this model from 10-Fold Cross Validation.Accuracy of model is 0.9189 which is the lowest thus far out of all the models that have been trained.Since the accuracy of QDA appears to be lower than both Logistic Regression and Linear Discriminant Analysis model, we will not train any more models with QDA.Chosen ModelComparing these 3 models, the best model we will choose for our prediction task will the 2nd model of Linear Discriminant Analysis containing the set of predictor variables (V18, V41, V47, V58, V59, V82, V83) that were identified as significant from the first 4 Logistic Regression Models. Firstly, even though the CV accuracy is lower compared to other models in Logistic Regression, it’s specificity is the highest. Since the task is to find the subset of customers who are likely to purchase caravan policies so that the rest who don’t will receive a mailing, our aim will be to maximise the identification of this group of customers who are likely to purchase mobile home policies so that the insurance company can save cost on the mailing who a smaller group of customers who are identified as not likely to purchase the mobile home policies, which means we will be looking more on specificity of the model. Although the specificity of the QDA model is the highest, the CV accuracy for the QDA model is very low. As a result, we chose the 2nd model in LDA as the best model for our prediction task as we want something in between for accuracy and specificity.Prediction using testing datasetWe are also provided with the testing dataset and we can use it to see how well will the LDA model we have identified as best perform. We are supposed to find a set of 800 customers in this testing dataset that are most likely to purchase mobile home policies based on the probabilities predicted by our model. After identifying this set of 800 customers, we then use the true testing data target values and see how many of them are identified correctly and incorrectly. Note that in real life problems, all we may have is a set of data which we have to use method like 70:30 split for us to get training and testing data in order to test our ML algorithms.The accuracy of our model using testing dataset is 79.7% in which it’s sensitivity was 81.74% and specificity 47.48%. Out of a total of 238 actual mobile home policy customers, our model correctly identified 113 of them.SummaryThe winner of this challenge came up with an algorithm that was able to identify 121 customers correctly, and ours which successfully identified 113, wasn’t that bad. The standard benchmark tests result in 94 using K-nearest neighbours, 102 using naive bayes and 105 using neural networks, which were all more complicated than our LDA model. Hope you guys enjoy this ML model building walkthrough and have a better sense now on how to approach a problem using exploratory data analysis, feature selection and machine learning!The StartupMedium's largest active publication, followed by +723K people. Follow to join our community.Follow52 Machine LearningRData ScienceData AnalysisLogistic Regression52 claps52 clapsWritten byKieran Tan Kah WangFollowData Analytics | Artificial Intelligence | Data Visualization | Perspective | https://www.linkedin.com/in/tankahwang/FollowThe StartupFollowMedium's largest active publication, followed by +723K people. Follow to join our community.FollowWritten byKieran Tan Kah WangFollowData Analytics | Artificial Intelligence | Data Visualization | Perspective | https://www.linkedin.com/in/tankahwang/The StartupFollowMedium's largest active publication, followed by +723K people. Follow to join our community.More From MediumTransferring large CSV files into a relational database using dingDONGTal shany in The StartupK-Nearest Neighbor(k-NN)Akshay Patel in Analytics VidhyaExploring the Bin Packing ProblemColton Saska in The StartupRobustness of Limited Training Data: Part 2Daniel Hogan in The DownLinQSearching for Pulsars with Machine LearningFrank Ceballos in Frank CeballosDealing with Categorical DataVictor Popov in machine_learning_eli5Rocket your Machine Learning practice with this bookJaime Zornoza in The StartupLinear RegressionMohammad RoufaLearn more.Medium is an open platform where 170 million readers come to find insightful and dynamic thinking."
A/B Hypothesis Testing Explained Using R,https://medium.com/@marco_17979/a-b-hypothesis-testing-explained-using-r-a44252f47149?source=tag_archive---------2-----------------------,"Conversion Optimization,Statistics,Math,R","PreludeIn the last article, I talked about the importance of using descriptive statistics for your data.If you didn’t read the last article, I highly suggest you do it.Over explaining 8 famous descriptive statistics concepts, some are taken for granted in this article and they won’t be explained again.Hopefully, you learned a lot just by looking at the practical examples.If you do it yourself using R, you’ll learn much faster.Now, let’s dive into the second article.Ever heard of CRO?It stands for conversion rate optimization and it’s one of the branches of digital marketing nowadays.It’s based on the hypothesis that your website might perform better and take action on that.CRO consultants tweak buttons, product messaging, copy, design, and more to increase your conversion rate.If you’re a math geek and you’d like to get a sneak peek into the CRO world, this article is for you.If you’re a CRO consultant and you’d like to know how to use R for simple A/B testing concepts, this article is for you.If you’re just curious and you want to learn, this article is for you as well.We’re gonna explain the fundamental concepts of A/B testing using R:Sampling, sample size, and PopulationHypothesis Creation and Null HypothesisType I and Type II errorsP-Value and Significance LevelConfidence Level and PowerOne Sample T-test and Multiple T-testsMultiple T-tests problems and ANOVAHow Can A/B Hypothesis Testing Help you?Let’s say that you work for an E-com website and you want to learn how the last UX redesign impacted your user engagement or your purchase rate.You don’t know whether that’s positive or negative yet, but you have lots of data at hand.That’s where hypothesis testing comes into the picture.It’ll help you evaluate whether the change is real and durable, or it’s just a random fluctuation.Hypothesis testing provides you with a framework you can repeatedly use to make informed decisions based on data.The better your test results, the higher is your confidence level that changing your website has been a good choice.We’ll talk about that later on.Sampling, sample size, and PopulationBefore making tests, we need to know the fundamental concepts.In statistics, a sample is a portion of the original population.Why do we do this?Think about it.You want to know which is the average male height in the US.Would you measure all of us?You can, but it’s not efficient: it’d take you years.Instead, analysts take a sample of the original population, through a process called sampling.Keep in mind that there’s something called sampling error, and for a few reasons.Take a look at the chart below.Sampling Error — Publication Age and Count of Books Published for Top 100 Novel Authors in FrenchDo you think that the sample we’ve taken we’ll help us make accurate predictions on the given population?Of course not.But what if we take a sample like this?Sample — Publication Age and Count of Books Published for Top 100 Novel Authors in FrenchYou got it.This sample will help us make accurate predictions on the data, as it almost represents the population.We can’t talk of perfection in an imperfect world, but in statistics, everything is always a “hypothesis”.How do you calculate the minimum sample size to take for the experiment?It all comes down to three factors:How large of a difference you want to detectConfidence levelPower and variabilityYou’re probably familiar just with the first one.We’ll talk about the other two ones below. Keep reading!Hypothesis Creation and Null HypothesisNow that we found out what sample and population are, let’s talk about the hypothesis.I remember how boring it was trying to coming up with hypotheses for triangles at high school.In the real world, however, it’s all much more exciting.Let’s make an example.You think that changing your product image on your product page caused a lift in % of visitors adding the product to their carts.Using common sense, you’d use the “true” hypothesis:“The new product image causes a lift in conversion rate for the visitors-cart segment.”In statistics instead, to make things less confusing, we’ll use the null hypothesis, which it’s the exact opposite:“The new product image doesn’t cause any effect on the conversion rate for the visitors-cart segment.”This helps us quite a bit, especially with errors.Think about why Type I and Type II errors are always false and come up with your own hypothesis (null).Write it down and we’ll see if you got it right.Type I and Type II errorsWe’ll now start using R, but let’s design an experiment first.We want to know whether“history and chemistry scholars are interested in volleyball at the same rates”.We invite 100 history majors and 100 chemistry majors to join a volleyball team.After one week, we check our subscribers.39% of chemistry majors subscribed, while just 34% of history majors subscribed.Since we’ve taken samples of our populations, we want to know whether this result is accurate enough to predict the behavior of an entire population or it’s just a sampling error.We analyze our data again and we decide we should keep our null hypothesis since this is a sampling error.In other words:“The subscription rate for history majors is the same as for chemistry majors and any difference is due to sampling error.”Now, a better analyst (you just started learning) works on this experiment and he states you’re wrong.You kept the null hypothesis but in reality, you should have rejected it.Well, your error is a false negative:A false negative happens when you kept the null hypothesis but in reality you should have rejected it. The hypothesis is false.This is called a Type II error.So now your result is changed:“The subscription rate for history and chemistry majors is different and we should reject the null hypothesis.”Now let’s try with another experiment.We want to know whether people that took a Coursera certificate are more likely to get a pay raise or not.The population is around 10000 people, but we took a sample for efficiency.We call 200 people who have taken the certificates and 200 who did not.We then found that 25% of people who have taken the certificate got a pay raise, while just 18% of people who have not taken the certificate got a pay raise.We come up with our null hypothesis:“There’s no noticeable difference between people who have taken a Coursera certificate and people who did not in their pay raise.”You analyze the data and you found out that there’s a noticeable difference in the pay raise between people who have taken the certificate and people who have not.You formulate your results in plain words:“There’s a difference in the pay raise between people who have taken a Coursera certificate and people who have not.”As before, a senior analyst with some 20 years of experience comes and he states you’re wrong.Why?Because he found a Type I Error.The analyst:“You rejected the null hypothesis but in reality, you should not have done it. Now we’re in trouble.”You:“Let’s go to that night club you really enjoy tonight…”The analyst:“Stop it!!”And he starts crying.after having lunch, you analyze the data and you find that he’s right.A Type I Error is often called a false positive.As the analyst said, it means your null hypothesis should be kept but in reality, you’ve rejected it.You then formulate the new results in plain words:“After accurate analysis, there’s no noticeable difference in the pay raise between people who have taken a Coursera certificate and people who have not. ”In R, these results are found out using the function intersect().Let’s say that the outcomes of our last experiments are summarized in four vectors of numbers:Actual positiveActual negativeExperiemental positiveExperimental negativeThe first two ones are true positives and negatives.The last ones have been determined by running the experiment.In R:real_positive <- c(2, 5, 6, 7, 8, 10, 18, 21, 24, 25, 29, 30, 32, 33, 38, 39, 42, 44, 45, 47)real_negative <- c(1, 3, 4, 9, 11, 12, 13, 14, 15, 16, 17, 19, 20, 22, 23, 26, 27, 28, 31, 34, 35, 36, 37, 40, 41, 43, 46, 48, 49)experimental_positive <- c(2, 4, 5, 7, 8, 9, 10, 11, 13, 15, 16, 17, 18, 19, 20, 21, 22, 24, 26, 27, 28, 32, 35, 36, 38, 39, 40, 45, 46, 49)experimental_negative <- c(1, 3, 6, 12, 14, 23, 25, 29, 30, 31, 33, 34, 37, 41, 42, 43, 44, 47, 48)To detect a false positive or Type I Error, we’ll intersect the real negative with the experimental positive.Why?Well, you rejected the null hypothesis with your experiment, so you’ve got a positive.In reality, the result is negative.That’s why.In R:type_i_errors <- intersect(real_negative, experimental_positive)type_i_errorsTo detect a false negative or Type II Error, we’ll intersect the real positive with the experimental negative.Why?Well, you kept the null hypothesis with your experiment, so you’ve got a negative.In reality, the result is positive.That’s why.In R:type_ii_errors <- intersect(actual_positive, experimental_negative)type_ii_errorsThese lines of codes above will return a vector with elements in common between the first and second vectors taken as inputs.[4  9 11 13 15 16 17 19 20 22 26 27 28 35 36 40 46 49] #Type I[6 25 29 30 33 42 44 47] #Type IINow, let’s go on with the P-value and Confidence Level.P-Value and Significance LevelThe P-value is the probability of obtaining the difference you saw from a sample if there really isn’t a difference for all the population.P-values help determine how confident you can be in validating the null hypothesis.Let’s make an example.Do you remember the history and chemistry majors?We had got a 39% subscription rate to our volleyball team for chemistry majors, and 34% for history majors, with a difference of 5%.We run a test on the experiment and among other data, we find to have a p-value of 4%.This means that we’d see at least a 5% difference only 4 times out of 100 due to sampling error, given the assumption the null hypothesis is true (that’s where we start).The significance level is a threshold for your P-value.Conventionally, it’s been set at 5%, so you’d have a 5% probability of getting a false positive.When you’re running an A/B test, this is extremely important.However, sometimes it happens you’ll accept a higher significance level to keep moving quickly.That’s fine if you don’t want to lose your shirt.Keep in mind that your P-value:does not tell you that B > Adoes not tell you the probability of making a mistake when you select B over AThese are common misconceptions and important to highlight.Confidence Level and PowerNow that you found the P-value, you can compute the confidence level by subtracting it to 100%P_value <- 4%Confidence_Level = 100% - P_valueprint(Confidence_Level) #96%The main difference between the P-value and the confidence level is in timing:P-value is obtained after you ran the test and indicates the probability of getting a false positiveThe confidence level is set before running the test and affects the confidence interval.Now, since we usually want to be able to reject the null hypothesis, we need to understand our “powerful” is our test.That’s why we need to introduce the concept of statistical power.Statical power is:“the likelihood that a study will detect an effect, when there is an effect to be detected.”And it’s determined by:size of the effect you want to detectsize of the sample usedOutcome #1:The bigger the effect the easier it is to detect.Outcome #2:The bigger the sample size the easier it is to detect.When you have an inaccurate sample size, you’re likely to get into an underpowered A/B test.That means you don’t have enough data to determine the result accurately.Your probability of getting a false negative type II error is higher than it should be.Actually, you can overpower an underpowered A/B test to make up for the difference, but not too much.Why?Because by doing that you can actually achieve the opposite outcome, getting a false positive type I error.So how do we regulate?Conventionally, you want to keep your statistical power around 80%, which means there’s a 20% probability of getting a type II error for your A/B tests.Now let’s finally do some testing!One Sample T-test and Multiple T-testsSuppose you run a blog and you estimate the average age of your readers to be 30.Yesterday you got 500 visitiros and the average age was 32.Are the visitors older than expected or it’s just due to sampling error?First, let’s set a null hypothesis:“The sample belongs to a population with the target mean.”Or“The average age between the sample and the population are equal”In R, you can test this using the t.test() function.The t.test() function takes as inputs:Values of your samplethe argument mu, indicating the desired meanexpected_mean, indicating the value of your desired meanLet’s code this:load(""ages.Rda"")ages # [33 34 29 30 22 39 38 37 38 36 30 26 22 22]ages_mean <- mean(ages)ages_mean #32results <- t.test(ages, mu = 30)results# data:  ages# t = 0.59738, df = 13, p-value = 0.5605# alternative hypothesis: true mean is not equal to 30# 95 percent confidence interval:# 27.38359 34.61641# sample estimates:# mean of x :32The P-value is higher than the significance threshold usually accepted, so this means that we might get a false positive, hence rejecting the hypothesis when we should not.However, in the business world, we can accept a 5–6% P-value.In this case, we accept the alternative hypothesis, hence the true mean is 32, not 30.Now, let’s see how t.test() with multiple samples work.You want to compare two samples of your traffic:the average age of last week’s ordersthe average age of this week’s ordersYou calculate the means using R:last_week_mean <- mean(last_week)last_week_mean  # 25.44this_week_mean <- mean(this_week)this_week_mean # 29.02And you run the test to find out:results <- t.test(week_1,week_2)results # t = -3.5109, df = 94.554, p-value = 0.0006863# alternative hypothesis: true difference in means is not equal to 0# 95 percent confidence interval:# -5.594299 -1.552718# sample estimates:# mean of x mean of y # 25.44806  29.02157With a very low P-value and the right confidence interval, we can surely state that there’s a difference between the two sample means and the difference is not due to sampling error.Multiple T-tests and ANOVAWe’re actually a CRO agency and our client is in a hurry.He wants to speed up the process and he asks you to run multiple t.tests() between three different samples.Your client believes that the P-value stays always the same.However, you know your thing and you explain to him the exact opposite:“Running N t.tests() means you have to subtract to 1 the confidence level multiplied by N times, therefore highly increasing the chance of getting a false positive Type I error.”If your confidence level is 95% and you run 3 tests between 3 samples, then the probability of getting a false positive Type I error is:prob_error = 1 - (0.95*3)prob_error = 0.14 This error is unacceptable in statistics, and you explain it to the client.Now, if your client insists the only way to keep your error probability low is to use ANOVA or Analysis of Variance.In the case you’re comparing the means, ANOVA tests the null hypothesis that all of the datasets you are considering have the same mean.If you reject the null hypothesis using ANOVA, you're saying that at least one of your sample has a different mean, but it doesn't tell you which one.If you want to know which is the one, you’ll need to perform uni-factorial analysis.In R, the ANOVA function is aov() and it takes as inputs the two vectors of samples and combine them into a new data frame, a table.Let’s say that you want to test the scores at a given game for each major in your college.Data frame for ANOVAWell, in R you’ll use:results <- aov(score ~ group, data = df_scores)Note: Score ~ group indicates the relationship you want to analyze or how each major relates to the game score.To retrieve the P-value you need, you’ll run the following piece of code:summary(results)In this case, the null hypothesis is that“all the majors score the same results at the video game”If you reject the null hypothesis, you can confidently state that a pair of datasets is significantly different.As we said though, you do know which ones.You talk with the client and now he finally understands.You can keep going with simple A/B testing, not multivariate A/B testing.FeedbackAwesome, this was the last piece of our article.How did I do?Did you understand the concepts or not?Let me know if you have any questions in the comment section below.For now, enjoy your day!MarcoWritten byMarco BasileCXL Certified Growth Marketer Hunting for a FT PositionFollowConversion OptimizationStatisticsMathRMore from Marco BasileFollowCXL Certified Growth Marketer Hunting for a FT PositionMore From MediumContent Strategist User GuidePhoebe Assenza in rhetoricaRethinking and Predicting the Future of TV AdvertisingAri Lewine in The StartupHarry Potter and The Psychology of KPop FansPrince & Matt in Pop Neuro MagazineHow to Choose Social Channels for Your Brand — Pt. 2Mindstream Interactive in Mindstream InteractiveThe 6 Steps of a Successful Product LaunchGeraint Clarke in The Startup5 Ways to Improve Your Newsletter Open RateBram Berkowitz in Better MarketingHow Will Being Forced to Use USB-C Charging Cables Affect Apple’s Brand?Andrei Tapalaga ✒️ in Better MarketingWhy a Jewellery Ad Sparked Major Outrage in IndiaNitish Menon in Better MarketingAboutHelpLegalGet the Medium app"
My first class of visualisation with R,https://medium.com/@zumaia/my-first-class-of-visualisation-with-r-762ee0ff030e?source=tag_archive---------3-----------------------,"R,Visual,Plots,Ggplot2,Graphics","“You won’t go to bed without knowing something else” is a saying that refers to the idea that every day we learn something new.This phrase highlights the nature of our lifelong learning, which is continuous and unstoppable, which increases day by day with small things: a new activity, information we didn’t know, a different way of looking at things.The saying implies that every day we must increase a little more our knowledge about things, that we must not waste time but use it to know new things.This phrase is generally used to express that we have learned something new. For example, someone tells us that the Atacama Desert in Chile is the driest in the world, and we respond to that, satisfied, “you won’t go to bed without knowing something else”. So it is used to indicate that we have learned something new or interesting.Photo by Simon Berger on UnsplashVariations of this saying are “you will not go to bed without knowing one more thing”, “you will not go to bed without knowing one more thing”, “you will never go to bed without knowing one more thing”, or “you will not go to bed without knowing one more thing”.Today was the first day I was given a master class on graphics and visualization! How the perspective of the data changes when you see it on gŕaficas instead of on a table.As an example they have shown us the “datasauRus” library.The Datasaurus data packageThis package wraps the awesome Datasaurus Dozen dataset, which contains 13 sets of x-y data. Each sub-dataset has five statistics that are (almost) the same in each case. (These are the mean of x, mean of y, standard deviation of x, standard deviation of y, and Pearson correlation between x and y). However, scatter plots reveal that each sub-dataset looks very different. The dataset is intended to be used to teach students that it is important to plot their own datasets, rather than relying only on statistics.The Datasaurus was created by Alberto Cairo in this great blog post.Datasaurus shows us why visualization is important, not just summary statistics.He’s been subsequently made even more famous in the paper Same Stats, Different Graphs: Generating Datasets with Varied Appearance and Identical Statistics through Simulated Annealing by Justin Matejka and George Fitzmaurice.In the paper, Justin and George simulate a variety of datasets that the same summary statistics to the Datasaurus but have very different distributions.This package looks to make these datasets available for use as an advanced Anscombe’s Quartet, available in R as anscombe.Load packagespackages <- c(""datasauRus"",""ggplot2"",""gganimate"")newpack  = packages[!(packages %in% installed.packages()[,""Package""])]if(length(newpack)) install.packages(newpack)a=lapply(packages, library, character.only=TRUE)UsageTo see that statistics are (almost) the same for each sub-dataset, you can use dplyr.The Datasaurus data packageSteph Locke This package wraps the awesome Datasaurus Dozen dataset, which contains 13 sets of x-y data. Each…cran.r-project.orgif(requireNamespace(""dplyr"")){  suppressPackageStartupMessages(library(dplyr))  datasaurus_dozen %>%     group_by(dataset) %>%     summarize(      mean_x    = mean(x),      mean_y    = mean(y),      std_dev_x = sd(x),      std_dev_y = sd(y),      corr_x_y  = cor(x, y)    )}To see that each sub-dataset looks very different, you can draw scatter plots.if(requireNamespace(""ggplot2"")){  library(ggplot2)  ggplot(datasaurus_dozen, aes(x=x, y=y, colour=dataset))+    geom_point()+    theme_void()+    theme(legend.position = ""none"")+    facet_wrap(~dataset, ncol=3)}Let´s make animate!#library(datasauRus) #library(ggplot2) #library(gganimate)p <- ggplot(datasaurus_dozen, aes(x=x,y=y)) +geom_point() +theme_minimal() +transition_states(dataset,3,1) + ease_aes() anim_save(""myfilename.gif"",p)I hope you like it.No matter what books or blogs or courses or videos one learns from, when it comes to implementation everything can look like “Outside the Curriculum”.The best way to learn is by doing! The best way to learn is by teaching what you have learned!Never give up!See you on Linkedin!Oscar Rojo Martín - Studing Data Science at Universidad de Deusto - San Sebastián, Basque Country, Spain | LinkedInView Oscar Rojo Martín's profile on LinkedIn, the world's largest professional community.www.linkedin.comReferences:https://cran.r-project.org/web/packages/datasauRus/vignettes/Datasaurus.htmlWritten byOscar RojoCurrently studing a Master in Data Science. Passionate about learning new skills. Former branch risk analyst. https://www.linkedin.com/in/oscar-rojo-martin/FollowPublic domain.RVisualPlotsGgplot2GraphicsMore from Oscar RojoFollowCurrently studing a Master in Data Science. Passionate about learning new skills. Former branch risk analyst. https://www.linkedin.com/in/oscar-rojo-martin/More From MediumBuild a Simple DL Model Using TensorFlowChan Naseeb in Analytics VidhyaBuild a Trump vs Biden Prediction Model With R From ScratchMatt C in The StartupData Science’s Evolution, and MineCaroline Clark in The StartupUnderstanding Linear and Polynomial Regression in Few StepsSnigdha Sen in The StartupAccounting for Uncertainty in Predictive OptimizationJo Saakvitne in BCG GAMMAPredictive modeling of  Diabetes DatabaseMurali Ambekar in The Startup3 Steps to Advanced Alerting on Airflow with DatabandJosh Benamram in DatabandCan we judge a movie by its cover?Anelia ValtchanovaAboutHelpLegalGet the Medium app"
