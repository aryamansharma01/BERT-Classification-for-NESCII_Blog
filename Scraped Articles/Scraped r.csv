title,articleUrls,keywords,text
Neural Network on Beer Dataset,https://medium.com/swlh/neural-network-on-beer-dataset-55d62a0e7c32?source=tag_archive---------0-----------------------,"Neural Network,Beer,R,Nn,Ann","Artificial neural networks (ANNs), usually simply called neural networks (NNs), are computing systems vaguely inspired by the biological neural networks that constitute animal brains.An ANN is based on a collection of connected units or nodes called artificial neurons, which loosely model the neurons in a biological brain. Each connection, like the synapses in a biological brain, can transmit a signal to other neurons. An artificial neuron that receives a signal then processes it and can signal neurons connected to it. The ‚Äúsignal‚Äù at a connection is a real number, and the output of each neuron is computed by some non-linear function of the sum of its inputs. The connections are called edges. Neurons and edges typically have a weight that adjusts as learning proceeds. The weight increases or decreases the strength of the signal at a connection. Neurons may have a threshold such that a signal is sent only if the aggregate signal crosses that threshold. Typically, neurons are aggregated into layers. Different layers may perform different transformations on their inputs. Signals travel from the first layer (the input layer), to the last layer (the output layer), possibly after traversing the layers multiple times.Photo by Vlad Tchompalov on UnsplashNeural networks learn (or are trained) by processing examples, each of which contains a known ‚Äúinput‚Äù and ‚Äúresult,‚Äù forming probability-weighted associations between the two, which are stored within the data structure of the net itself. The training of a neural network from a given example is usually conducted by determining the difference between the processed output of the network (often a prediction) and a target output. This is the error. The network then adjusts it‚Äôs weighted associations according to a learning rule and using this error value. Successive adjustments will cause the neural network to produce output which is increasingly similar to the target output. After a sufficient number of these adjustments the training can be terminated based upon certain criteria. This is known as [[supervised learning]].Let‚Äôs workInstall Packagespackages <- c(""xts"",""zoo"",""PerformanceAnalytics"", ""GGally"", ""ggplot2"", ""ellipse"", ""plotly"")newpack  = packages[!(packages %in% installed.packages()[,""Package""])]if(length(newpack)) install.packages(newpack)a=lapply(packages, library, character.only=TRUE)Load datasetbeer <- read.csv(""MyData.csv"")head(beer)summary(beer)Clase               Color        BoilGravity        IBU         Length:1000        Min.   : 1.99   Min.   : 1.0   Min.   :  0.00   Class :character   1st Qu.: 5.83   1st Qu.:27.0   1st Qu.: 32.90   Mode  :character   Median : 7.79   Median :33.0   Median : 47.90                      Mean   :13.45   Mean   :33.8   Mean   : 51.97                      3rd Qu.:12.57   3rd Qu.:39.0   3rd Qu.: 67.77                      Max.   :50.00   Max.   :90.0   Max.   :144.53        ABV         Min.   : 2.390   1st Qu.: 5.240   Median : 5.990   Mean   : 6.093   3rd Qu.: 6.810   Max.   :10.380Visualization of Iris Data SetYou can also embed plots, for example:pairs(beer[2:5],       main = ""Craft Beer Data -- 5 types"",      pch = 21, bg = c(""red"", ""green"", ""blue"", ""orange"", ""yellow""))library(GGally)pm <- ggpairs(beer,lower=list(combo=wrap(""facethist"",  binwidth=0.5)),title=""Craft Beer"", mapping=aes(color=Clase))pmlibrary(PerformanceAnalytics)chart.Correlation2 <- function (R, histogram = TRUE, method = NULL, ...)    {        x = checkData(R, method = ""matrix"")        if (is.null(method)) #modified            method = 'pearson'          use.method <- method #added        panel.cor <- function(x, y, digits = 2, prefix = """",                               use = ""pairwise.complete.obs"",                               method = use.method, cex.cor, ...)         { #modified        usr <- par(""usr"")        on.exit(par(usr))        par(usr = c(0, 1, 0, 1))        r <- cor(x, y, use = use, method = method)        txt <- format(c(r, 0.123456789), digits = digits)[1]        txt <- paste(prefix, txt, sep = """")        if (missing(cex.cor))             cex <- 0.8/strwidth(txt)        test <- cor.test(as.numeric(x), as.numeric(y), method = method)        Signif <- symnum(test$p.value, corr = FALSE, na = FALSE,                          cutpoints = c(0, 0.001, 0.01, 0.05, 0.1, 1),                         symbols = c(""***"",""**"", ""*"", ""."", "" ""))        text(0.5, 0.5, txt, cex = cex * (abs(r) + 0.3)/1.3)        text(0.8, 0.8, Signif, cex = cex, col = 2)        }    f <- function(t)        {        dnorm(t, mean = mean(x), sd = sd.xts(x))        }    dotargs <- list(...)    dotargs$method <- NULL    rm(method)    hist.panel = function(x, ... = NULL)         {        par(new = TRUE)        hist(x, col = ""light gray"", probability = TRUE, axes = FALSE,              main = """", breaks = ""FD"")        lines(density(x, na.rm = TRUE), col = ""red"", lwd = 1)        rug(x)        }    if (histogram)         pairs(x, gap = 0, lower.panel = panel.smooth,               upper.panel = panel.cor, diag.panel = hist.panel)    else pairs(x, gap = 0, lower.panel = panel.smooth, upper.panel = panel.cor)    }#if method option not set default is 'pearson'chart.Correlation2(beer[,2:5], histogram=TRUE, pch=""21"")library(plotly)pm <- GGally::ggpairs(beer, aes(color = Clase), lower=list(combo=wrap(""facethist"",  binwidth=0.5)))class(pm)pm‚Äògg‚Äô‚Äòggmatrix‚ÄôSetup and Train the Neural Network for Beer DataNeural Network emulates how the human brain works by having a network of neurons that are interconnected and sending stimulating signal to each other.In the Neural Network model, each neuron is equivalent to a logistic regression unit. Neurons are organized in multiple layers where every neuron at layer i connects out to every neuron at layer i+1 and nothing else.The tuning parameters in Neural network includes the number of hidden layers, number of neurons in each layer, as well as the learning rate.There are no fixed rules to set these parameters and depends a lot in the problem domain. My default choice is to use a single hidden layer and set the number of neurons to be the same as the input variables. The number of neurons at the output layer depends on how many binary outputs need to be learned. In a classification problem, this is typically the number of possible values at the output category.The learning happens via an iterative feedback mechanism where the error of training data output is used to adjusted the corresponding weights of input. This adjustment will be propagated back to previous layers and the learning algorithm is known as back-propagation.library(neuralnet)beer <- beer%>%    select(""IBU"",""ABV"",""Color"",""BoilGravity"",""Clase"")head(beer)# Binarize the categorical outputbeer <- cbind(beer, beer$Clase == 'ALE')beer <- cbind(beer, beer$Clase == 'IPA')beer <- cbind(beer, beer$Clase == 'PALE')beer <- cbind(beer, beer$Clase == 'STOUT')beer <- cbind(beer, beer$Clase == 'PORTER')names(beer)[6] <- 'ALE'names(beer)[7] <- 'IPA'names(beer)[8] <- 'PALE'names(beer)[9] <- 'STOUT'names(beer)[10] <- 'PORTER'head(beer)set.seed(101)beer.train.idx <- sample(x = nrow(beer), size = nrow(beer)*0.5)beer.train <- beer[beer.train.idx,]beer.valid <- beer[-beer.train.idx,]Visulization of the Neural Network on Beer DataHere is the plot of the Neural network we learnNeural network is very good at learning non-linear function and also multiple outputs can be learnt at the same time. However, the training time is relatively long and it is also susceptible to local minimum traps. This can be mitigated by doing multiple rounds and pick the best learned model.nn <- neuralnet(ALE+IPA+PALE+STOUT+PORTER ~ IBU+ABV+Color+BoilGravity, data=beer.train, hidden=c(5))plot(nn, rep = ""best"")Resultbeer.prediction <- compute(nn, beer.valid[-5:-10])idx <- apply(beer.prediction$net.result, 1, which.max)predicted <- c('ALE','IPA', 'PALE', 'STOUT', 'PORTER')[idx]table(predicted, beer.valid$Clase)predicted ALE IPA PALE PORTER STOUT    ALE    17   3   12      0     0    IPA     1 203   21      0     2    PALE   29  26   84      1     0    STOUT   0   4    0     30    67Accuracy of model is calculated as follows((17+203+84+0+67)/nrow(beer.valid))*10074.2# nn$result.matrixstr(nn)List of 14 $ call               : language neuralnet(formula = ALE + IPA + PALE + STOUT + PORTER ~ IBU + ABV + Color +      BoilGravity, data = beer.train, hidden = c(5)) $ response           : logi [1:500, 1:5] FALSE FALSE FALSE FALSE FALSE FALSE ...  ..- attr(*, ""dimnames"")=List of 2  .. ..$ : chr [1:500] ""841"" ""825"" ""430"" ""95"" ...  .. ..$ : chr [1:5] ""ALE"" ""IPA"" ""PALE"" ""STOUT"" ... $ covariate          : num [1:500, 1:4] 62.3 27.1 39 72.3 67.8 ...  ..- attr(*, ""dimnames"")=List of 2  .. ..$ : chr [1:500] ""841"" ""825"" ""430"" ""95"" ...  .. ..$ : chr [1:4] ""IBU"" ""ABV"" ""Color"" ""BoilGravity"" $ model.list         :List of 2  ..$ response : chr [1:5] ""ALE"" ""IPA"" ""PALE"" ""STOUT"" ...  ..$ variables: chr [1:4] ""IBU"" ""ABV"" ""Color"" ""BoilGravity"" $ err.fct            :function (x, y)    ..- attr(*, ""type"")= chr ""sse"" $ act.fct            :function (x)    ..- attr(*, ""type"")= chr ""logistic"" $ linear.output      : logi TRUE $ data               :'data.frame':	500 obs. of  10 variables:  ..$ IBU        : num [1:500] 62.3 27.1 39 72.3 67.8 ...  ..$ ABV        : num [1:500] 5.9 5.07 6.57 5.7 6.86 5.21 4.22 5.57 5.76 7.76 ...  ..$ Color      : num [1:500] 5.61 32.07 39.92 9.62 8.29 ...  ..$ BoilGravity: int [1:500] 37 25 40 37 31 28 19 27 30 44 ...  ..$ Clase      : chr [1:500] ""IPA"" ""PORTER"" ""STOUT"" ""PALE"" ...  ..$ ALE        : logi [1:500] FALSE FALSE FALSE FALSE FALSE FALSE ...  ..$ IPA        : logi [1:500] TRUE FALSE FALSE FALSE TRUE FALSE ...  ..$ PALE       : logi [1:500] FALSE FALSE FALSE TRUE FALSE TRUE ...  ..$ STOUT      : logi [1:500] FALSE FALSE TRUE FALSE FALSE FALSE ...  ..$ PORTER     : logi [1:500] FALSE TRUE FALSE FALSE FALSE FALSE ... $ exclude            : NULL $ net.result         :List of 1  ..$ : num [1:500, 1:5] 0.00942 0.01859 0.01845 0.00916 0.00478 ...  .. ..- attr(*, ""dimnames"")=List of 2  .. .. ..$ : chr [1:500] ""841"" ""825"" ""430"" ""95"" ...  .. .. ..$ : NULL $ weights            :List of 1  ..$ :List of 2  .. ..$ : num [1:5, 1:5] -10.8295 0.0944 0.9985 -0.1776 0.0445 ...  .. ..$ : num [1:6, 1:5] 0.0576 -0.058 -0.4324 0.4371 -0.0437 ... $ generalized.weights:List of 1  ..$ : num [1:500, 1:20] -0.08239 -0.000124 -0.000822 -0.082905 -0.093232 ...  .. ..- attr(*, ""dimnames"")=List of 2  .. .. ..$ : chr [1:500] ""841"" ""825"" ""430"" ""95"" ...  .. .. ..$ : NULL $ startweights       :List of 1  ..$ :List of 2  .. ..$ : num [1:5, 1:5] -0.5 1.832 -0.329 0.261 -1.112 ...  .. ..$ : num [1:6, 1:5] 0.341 1.107 0.689 0.471 -1.64 ... $ result.matrix      : num [1:58, 1] 8.02e+01 8.76e-03 7.37e+04 -1.08e+01 9.44e-02 ...  ..- attr(*, ""dimnames"")=List of 2  .. ..$ : chr [1:58] ""error"" ""reached.threshold"" ""steps"" ""Intercept.to.1layhid1"" ...  .. ..$ : NULL - attr(*, ""class"")= chr ""nn""beer.net <- neuralnet(ALE+IPA+PALE+STOUT+PORTER ~ IBU+ABV+Color+BoilGravity,                       data=beer.train, hidden=c(5),  err.fct = ""ce"",                       linear.output = F, lifesign = ""minimal"",                       threshold = 0.1)hidden: 5    thresh: 0.1    rep: 1/1    steps:   86036	error: 431.94881	time: 24.02 secsplot(beer.net, rep=""best"")Predicting Resultbeer.prediction <- compute(beer.net, beer.valid[-5:-10])idx <- apply(beer.prediction$net.result, 1, which.max)predicted <- c('ALE','IPA', 'PALE', 'STOUT', 'PORTER')[idx]table(predicted, beer.valid$Clase)predicted ALE IPA PALE PORTER STOUT   ALE     26   4    9      0     0   IPA      0 197   30      1     3   PALE    21  33   78      0     0   PORTER   0   1    0     10     6   STOUT    0   1    0     20    60Accuracy of model is calculated as follows((26+197+78+10+60)/nrow(beer.valid))*10074.2ConclusionAs you can see the accuracy is equal!I hope it will help you to develop your training.Never give up!See you in Linkedin!References:https://rpubs.com/vitorhs/irishttps://rstudio-pubs-static.s3.amazonaws.com/392220_1323b4203fb04ac4a326d20b6d445d63.html#loading-data-from-uci-ml-repositoryThe StartupMedium's largest active publication, followed by +723K people. Follow to join our community.Follow53 Public domain.Neural NetworkBeerRNnAnn53¬†claps53¬†clapsWritten byOscar RojoFollowCurrently studing a Master in Data Science. Passionate about learning new skills. Former branch risk analyst. https://www.linkedin.com/in/oscar-rojo-martin/FollowThe StartupFollowMedium's largest active publication, followed by +723K people. Follow to join our community.FollowWritten byOscar RojoFollowCurrently studing a Master in Data Science. Passionate about learning new skills. Former branch risk analyst. https://www.linkedin.com/in/oscar-rojo-martin/The StartupFollowMedium's largest active publication, followed by +723K people. Follow to join our community.More From MediumArithmetic Mean and Its Applications in Data AnalyticsMahbubul Alam in Towards AIDeploy a Digital Twin in 6 Months for $1M USDDanny CastonguayAn introduction to frequent pattern mining researchMadalina CiortanMaking Decisions with TreesSteven Loaiza in The StartupFor the love of regressionSyed Misbah in Data DecodedModelling Resale Flat Prices in Singapore | How do spatial attributes play a part?HiddenJellyfishCross-platform geospatial visualization with deck.gl-nativeGoogle Earth in Google Earth and Earth EngineAvoiding technical debt in social science researchSkye Toor in Pew Research Center: DecodedLearn more.Medium is an open platform where 170 million readers come to find insightful and dynamic thinking."
A Complete Introduction To Time Series Analysis (with R):: Linear processes I,https://medium.com/analytics-vidhya/a-complete-introduction-to-time-series-analysis-with-r-linear-processes-i-88a1b55db9ef?source=tag_archive---------1-----------------------,"Time Series Analysis,Machine Learning,Forecasting,R,Statistics","All linear processes can be expressed as a weighted sum of past noise, given certain conditions.In the last tutorial , we saw how we could express the probabilistic form of the best linear predictor of a future observation based on the data at hand. We will see how to implement this in R later! In this article, we will study an important class of time series: linear processes. Let‚Äôs jump right into it!q-correlation and strong stationarityHow can more rigorously define the stationarity of time series? How about ‚Äúsemi-stationarity‚Äù? We use the concept of q-correlation to do this.A time-series process is called strictly stationary or strongly stationary ifthat is, if the joint distribution of the observations X_{1}, ‚Ä¶, X_{n} is the same as the one of the h-lagged set of observations.PropertiesAll elements of a strongly stationary time series are identically distributed (but not necessarily independent!)The distribution of any subset of observations is the same as the h-lagged setFinite second moment implies weakly stationarityFor convenience, here‚Äôs once more the definition of weak stationarity:Note that strong stationarity is a much stronger condition! There are two important things to notice:Strong stationarity implies weak stationarity, but the opposite is not true!The I.I.D process is strongly stationary.The main question is: how do we construct / how can we characterize stationary processes? In particular, if we know that the IID is stationary, perhaps there is some mapping to more general sequences. In such a case, how can we determine their ‚Äúlevel of stationarity‚Äù? The following concepts.q-dependence and q-correlationWhat the previous proposition is saying is that if we have an IID sequence, under certain conditions, we can construct a new series that is also strongly stationary, using some function g. Further, we can define q-dependence by saying that observations |t-s| lags apart are independent, but everything in between is dependent, and similarly for correlation.Linear ProcessesNow we finally get to one of the most important parts of Time Series Analysis: linear processes.Example: MA(1)The MA(1) processExample: AR(1)The AR(1) processand we have thatIf you are curious about why the coefficients are that way, you can attempt to solve the recursion by plugging back the definition of X_{t} substituting {t-1}. Note that we have to play the restriction on the AR(1) coefficients so that it satisfies the conditions of a linear process as we described above.Linear processes in terms of the backward operatorRemember the backward shift operator that we saw back in the differencing section? We can also use it to represent linear processes.That is, we define the Psi operator as the infinite polynomial above, in which each term is exponentiated accordingly. If we plug in the backward shift operator, we then have a concrete way to represent the linear process briefly! This will become very useful to represent a more complex process without having to write down everything explicitly. Another way to see it is that we can see a linear process as an operation applied to the noise at time t.Next timeThat‚Äôs it for today! Next time, we will continue studying some more properties of the linear process, along with some interesting propositions, and other useful operators commonly used when dealing with Time Series. Until then!A Complete Introduction To Time Series Analysis (with R):: Linear processes IILast time, we left off at the representing Linear Processes in terms of the backward-shift operator:medium.comLast timePrediction 1 ‚Üí Best Linear Predictors IIA Complete Introduction To Time Series Analysis (with R):: Prediction 1 ‚Üí Best Predictors IIIn the last article, we saw how we could obtain the best linear predictor (BLP) of X_{n+h} given a function of X_{n}‚Ä¶medium.comMain pageA Complete Introduction To Time Series Analysis (with R)During these times of the Covid19 pandemic, you have perhaps heard about the collaborative efforts to predict new‚Ä¶medium.comFollow me athttps://blog.jairparraml.com/https://www.linkedin.com/in/hair-parra-526ba19b/https://github.com/JairParrahttps://medium.com/@hair.parraHair Parra ‚Äî Data Engineer / Software Engineer- DevX Analytics ‚Äî Cisco | LinkedInhttps://blog.jairparraml.com/ *** B.A. major Computer Science. Double minor in Statistics and Linguistics, at McGill‚Ä¶www.linkedin.comAnalytics VidhyaAnalytics Vidhya is a community of Analytics and Data‚Ä¶Follow5 Sign up for Data Science Blogathon: Win Lucrative Prizes!By Analytics VidhyaLaunching the Second Data Science Blogathon ‚Äì An Unmissable Chance to Write and Win Prizesprizes worth INR 30,000+!¬†Take a lookGet this newsletterBy signing up, you will create a Medium account if you don‚Äôt already have one. Review our Privacy Policy for more information about our privacy practices.Check your inboxMedium sent you an email at  to complete your subscription.Time Series AnalysisMachine LearningForecastingRStatistics5¬†claps5¬†clapsWritten byHair ParraFollowData Scientist & Data Engineer at Cisco, Canada. McGill University CS, Stats & Linguistics graduate. Polyglot.FollowAnalytics VidhyaFollowAnalytics Vidhya is a community of Analytics and Data Science professionals. We are building the next-gen data science ecosystem https://www.analyticsvidhya.comFollowWritten byHair ParraFollowData Scientist & Data Engineer at Cisco, Canada. McGill University CS, Stats & Linguistics graduate. Polyglot.Analytics VidhyaFollowAnalytics Vidhya is a community of Analytics and Data Science professionals. We are building the next-gen data science ecosystem https://www.analyticsvidhya.comMore From MediumSentiment Classification with BOWDipika Baad in The StartupSliding Window Price PredictionsLauren Faulds in Analytics VidhyaDemocratizing ML: Rise of the Teachable MachinesShuvam Manna in The StartupAutoencoders & Power of Mathematical OptimizationsChan Woo Kim in Human Machine LearningReview: Recursive Deep Models for Semantic Compositionality Over a Sentiment TreebankAnindya S. Das in The StartupMixed Formal Learning:Sandra Carrico in GLYNT.AIXGBoost Classifier Hand Written Digit recognitionNiketanpanchal in Analytics VidhyaHow to not build a three-point ROC curveOt√°vio Vasques in DataLab LogLearn more.Medium is an open platform where 170 million readers come to find insightful and dynamic thinking."
Predicting large text data with spark via the R package sparklyr,https://medium.com/@zumaia/predicting-large-text-data-with-spark-via-the-r-package-sparklyr-bfe102b2dce0?source=tag_archive---------2-----------------------,"Spark,R,Predict,Sparklyr,Wordcloud","Unlike the classical programming languages that are very slow and even sometimes fail to load very large data sets since they use only a single core, Apache Spark is known as the fastest distributed system that can handle with ease large datasets by deploying all the available machines and cores to build cluster, so that the computing time of each task performed on the data will be drastically reduced since each worker node in the cluster takes in charge small part of the task in question. Even that the native language of spark is scala (but it can also support java and sql), the good news for R users is that they can benefit from spark without having to learn the above supported languages by making use of the R package sparklyr. In this article we trained random forest model using text data which is in practice known as large data set. for illustration purposes and to make things faster however we used a small data set about email messages and also constrained ourselves to use the local mode in which spark created a cluster from the available cores in my machine. Notice that the same codes in this paper can be used in the cloud whatever the size of our data, even with billions of data points, except for the connection method to spark which is slightly different. Since the raw data requires some transformation to be consumed by the model, we applied the well-known method called tokenization to create the model features, then trained and evaluated a random forest model applied on the design matrix after having been filled using the TF method. Lastly, we trained the same model (random forest model with the same hyperparameter values) using another mothed called TF-IDF method (Sparck , 1972).Photo by Mahir Uysal on UnsplashKeywordsLarge dataset, R, spark, sparklyr, cluster, tokenization, TF, TF-IDF, random forest model, machine learning.IntroductionR is one of the best programming languages for statistical analysis, and provides data scientist by super powerful tools that make their work super easy and more exciting. However, since the amount of information today is growing exponentially, R and all the classical languages (python, java,‚Ä¶etc.) that use one single machine (one single core node) would face a great challenges to handle and deal with large dataset that, in some cases, its size can even exceed the memory size. As a solution to the above classical programming language limitations, spark and hadoop are two new systems. Both use a computing distributed system that run multiple tasks using multiple machines (called nodes, and together called cluster) at the same time. However, spark has the superiority over hadoop by its ability to load the data in memory which makes it much higher faster (Luraschi, 2014). Spark creates a cluster using either physical machines or virtual machines provided by some cloud provider such as google, amazon, microsoft‚Ä¶etc (it can also creat a cluster using the available cores in a single machine known as local mode). Its native language is scala, but also can support sql and java. Thankfully, spark provides a high level APIs in python and R so that the R users can use spark as a platform to work with large datasets using their familiar codes and without having to learn scala, sql or java. However, the connection between R and spark is not straightforward, it is set by the help of sparklyr package, which is like any other R packages, with its own functions and supports almost all the famous dplyr R package functions. Usually, most of text data are considered as large datasets, either due to their large sizes or the large computing time required for their manipulations or modulizations. That is why, in this paper, we will train Random forest model using sparklyr to predict whether a text message is spam or ham from the data set SMSSpamCollection uploaded from kaggle website. To convert the character features to numeric type we will use two famous methods , TF transformation, and TF-IDF (Jones, 1972) transformation. This article will be divided into the following sections:Data Preparation: we will illustrate how do we read, clean, and prepare the data to be consumed by the model.TF Method: we will train a random forest model (James et al, 2013) on the term frequency TF features.TF-IDF method: We will train the random forest model on the TF_IDF features.Add features: we will create another feature from the data to be used as a new predictor.InformationTo do this exercise my computer has worked on:Spark version 3.0.0Jupyter notebook 6.0.3R version 4.0.2 (2020‚Äì06‚Äì22)Data preparationFirst, we call the R packages tidyverse and sparklyr, and we set up the connection to spark using the following R codes.packages <- c(""sparklyr"", ""wordcloud"", ""tm"", ""slam"", ""NLP"", ""RColorBrewer"")newpack  = packages[!(packages %in% installed.packages()[,""Package""])]if(length(newpack)) install.packages(newpack)a=lapply(packages, library, character.only=TRUE)Let‚Äôs see the session infosessionInfo()R version 4.0.2 (2020-06-22)Platform: x86_64-pc-linux-gnu (64-bit)Running under: Ubuntu 20.04.1 LTSMatrix products: defaultBLAS:   /usr/lib/x86_64-linux-gnu/blas/libblas.so.3.9.0LAPACK: /usr/lib/x86_64-linux-gnu/lapack/liblapack.so.3.9.0locale: [1] LC_CTYPE=es_ES.UTF-8       LC_NUMERIC=C               [3] LC_TIME=es_ES.UTF-8        LC_COLLATE=es_ES.UTF-8     [5] LC_MONETARY=es_ES.UTF-8    LC_MESSAGES=es_ES.UTF-8    [7] LC_PAPER=es_ES.UTF-8       LC_NAME=C                  [9] LC_ADDRESS=C               LC_TELEPHONE=C            [11] LC_MEASUREMENT=es_ES.UTF-8 LC_IDENTIFICATION=C       attached base packages:[1] stats     graphics  grDevices utils     datasets  methods   base     other attached packages:[1] slam_0.1-47        tm_0.7-7           NLP_0.2-0          wordcloud_2.6     [5] RColorBrewer_1.1-2 sparklyr_1.3.1    loaded via a namespace (and not attached): [1] Rcpp_1.0.5        pillar_1.4.4      compiler_4.0.2    dbplyr_1.4.4      [5] r2d3_0.2.3        base64enc_0.1-3   tools_4.0.2       digest_0.6.25     [9] uuid_0.1-4        jsonlite_1.7.0    evaluate_0.14     tibble_3.0.2     [13] lifecycle_0.2.0   pkgconfig_2.0.3   rlang_0.4.6       IRdisplay_0.7.0  [17] DBI_1.1.0         rstudioapi_0.11   parallel_4.0.2    IRkernel_1.1     [21] xml2_1.3.2        repr_1.1.0        httr_1.4.1        dplyr_1.0.0      [25] generics_0.0.2    htmlwidgets_1.5.1 vctrs_0.3.1       rprojroot_1.3-2  [29] tidyselect_1.1.0  glue_1.4.1        forge_0.2.0       R6_2.4.1         [33] pbdZMQ_0.3-3      purrr_0.3.4       blob_1.2.1        magrittr_1.5     [37] backports_1.1.8   ellipsis_0.3.1    htmltools_0.5.0   assertthat_0.2.1 [41] crayon_1.3.4suppressPackageStartupMessages(library(sparklyr))suppressPackageStartupMessages(library(tidyverse))sc<-spark_connect(master = ""local"")Second, get the sms saved on my smartphone with an app.I use ‚ÄúSMS Backup & Restore‚ÄùSMS Backup & Restore - Aplicaciones en Google PlaySMS Backup & Restore is an app that backs up (creates a copy of) SMS & MMS messages and call logs currently available‚Ä¶play.google.comThird, we load the xml data and transform in dataframelibrary(xml2)x <- read_xml(""sms.xml"")xml_name(x)‚Äòallsms‚Äôxml_children(x){xml_nodeset (463)} [1] <sms address=""Allianz"" time=""29 jul. 2020 10:00:41"" date=""1596009641280"" ... [2] <sms address=""Loterias"" time=""26 jul. 2020 16:54:11"" date=""1595775251153 ... [3] <sms address=""Loterias"" time=""26 jul. 2020 13:10:24"" date=""1595761824372 ... [4] <sms address=""Lowi"" time=""24 jul. 2020 18:58:49"" date=""1595609929489"" ty ... [5] <sms address=""Lowi"" time=""24 jul. 2020 18:58:22"" date=""1595609902331"" ty ... [6] <sms address=""TARJETACTF"" time=""24 jul. 2020 12:38:37"" date=""15955871177 ... [7] <sms address=""SANT-ENVIOS"" time=""21 jul. 2020 13:20:07"" date=""1595330407 ... [8] <sms address=""Hawkers Co"" time=""21 jul. 2020 10:02:15"" date=""15953185358 ... [9] <sms address=""powert"" time=""20 jul. 2020 14:12:52"" date=""1595247172541""  ...[10] <sms address=""SEGURIDAD"" time=""20 jul. 2020 14:06:35"" date=""159524679541 ...[11] <sms address=""Loterias"" time=""19 jul. 2020 13:54:36"" date=""1595159676865 ...[12] <sms address=""34973900233"" time=""19 jul. 2020 13:07:31"" date=""1595156851 ...[13] <sms address=""34973900233"" time=""19 jul. 2020 13:07:10"" date=""1595156830 ...[14] <sms address=""217512"" time=""15 jul. 2020 17:33:20"" date=""1594827200346""  ...[15] <sms address=""INFOSNET"" time=""15 jul. 2020 14:19:11"" date=""1594815551892 ...[16] <sms address=""Loterias"" time=""15 jul. 2020 12:34:00"" date=""1594809240461 ...[17] <sms address=""ING"" time=""14 jul. 2020 10:45:36"" date=""1594716336196"" typ ...[18] <sms address=""Fintonic"" time=""14 jul. 2020 10:29:56"" date=""1594715396086 ...[19] <sms address=""INFOSNET"" time=""13 jul. 2020 14:41:10"" date=""1594644070835 ...[20] <sms address=""SegSocial"" time=""13 jul. 2020 9:46:39"" date=""1594626399661 ......baz <- xml_find_all(x, "".//sms"")xml_path(baz)address <- xml_attr(baz, ""address"")time <- xml_attr(baz, ""time"")type <- xml_attr(baz, ""type"")date <- xml_attr(baz, ""date"")body <- xml_attr(baz, ""body"")read <- xml_attr(baz, ""read"")service <- xml_attr(baz, ""service_center"")name <- xml_attr(baz, ""name"")X = data.frame(address, time, type, date, body, read, service, name, stringsAsFactors = FALSE)names(X)<-c(""sender"", ""time"",""label"",""ref"",""message"",""read"",""phone_number"",""name"")X$label <- str_replace(X$label, ""1"", ""spam"")X$label <- str_replace(X$label, ""2"", ""ham"")‚Äò/allsms/sms[1]‚Äô‚Äò/allsms/sms[2]‚Äô‚Äò/allsms/sms[3]‚Äô‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Äò/allsms/sms[461]‚Äô‚Äò/allsms/sms[462]‚Äô‚Äò/allsms/sms[463]‚Äôwrite.table(X, file = ""sms.txt"", sep = ""\t"",            row.names = TRUE, col.names = NA)path <- ""sms.txt""mydata<-spark_read_csv(sc,name=""SMS"",path=path, header=TRUE, delimiter = ""\t"",overwrite = TRUE)knitr::kable(head(mydata,2))| _c0|sender   |time                  |label |          ref|message                                                                                                                                                 | read| phone_number|name                    ||---:|:--------|:---------------------|:-----|------------:|:-------------------------------------------------------------------------------------------------------------------------------------------------------|----:|------------:|:-----------------------||   1|Allianz  |29 jul. 2020 10:00:41 |spam  | 1.596010e+12|Allianz METEO: Alerta en DONOSTIA-SAN SEBASTIAN ma√±ana por: Temperaturas elevadas.  Mas info. 902300186  https://www.allianz.es/centro-respuesta-rapida |    1|  34609090909|NA                      ||   2|Loterias |26 jul. 2020 16:54:11 |spam  | 1.595775e+12|Tu apuesta PRIM en www.loteriasyapuestas.es ha sido premiada con 2,00 EUR. Ya dispones del importe en tu Lotobolsa                                      |    1| summary(mydata)Length Class          Modesrc 1      src_spark      listops 2      op_base_remote listsdf_dim(mydata)4639str(mydata)List of 2 $ src:List of 1  ..$ con:List of 13  .. ..$ master      : chr ""local[4]""  .. ..$ method      : chr ""shell""  .. ..$ app_name    : chr ""sparklyr""  .. ..$ config      :List of 6  .. .. ..$ spark.env.SPARK_LOCAL_IP.local           : chr ""127.0.0.1""  .. .. ..$ sparklyr.connect.csv.embedded            : chr ""^1.*""  .. .. ..$ spark.sql.legacy.utcTimestampFunc.enabled: logi TRUE  .. .. ..$ sparklyr.connect.cores.local             : int 4  .. .. ..$ spark.sql.shuffle.partitions.local       : int 4  .. .. ..$ sparklyr.shell.driver-memory             : chr ""2g""  .. .. ..- attr(*, ""config"")= chr ""default""  .. .. ..- attr(*, ""file"")= chr ""/home/oscar/R/x86_64-pc-linux-gnu-library/4.0/sparklyr/conf/config-template.yml""  .. ..$ state       :<environment: 0x55b8cbc89bd8>   .. ..$ extensions  :List of 5  .. .. ..$ jars        : chr(0)   .. .. ..$ packages    : chr(0)   .. .. ..$ initializers: list()  .. .. ..$ catalog_jars: chr(0)   .. .. ..$ repositories: chr(0)   .. ..$ spark_home  : chr ""/opt/spark""  .. ..$ backend     : 'sockconn' int 5  .. .. ..- attr(*, ""conn_id"")=<externalptr>   .. ..$ monitoring  : 'sockconn' int 6  .. .. ..- attr(*, ""conn_id"")=<externalptr>   .. ..$ gateway     : 'sockconn' int 4  .. .. ..- attr(*, ""conn_id"")=<externalptr>   .. ..$ output_file : chr ""/tmp/RtmpQqu9TZ/filea40f3932a5af_spark.log""  .. ..$ sessionId   : num 43247  .. ..$ home_version: chr ""3.0.0""  .. ..- attr(*, ""class"")= chr [1:3] ""spark_connection"" ""spark_shell_connection"" ""DBIConnection""  ..- attr(*, ""class"")= chr [1:3] ""src_spark"" ""src_sql"" ""src"" $ ops:List of 2  ..$ x   : 'ident' chr ""SMS""  ..$ vars: chr [1:9] ""_c0"" ""sender"" ""time"" ""label"" ...  ..- attr(*, ""class"")= chr [1:3] ""op_base_remote"" ""op_base"" ""op"" - attr(*, ""class"")= chr [1:4] ""tbl_spark"" ""tbl_sql"" ""tbl_lazy"" ""tbl""colnames(mydata)‚Äò_c0‚Äô‚Äòsender‚Äô‚Äòtime‚Äô‚Äòlabel‚Äô‚Äòref‚Äô‚Äòmessage‚Äô‚Äòread‚Äô‚Äòphone_number‚Äô‚Äòname‚ÄôWe can also take a look at some messages by displaying the first three rows.select(mydata,message)%>%  head(3) %>%   knitr::kable(""html"")<table> <thead>  <tr>   <th style=""text-align:left;""> message </th>  </tr> </thead><tbody>  <tr>   <td style=""text-align:left;""> Allianz METEO: Alerta en DONOSTIA-SAN SEBASTIAN ma√±ana por: Temperaturas elevadas.  Mas info. 902300186  https://www.allianz.es/centro-respuesta-rapida </td>  </tr>  <tr>   <td style=""text-align:left;""> Tu apuesta PRIM en www.loteriasyapuestas.es ha sido premiada con 2,00 EUR. Ya dispones del importe en tu Lotobolsa </td>  </tr>  <tr>   <td style=""text-align:left;""> Tu apuesta BONO en www.loteriasyapuestas.es ha sido premiada con 5,00 EUR. Ya dispones del importe en tu Lotobolsa </td>  </tr></tbody></table>Modeling text data requires special attention since most of the machine learning algorithms require numeric data, so how do we can transform the text entries in messages into numeric type?.The most well known approach is called tokenization, this simply means splitting each text in the column messages into small pieces called tokens (also called bag of words) in a way such that each token has meaningful effect to discriminating between the dependent variable labels. For example, if we think that arbitrary numbers or some symbols like / or dots ‚Ä¶ etc. do not have any discriminating impact then we can remove them from the entries.Each row in this data (which is labeled as ham or spam ) is considered as document ( 5574 documents in our case) that has a text (which is a collection of tokens), and the whole data after tokenization (as a rectangular matrix) is called corpus.To keep things simple let‚Äôs suppose that everything except the words are useless for predicting the labels, so we can use the function spark sql function regexp_replace to remove everything except letters, then we rename the resulted column cleaned.options(max.print=999999)require(sparklyr)#newdata <- X %<>%#    mutate(cleaned=str_replace(message,""[^a-zA-Z]"","" ""))%>%#    mutate(cleaned = tolower(cleaned))%>%#    select(type,cleaned)newdata<-mydata%>%    mutate(cleaned=regexp_replace(message,""[^a-zA-Z]"","" ""))%>%    mutate(cleaned=lower(cleaned))%>%    select(label,cleaned)#newdata%>%#    select(cleaned)%>%#    head(3)%>%#    knitr::kable()newdata%>%    select(cleaned)%>%    head(3)%>%    knitr::kable()|cleaned                                                                                                                                                 ||:-------------------------------------------------------------------------------------------------------------------------------------------------------||allianz meteo  alerta en donostia san sebastian ma ana por  temperaturas elevadas   mas info             https   www allianz es centro respuesta rapida ||tu apuesta prim en www loteriasyapuestas es ha sido premiada con      eur  ya dispones del importe en tu lotobolsa                                      ||tu apuesta bono en www loteriasyapuestas es ha sido premiada con      eur  ya dispones del importe en tu lotobolsa                                      |At this stage and before going ahead we should split the data between training set and testing set. However, since we have an imbalanced data with roughly 7,12% of ham‚Äôs and 92,87% of spam‚Äôs, we should preserve the proportion of the labels by splitting the data in a such way to get stratified samples.# newdata %>%#     group_by(type)%>%#     summarise(n = n()) %>%#     mutate(freq = n / sum(n))newdata%>%    group_by(label)%>%    count()%>%    collect()%>%    mutate(prop=n/sum(n))%>%    knitr::kable()|label |   n|      prop||:-----|---:|---------:||ham   |  33| 0.0712743||spam  | 430| 0.9287257|To accomplish this task by hand, first we filter the data between ham and spam, then each set will be split randomly between training set and testing set, and next we rbind together the training sets in one set and then we do the same thing for testing sets.dataham<-newdata%>%  filter(label==""ham"")dataspam<-newdata%>%  filter(label==""spam"")partitionham<-dataham%>%    sdf_random_split(training=0.8,test=0.2,seed = 111)partitionspam<-dataspam%>%    sdf_random_split(training=0.8,test=0.2,seed = 111)train<-sdf_bind_rows(partitionham$training,partitionspam$training)%>%    compute(""train"")test<-sdf_bind_rows(partitionham$test,partitionspam$test)%>%    compute(""test"")head(train)[38;5;246m# Source: spark<?> [?? x 2][39m  label cleaned                                                                   [3m[38;5;246m<chr>[39m[23m [3m[38;5;246m<chr>[39m[23m                                                                   [38;5;250m1[39m ham   [38;5;246m""[39m me parece bien la chapa[38;5;246m""[39m                                              [38;5;250m2[39m ham   [38;5;246m""[39mbuenas  estuve el sabado  dime cuando vas a pasar por la casa  tenemos‚Ä¶[38;5;250m3[39m ham   [38;5;246m""[39mbuenas  ma ana a las   h en alkain [38;5;246m""[39m                                   [38;5;250m4[39m ham   [38;5;246m""[39mbuenas  qtal  he estado en morlans  quieres una reforma semi completa ‚Ä¶[38;5;250m5[39m ham   [38;5;246m""[39mbuenas alfonso  acu rdate de mi  ya me diras cuando vas [38;5;246m""[39m              [38;5;250m6[39m ham   [38;5;246m""[39mbuenas alfonso  el grifo del agua se puede abrir    [38;5;246m""[39mhead(test)[38;5;246m# Source: spark<?> [?? x 2][39m  label cleaned                                                                   [3m[38;5;246m<chr>[39m[23m [3m[38;5;246m<chr>[39m[23m                                                                   [38;5;250m1[39m ham   [38;5;246m""[39mbuenas  como van mis ventanas    [38;5;246m""[39m                                     [38;5;250m2[39m ham   [38;5;246m""[39mbuenas  he visto que recoges electrodom sticos  tengo nevera  lavadora‚Ä¶[38;5;250m3[39m ham   [38;5;246m""[39mbuenas  mejor        que voy con leire [38;5;246m""[39m                               [38;5;250m4[39m ham   [38;5;246m""[39mno puedo hablar  ll mame luego [38;5;246m""[39m                                       [38;5;250m5[39m ham   [38;5;246m""[39mok[38;5;246m""[39m                                                                    [38;5;250m6[39m ham   [38;5;246m""[39mok  genial [38;5;246m""[39mTF modelSince machine learning models require inputs as numeric data, the common practice in text analysis thus is to convert each single text into tokens (or pieces) so that these tokens will be the features that can be used to discriminate between class labels, In our case, they are a simple words. Using the TF method, if a particular word exists in a particular document we assign the number of frequency of this word (or just 1 if we do not care about the frequency) in the corresponding cell in the design matrix (which is called Document Term Matrix DTM), otherwise we assign zero.this method will give us a very large and sparse rectangular matrix with huge number of features compared to the number of documents, that is why spark can help to handle this type of data. Due to its popularity, we will fit random forest model, which known as one of the most powerful machine learning models, to the transformed data. to be brief We will make use of the spark feature pipline that helps us to group all the following required steps to enable running the model:convert the dependent variable labels to integer type.tokenize the cleaned messages into words (tokens).remove stop words from the tokens since they tend to spread out randomly among documents.replace each term in each document by its frequency number.define the model that will be used (here random forest model).At the final step we use ml_random_forest function and we keep all the default values, for example, 20 for number of trees, 5 for the max depth, and gini as the impurity function, and do not forget to set the seed to get the result reproducible. lastly we call the ml_fit function to fit the model.pipline<-ml_pipeline(sc)%>%    ft_string_indexer(input_col = ""label"",output_col=""class"")%>%    ft_tokenizer(input_col = ""cleaned"", output_col=""words"")%>%    ft_stop_words_remover(input_col = ""words"",output_col = ""cleaned_words"")%>%    ft_count_vectorizer(input_col = ""cleaned_words"",                        output_col=""terms"",                        min_df=5,                        binary=TRUE)%>%    ft_vector_assembler(input_cols = ""terms"",                        output_col=""features"")%>%    ml_random_forest_classifier(label_col=""class"",                                features_col=""features"",                                seed=222)piplinePipeline (Estimator) with 6 stages<pipeline_a40f182d58ae>   Stages   |--1 StringIndexer (Estimator)  |    <string_indexer_a40f18afeb99>   |     (Parameters -- Column Names)  |      input_col: label  |      output_col: class  |     (Parameters)  |      handle_invalid: error  |      string_order_type: frequencyDesc  |--2 Tokenizer (Transformer)  |    <tokenizer_a40f18f1fc84>   |     (Parameters -- Column Names)  |      input_col: cleaned  |      output_col: words  |--3 StopWordsRemover (Transformer)  |    <stop_words_remover_a40f2e833f77>   |     (Parameters -- Column Names)  |      input_col: words  |      output_col: cleaned_words  |--4 CountVectorizer (Estimator)  |    <count_vectorizer_a40f55801106>   |     (Parameters -- Column Names)  |      input_col: cleaned_words  |      output_col: terms  |     (Parameters)  |      binary: TRUE  |      maxDF: 9223372036854775808  |      min_df: 5  |      min_tf: 1  |      vocab_size: 262144  |--5 VectorAssembler (Transformer)  |    <vector_assembler_a40f3142a26b>   |     (Parameters -- Column Names)  |      input_cols: terms  |      output_col: features  |--6 RandomForestClassifier (Estimator)  |    <random_forest_classifier_a40f39a3d1a3>   |     (Parameters -- Column Names)  |      features_col: features  |      label_col: class  |      prediction_col: prediction  |      probability_col: probability  |      raw_prediction_col: rawPrediction  |     (Parameters)  |      bootstrap: TRUE  |      cache_node_ids: FALSE  |      checkpoint_interval: 10  |      feature_subset_strategy: auto  |      impurity: gini  |      leafCol:   |      max_bins: 32  |      max_depth: 5  |      max_memory_in_mb: 256  |      min_info_gain: 0  |      min_instances_per_node: 1  |      minWeightFractionPerNode: 0  |      num_trees: 20  |      seed: 222  |      subsampling_rate: 1model_rf<-ml_fit(pipline,train)To evaluate our model we use the ml_transfrom function.ml_transform(model_rf,train)%>%  ml_binary_classification_evaluator(label_col = ""class"",                                     metric_name= ""areaUnderROC"")0.963448275862069Notice that in binary classification model sparklyr provides only two metrics areaUnderROC and areaUnderPR (Murphy, 2012). Using the former metric we get high score which is about 0.97.This rate is ranged between 0 and 1, The higher the rate the best the model. However, since this rate is resulted from the training data, it might be the result of an overfitting (Lantz, 2016) problem, that is why the more reliable one is that that resulted from the testing set,which is now 0.963.ml_transform(model_rf,test)%>%    ml_binary_classification_evaluator(label_col = ""class"",                                       metric_name= ""areaUnderROC"")0.935975609756098Fortunately The two rate values are very close to each other indicating the good generalization of our model.To get the prediction we use the ml_predict function .pred<-ml_predict(model_rf,test)As we see some columns are nested. This is not problem since you can extract the elements of this list using the function unlist. For instance, we can show the most used words in each class label using the package wordcloudp1<-pred%>%    filter(label==""ham"")%>%    pull(cleaned_words)%>%    unlist()library(""wordcloud"")wordcloud::wordcloud(p1,max.words = 50, random.order = FALSE,                     colors=c(""blue"",""red"",""green"",""yellow""),random.color = TRUE)Warning message in tm_map.SimpleCorpus(corpus, tm::removePunctuation):‚Äútransformation drops documents‚ÄùWarning message in tm_map.SimpleCorpus(corpus, function(x) tm::removeWords(x, tm::stopwords())):‚Äútransformation drops documents‚Äùp2<-pred%>%    filter(label==""spam"")%>%    pull(cleaned_words)%>%    unlist()wordcloud::wordcloud(p2,max.words = 50,random.order = FALSE,                      colors=c(""blue"",""red"",""green"",""purple""),random.color = TRUE)Warning message in tm_map.SimpleCorpus(corpus, tm::removePunctuation):‚Äútransformation drops documents‚ÄùWarning message in tm_map.SimpleCorpus(corpus, function(x) tm::removeWords(x, tm::stopwords())):‚Äútransformation drops documents‚ÄùFrom the upper figure we see that the most common words in hm‚Äôs are: get, good, know, whereas the lower figure shows the most ones for spam‚Äôs, which are: call, free, mobile. This means that if we receive a new email message that has the word free for instance , it will be more probable to be spam.TF-IDF modelThe main drawback of TF method is that it does not take into account the distribution of each term across the documents that reflects how much information each term provides. To measure the information of each term we compute its DF document frequency value which is the number of documents d where the term t appears, and hence the inverse document frequency IDF value for each pair (d,t) will be computed as follows:[idf(t,d)=log(\frac{N}{1+|d\epsilon D,t\epsilon d|})]Where N is the total number of documents (number of rows).By multiplying TF with IDF we get TF-IDF value for each term. In the above TF pipline we include the function ft_idf , then we fit again random forest model on the transformed data, and we evaluate the model directly by using the test data.pipline2<-ml_pipeline(sc)%>%    ft_string_indexer(input_col = ""label"",output_col=""class"")%>%    ft_tokenizer(input_col = ""cleaned"", output_col=""words"")%>%    ft_stop_words_remover(input_col = ""words"",                          output_col = ""cleaned_words"")%>%    ft_count_vectorizer(input_col = ""cleaned_words"",                         output_col=""tf_terms"")%>%    ft_idf(input_col = ""tf_terms"", output_col=""tfidf_terms"")%>%    ml_random_forest_classifier(label_col=""class"",                                 features_col=""tfidf_terms"",                                seed=222)model_rf.tfidf <- ml_fit(pipline2, train)ml_transform(model_rf.tfidf,test)%>%    ml_binary_classification_evaluator(label_col = ""class"",                                       metric_name= ""areaUnderROC"")0.929878048780488Using this more complex model than the previous one is not justified for this data since their rates are close to each other.Add new featuresCustomizing new features from the data that we think they are more relevant than the old ones is a popular strategy used to improve prediction quality. For example, with our data we think that spam messages tend to be shorter than ham messages, we can, thus, add the messages‚Äô lengths as new features.train1 <- train %>% mutate(lengths=nchar(cleaned))test1 <- test %>% mutate(lengths=nchar(cleaned))Now let‚Äôs retrain the above models again with this new added feature.TF modelpipline_tf<-ml_pipeline(sc)%>%    ft_string_indexer(input_col = ""label"",output_col=""class"")%>%    ft_tokenizer(input_col = ""cleaned"", output_col=""words"")%>%    ft_stop_words_remover(input_col = ""words"",                          output_col = ""cleaned_words"")%>%    ft_count_vectorizer(input_col = ""cleaned_words"",                        output_col=""terms"",                        min_df=5,                        binary=TRUE)%>%    ft_vector_assembler(input_cols = c(""terms"",""lengths""),                        output_col=""features"")%>%    ml_random_forest_classifier(label_col=""class"",                                features_col=""features"",                                seed=222)model_rf_new<-ml_fit(pipline_tf,train1)ml_transform(model_rf_new,test1)%>%    ml_binary_classification_evaluator(label_col = ""class"",                                       metric_name= ""areaUnderROC"")0.9375Fortunately, our expectation about this new feature is confirmed since we have got a significant improvement compared to the previous results.tf_idf modelpipline_tfidf<-ml_pipeline(sc)%>%    ft_string_indexer(input_col = ""label"",                      output_col=""class"")%>%    ft_tokenizer(input_col = ""cleaned"",                  output_col=""words"")%>%    ft_stop_words_remover(input_col = ""words"",                          output_col = ""cleaned_words"")%>%    ft_count_vectorizer(input_col = ""cleaned_words"",                         output_col=""tf_terms"")%>%    ft_idf(input_col = ""tf_terms"", output_col=""tfidf_terms"")%>%    ft_vector_assembler(input_cols = c(""tfidf_terms"",""lengths""),                        output_col=""features"")%>%    ml_random_forest_classifier(label_col=""class"",                                features_col=""features"",seed=222)model_rf_new2 <- ml_fit(pipline_tfidf, train1)ml_transform(model_rf_new2,test1)%>%    ml_binary_classification_evaluator(label_col = ""class"",                                       metric_name= ""areaUnderROC"")0.967987804878049Again, as we said before, the use of idf method is not justified, and it would be better to stay with the tf method.n-gram modelIn contrast to the function ft_tokenizer that splits the text into tokens where each token has a single word, each token resulted from the sparklyr function ft_ngram has n words respecting the same appearance order as in the original text. To well understand let‚Äôs take the following example.data <- copy_to(sc, data.frame(x=""I like both R and python""), overwrite = TRUE)data[38;5;246m# Source: spark<?> [?? x 1][39m  x                         [3m[38;5;246m<chr>[39m[23m                   [38;5;250m1[39m I like both R and pythonthe ft_tokenizer function gives the following tokens:ft_tokenizer(data, ""x"", ""y"") %>%     mutate(y1=explode(y)) %>%     select(y1)[38;5;246m# Source: spark<?> [?? x 1][39m  y1      [3m[38;5;246m<chr>[39m[23m [38;5;250m1[39m i     [38;5;250m2[39m like  [38;5;250m3[39m both  [38;5;250m4[39m r     [38;5;250m5[39m and   [38;5;250m6[39m pythonWhereas, with ft_ngram, where (n=2) we get the following tokensdata  %>%      ft_tokenizer(""x"", ""y"") %>%     ft_ngram(""y"", ""y1"", n=2) %>%    mutate(z=explode(y1)) %>%     select(z)[38;5;246m# Source: spark<?> [?? x 1][39m  z           [3m[38;5;246m<chr>[39m[23m     [38;5;250m1[39m i like    [38;5;250m2[39m like both [38;5;250m3[39m both r    [38;5;250m4[39m r and     [38;5;250m5[39m and pythonNow let‚Äôs train 2_gram Random forest model.pipline_2gram<-ml_pipeline(sc)%>%    ft_string_indexer(input_col = ""label"",                      output_col=""class"")%>%    ft_tokenizer(input_col = ""cleaned"",                  output_col=""words"")%>%    ft_stop_words_remover(input_col = ""words"",                          output_col = ""cleaned_words"")%>%    ft_ngram(input_col = ""cleaned_words"",              output_col=""ngram_words"", n=2) %>%     ft_count_vectorizer(input_col = ""ngram_words"",                         output_col=""tf_terms"")%>%    ft_vector_assembler(input_cols = c(""tf_terms"",""lengths""),                        output_col=""features"")%>%    ml_random_forest_classifier(label_col=""class"",                                features_col=""features"",seed=222)model_rf_2gram <- ml_fit(pipline_2gram, train1)ml_transform(model_rf_2gram,test1)%>%    ml_binary_classification_evaluator(label_col = ""class"",                                        metric_name= ""areaUnderROC"")0.803353658536585You should know that this function takes only tokens with tow words exactly, not tokens with less or equal 2 words. That is why we have obtained a lower rate than the previous models.When you are satisfied by your final model, you can save it for further use as follows.#ml_save(model_rf_ngram,""spark_ngram"",overwrite = TRUE)The last thing to mention, is when you finish your work do not forget to free your resources by disconnecting from spark as followsspark_disconnect(sc)Conclusion:This article is a brief introduction to illustrate how easy to handle and model large data set with the combination of the two powerful languages R and spark. we have used a text data set since this type of data that characterizes the most large datasets encountered in the real world.I hope it will help you to develop your training.Never give up!See you in Linkedin!References:https://spark.rstudio.comhttps://www.r-bloggers.com/predicting-large-text-data-with-spark-via-the-r-package-sparklyr/Written byOscar RojoCurrently studing a Master in Data Science. Passionate about learning new skills. Former branch risk analyst. https://www.linkedin.com/in/oscar-rojo-martin/FollowPublic domain.SparkRPredictSparklyrWordcloudMore from Oscar RojoFollowCurrently studing a Master in Data Science. Passionate about learning new skills. Former branch risk analyst. https://www.linkedin.com/in/oscar-rojo-martin/More From MediumStories from the city, stories from the cloud: an introduction to city open data portals in the‚Ä¶Zo√´ Wilkinson Salda√±a in Messy DataWhat is Principal Component Analysis (PCA) and when should I use it?Jose CachoComprehensive Guide To Optimize Your Pandas CodeEyal Trabelsi in Towards AIHealthcare Analytics: Exploration vs. ConfirmationElder Research, Inc.What Are You Going to Write About for Nightingale?Isaac Levy-Rubinett in NightingaleDescriptive Statistics: Measures of SpreadFrancis Morales in The StartupNovel Coronavirus: Simple analysis and predictionsAlberto Naranjo in The StartupAirflow for Data ScientistsSTATWORX BlogAboutHelpLegalGet the Medium app"
Things you must know before starting any programming language,https://medium.com/@shresthvarshney980/thinks-you-must-know-before-starting-any-programming-language-9b3234acb768?source=tag_archive---------3-----------------------,"Programming,Coding,Python,R,Programming Languages","What‚Äôs Programming?Programming is the way to communicate with the computer and give instructions to system that how it has to work. Every application that we use in our system is programmed by using any programming language.Why Programming?Computer works on binary digits i.e., 0 and 1 and it is very tough for human to understand computer language. So human has created a middle path for us to communicate with system easily and the middle path is called as programming. In today‚Äôs world when everything is going in momentum with fast growing technologies we need to communicate with our system easily and this target is achieved when our system is highly programmed.How‚Äôs Programming?Programming can be done by programming languages. Programming languages are understandable by both human as well as system. They are like a bridge between human language and computer language. Every programming language has its own syntax and permanent keywords via which we can program our system.What‚Äôs behind programming?When we write any code in our system in any programming language the system first compile the code by compiler. Compiler is a device which change code language to system language so that it can be understandable by system. There are many types of compiler but here we are not concern about it.How we can do programming on our system?To do programming on our system we have to learn one programming language. There are many types of programming languages like C, C++, C#, Python, Java, R etc.Before starting any language let us first compare the syntax of all programming language with same output ‚ÄúHello World!‚ÄùC Syntax:#include<stdio.h>int main(){printf(‚ÄúHello World‚Äù);return 0;}C++ Syntax:#include<iostream>int main(){std::cout << ‚ÄúHello World!‚Äù;return 0;}C# Syntax:namespace HelloWorld{class Hello{static void Main(string[] args){System.Console.WriteLine(‚ÄúHello World!‚Äù);}}}Java Syntax:class HelloWorld{public static void main(String[] args){System.out.println(‚ÄúHello, World!‚Äù);}}Python Syntax:print(‚ÄúHello World!‚Äù)R Syntax:myString <- ‚ÄúHello, World!‚Äùprint ( myString)First Step after deciding any programming language:Firstly we have to decide that which programming language we want to learn and which is best for our carrier. Then instead of going for any paid course we first go for free beginner course of that language and if we are comfortable with that language then only we have to go for any paid courses.Here are some free courses to learn any programming language:Link for free C Course to learn: The Complete C programmingLink for free C++ Course to learn: C++ Tutorial for Complete BeginnersLink for free C# Course to learn: Diploma in C# ProgrammingLink for free Java Course to learn: Java Tutorial for Complete BeginnersLink for free Python Course to learn: Python for BeginnerLink for free R Course to learn: R Basics ‚Äî R Programming Language IntroductionWritten byShresth VarshneyFollow3 3¬†3¬†ProgrammingCodingPythonRProgramming LanguagesMore from Shresth VarshneyFollowMore From MediumMigrating to C++ From Other LanguagesAlain GalvanConditional imports across Flutter and WebAntonello Galip√≤ in Flutter CommunityIntroducing Kubectl Flame: Effortless Profiling on KubernetesEden Federman in The StartupSQL Server Transactions and Isolation LevelsMatt Eland in The StartupWhat Is the Walrus Operator in Python?Jonathan Hsu in Better ProgrammingCxO Abstract: Serverless Applications using AWSTravis Giffin in The StartupHow to Grant Access to Private Files Using Pre-Signed URLs on AWSNasi Jofche in Better ProgrammingPerforming Optical Character Recognition with Python and Pytesseract using AnacondaPranav Manoj in Analytics VidhyaAboutHelpLegalGet the Medium app"
N/A,https://medium.com/@cmbosma/create-new-column-based-on-values-in-another-column-in-r-ea49075f7691?source=tag_archive---------4-----------------------,R,"Create New Column Based on Values in Another Column in RHere is a short post on how to create a new column with the values from one column based on the values in separate column. There are a few situations where this might be useful. One is if you have data in long format with your surveys questions in one column and you want to separate the scores of one of the surveys into a separate column for time-series/longitudinal analyses. Another situation might be that you have several conditions and you want to create a column of scores retaining scores of a subset of the conditions.I recently needed to do this to tidy up some data in long format and noticed that there are no obvious blog posts or stack overflow entries addressing this data tidying scenario. The solutions you will find come with a caveat: if you create a new vector based on values in another vector, you will likely generate a vector with a length that does not match that of the data frame, precluding you from adding it to the data frame.I will provide an abstract example and then a real-life example.Below we have a table with columns varA and varB. We want to make a new column, varC, with values corresponding to rows with ‚Äòc‚Äô in the varA column.varA varB a 12 b 34 c 23 a 34 b 23 c 13Let‚Äôs first illustrate the problem. For example, you would think that df$newVar <- df$varB[(varA == ""c"")] would create a new column with values from varB that correspond with varA. The problem is that this approach, and many other approaches, will create a vector the length of the cases that fit the parameters you have for the new vector. In this case, since there are only two rows with ‚Äúc‚Äù, the length of the new vector would be 2.Okay, now let‚Äôs work on the solution. We can use the mutate and ifelse functions from the tidyverse to accomplish our task:# Load in the `tidyverse` package for the `dplyr` package tidying functions and piping.library(tidyverse)df <- df %>%     mutate(varC = ifelse(varA == ""b"", varB, NA)What does this code do?From the data frame, create varC using and ifelse statement and assign it to the original data frame.In the ifelse statement, we indicate that if values in varA equal ‚Äúb‚Äù, then keep the corresponding value from column varB, else NA.Here is the table you would get from this R code.varA varB varC a 12 NA b 34 34 c 23 NA a 34 NA b 23 23 c 13 NASince these abstract examples can be a little tricky to apply to our own stuff, here‚Äôs an example resembling a real data set, such as an experiential momentary assessment data set. Below is a table with data in long format. We want to make separate a column for negative affect subscale of the Positive and Negative Affect Schedule (PANAS-X) to conduct analyses solely on those scores. Although this can be accomplished with the data in wide format, you may want to keep it in long format.id time_index panas_x_subscale score 001 1 panas_x_sad 23 001 2 pans_x_pa 11 001 3 panas_x_na 6 002 1 panas_x_sad 3 002 2 panas_x_pa 2 002 3 panas_x_na 4 003 1 panas_x_sad 15 003 2 panas_x_pa 10 003 3 panas_x_na 8df <- df %>%    mutate(panas_x_na = ifelse(panas_x_subscale == ""panas_x_na"", score, NA)Below is a data table amended with a vector containing values copied from the score column corresponding with ‚Äúpanas_x_na‚Äù in the panas_x_subscale column.id time_index panas_x_subscale score panas_x_na 001 1 panas_x_sad 23 NA 001 2 pans_x_pa 11 NA 001 3 panas_x_na 6 6 002 1 panas_x_sad 3 NA 002 2 panas_x_pa 2 NA 002 3 panas_x_na 4 4 003 1 panas_x_sad 15 NA 003 2 panas_x_pa 10 NA 003 3 panas_x_na 8 8Written byColin M. BosmaFollowRMore from Colin M. BosmaFollowMore From MediumCreate Your Own Coefficient Plot Function in PythonJessica Forrest-Baldini in Analytics VidhyaHypothesis Testing for Inference using a DatasetJoju John Varghese in The StartupExperimenting with PySpark to Match Large Data SourcesCivis Analytics in The Civis JournalDecision Trees ClassifierArman Hussain in The StartupEvaluation Metrics for Classification Models Series‚Ää‚Äî‚ÄäPart 1:Ana Preciado ‚Äî Data Science To Go in The StartupConstructing a Career in Data Viz: Getting StartedWill Chase in NightingaleDimensions of Museum DataColin Brooks in Whitney DigitalOverview of data distributionsMadalina CiortanAboutHelpLegalGet the Medium app"
N/A,https://medium.com/@cmbosma/create-new-column-based-on-values-in-another-column-in-r-639303ff2694?source=tag_archive---------5-----------------------,R,"Create New Column Based on Values in Another Column in RHere is a short post on how to create a new column with the values from one column based on the values in separate column. There are a few situations where this might be useful. One is if you have data in long format with your surveys questions in one column and you want to separate the scores of one of the surveys into a separate column for time-series/longitudinal analyses. Another situation might be that you have several conditions and you want to create a column of scores retaining scores of a subset of the conditions.I recently needed to do this to tidy up some data in long format and noticed that there are no obvious blog posts or stack overflow entries addressing this data tidying scenario. The solutions you will find come with a caveat: if you create a new vector based on values in another vector, you will likely generate a vector with a length that does not match that of the data frame, precluding you from adding it to the data frame.I will provide an abstract example and then a real-life example.Below we have a table with columns varA and varB. We want to make a new column, varC, with values corresponding to rows with ‚Äòc‚Äô in the varA column.varA varB a 12 b 34 c 23 a 34 b 23 c 13Let‚Äôs first illustrate the problem. For example, you would think that df$newVar <- df$varB[(varA == ""c"")] would create a new column with values from varB that correspond with varA. The problem is that this approach, and many other approaches, will create a vector the length of the cases that fit the parameters you have for the new vector. In this case, since there are only two rows with ‚Äúc‚Äù, the length of the new vector would be 2.Okay, now let‚Äôs work on the solution. We can use the mutate and ifelse functions from the tidyverse to accomplish our task:# Load in the `tidyverse` package for the `dplyr` package tidying functions and piping.library(tidyverse)df <- df %>%     mutate(varC = ifelse(varA == ""b"", varB, NA)What does this code do?From the data frame, create varC using and ifelse statement and assign it to the original data frame.In the ifelse statement, we indicate that if values in varA equal ‚Äúb‚Äù, then keep the corresponding value from column varB, else NA.Here is the table you would get from this R code.varA varB varC a 12 NA b 34 34 c 23 NA a 34 NA b 23 23 c 13 NASince these abstract examples can be a little tricky to apply to our own stuff, here‚Äôs an example resembling a real data set, such as an experiential momentary assessment data set. Below is a table with data in long format. We want to make separate a column for negative affect subscale of the Positive and Negative Affect Schedule (PANAS-X) to conduct analyses solely on those scores. Although this can be accomplished with the data in wide format, you may want to keep it in long format depending on the analyses or data visualization you are interested in conducting.id time_index panas_x_subscale score 001 1 panas_x_sad 23 001 2 pans_x_pa 11 001 3 panas_x_na 6 002 1 panas_x_sad 3 002 2 panas_x_pa 2 002 3 panas_x_na 4 003 1 panas_x_sad 15 003 2 panas_x_pa 10 003 3 panas_x_na 8df <- df %>%    mutate(panas_x_na = ifelse(panas_x_subscale == ""panas_x_na"", score, NA)Below is a data table amended with a vector containing values copied from the score column corresponding with ‚Äúpanas_x_na‚Äù in the panas_x_subscale column.id time_index panas_x_subscale score panas_x_na 001 1 panas_x_sad 23 NA 001 2 pans_x_pa 11 NA 001 3 panas_x_na 6 6 002 1 panas_x_sad 3 NA 002 2 panas_x_pa 2 NA 002 3 panas_x_na 4 4 003 1 panas_x_sad 15 NA 003 2 panas_x_pa 10 NA 003 3 panas_x_na 8 8Written byColin M. BosmaFollowRMore from Colin M. BosmaFollowMore From MediumPython Getting Started Tutorial: Scientific Calculation with PandasData Analysis EnthusiastHouse Sales Predictor Using Deep LearningUmair Ayub in The StartupHow I measured your programming work by accessing online data using the REST API(Blog+ Tutorial).Garry Tiscovschi in Data SolsticeFinding Potentially Habitable Exoplanets in the NASA Dataset using PythonAdarsh Bulusu in The StartupHow to Get Financial Ratios Using PythonIbrahim Saidi in Data Driven InvestorConstructing a Career in Data Visualization: The HowWill Chase in Nightingale5 Scraping TipsArtem Rys in python4youDo NBA Superstars Really Get All the Calls?Peter Li in SportsRaidAboutHelpLegalGet the Medium app"
N/A,https://medium.com/@cmbosma/create-new-column-based-on-values-in-another-column-in-r-5232befe3708?source=tag_archive---------6-----------------------,R,"Create New Column Based on Values in Another Column in RHere is a short post on how to create a new column with the values from one column based on the values in separate column. There are a few situations where this might be useful. One is if you have data in long format with your surveys questions in one column and you want to separate the scores of one of the surveys into a separate column for time-series/longitudinal analyses. Another situation might be that you have several conditions and you want to create a column of scores retaining scores of a subset of the conditions.I recently needed to do this to tidy up some data in long format and noticed that there are no obvious blog posts or stack overflow entries addressing this data tidying scenario. The solutions you will find come with a caveat: if you create a new vector based on values in another vector, you will likely generate a vector with a length that does not match that of the data frame, precluding you from adding it to the data frame.I will provide an abstract example and then a real-life example.Below we have a table with columns varA and varB. We want to make a new column, varC, with values corresponding to rows with ‚Äòc‚Äô in the varA column.varA varB a 12 b 34 c 23 a 34 b 23 c 13Let‚Äôs first illustrate the problem. For example, you would think that df$newVar <- df$varB[(varA == ""c"")] would create a new column with values from varB that correspond with varA. The problem is that this approach, and many other approaches, will create a vector the length of the cases that fit the parameters you have for the new vector. In this case, since there are only two rows with ‚Äúc‚Äù, the length of the new vector would be 2.Okay, now let‚Äôs work on the solution. We can use the mutate and ifelse functions from the tidyverse to accomplish our task:# Load in the `tidyverse` package for the `dplyr` package tidying functions and piping.library(tidyverse)df <- df %>%     mutate(varC = ifelse(varA == ""b"", varB, NA)What does this code do?From the data frame, create varC using and ifelse statement and assign it to the original data frame.In the ifelse statement, we indicate that if values in varA equal ‚Äúb‚Äù, then keep the corresponding value from column varB, else NA.Here is the table you would get from this R code.varA varB varC a 12 NA b 34 34 c 23 NA a 34 NA b 23 23 c 13 NASince these abstract examples can be a little tricky to apply to our own stuff, here‚Äôs an example resembling a real data set, such as an experiential momentary assessment data set. Below is a table with data in long format. We want to make separate a column for negative affect subscale of the Positive and Negative Affect Schedule (PANAS-X) to conduct analyses solely on those scores. Although this can be accomplished with the data in wide format, you may want to keep it in long format depending on the analyses or data visualization you are interested in conducting.id time_index panas_x_subscale score 001 1 panas_x_sad 23 001 2 pans_x_pa 11 001 3 panas_x_na 6 002 1 panas_x_sad 3 002 2 panas_x_pa 2 002 3 panas_x_na 4 003 1 panas_x_sad 15 003 2 panas_x_pa 10 003 3 panas_x_na 8df <- df %>%    mutate(panas_x_na = ifelse(panas_x_subscale == ""panas_x_na"", score, NA)Below is a data table amended with a vector containing values copied from the score column corresponding with ‚Äúpanas_x_na‚Äù in the panas_x_subscale column.id time_index panas_x_subscale score panas_x_na 001 1 panas_x_sad 23 NA 001 2 pans_x_pa 11 NA 001 3 panas_x_na 6 6 002 1 panas_x_sad 3 NA 002 2 panas_x_pa 2 NA 002 3 panas_x_na 4 4 003 1 panas_x_sad 15 NA 003 2 panas_x_pa 10 NA 003 3 panas_x_na 8 8Written byColin M. BosmaFollowRMore from Colin M. BosmaFollowMore From MediumFrom Data Science to Knowledge ScienceDan McCrearyComputing Histograms Using Java ArraysRishi Sidhu in Data Driven InvestorData Analysis of Bike Sharing Rental using PythonSriyanda Afrida WAssociation rules: Operation of the apriori algorithmMartin Bernaola in Analytics VidhyaSlicing and Dicing with PandasVeena PrabhakaranUsing Regression modeling to predict purchasing habits at StarbucksRavish Chawla in ML 2 VecWeb Scraping with PythonBetaLabPython Deep Learning: Part 1Jon C-137AboutHelpLegalGet the Medium app"
Estat√≠stica e Sorvete‚Ä¶,https://medium.com/data-hackers/estat%C3%ADstica-e-sorvete-435438cdd490?source=tag_archive---------0-----------------------,"Estatistica,R,Ciencia De Dados,Data Science","Photo by Irene Kredenets on UnsplashResumoNeste artigo, voc√™ aprender√° um pouco sobre c√°lculos de probabilidade no R Studio. Por se tratar de uma linguagem estat√≠stica, o R j√° vem com v√°rios testes inclu√≠dos na base da ferramenta, com fun√ß√µes que podem lhe poupar muito trabalho se voc√™ souber como us√°-las.Falaremos sobre tr√™s deles aqui:Probabilidade para Distribui√ß√µes BinomiaisProbabilidade para distribui√ß√µes de PoissonProbabilidade para distribui√ß√µes normaisAntes de come√ßar ‚Ä¶Certo, voc√™ leu o resumo, est√° interessado no t√≥pico, mas ainda n√£o entendeu o que o sorvete tem a ver com tudo isso, certo?Bom, eu s√≥ queria trabalhar neste artigo com um conjunto de dados sobre sorvete. Nada mais. Aqui est√° como voc√™ pode criar um pequeno dataset no R Studio:ice_cream <- data.frame(month= c(1,2,3,4,5,6,7,8,9,10,11,12),                        sales= sample(100:500,                                      size=12, replace=T,                                       set.seed(12)),                        customers= sample(50:450,                                          size=12, replace=T,                                           set.seed(12)))| month| sales| customers||-----:|-----:|---------:||     1|   127|        77||     2|   427|       377||     3|   477|       427||     4|   208|       158||     5|   167|       117||     6|   113|        63||     7|   171|       121||     8|   357|       307||     9|   109|        59||    10|   103|        53||    11|   257|       207||    12|   426|       376|Tudo pronto. Vamos la!Distribui√ß√£o BinomialDistribui√ß√µes binomiais, como o nome j√° diz, s√£o aquelas em que podemos obter dois resultados poss√≠veis: Sim / N√£o, Certo / Errado, Verdadeiro / Falso, Sucesso / Falha.Este teste √© √∫til quando voc√™ precisa saber a probabilidade de um evento ocorrer se voc√™ tentar ‚Äôn‚Äô vezes.Usando nosso exemplo de sorvete, imagine que nossa loja tenha 15 ta√ßas predefinidas no menu, mas quer√≠amos nos concentrar na venda de Sundae. Se quis√©ssemos saber qual √© a probabilidade de uma pessoa entrar na loja e escolher um Sundae entre todas as outras 15 op√ß√µes, cair√≠amos em um problema cl√°ssico de estat√≠stica: a chance √© de 1/15 (6,67%), certo?Mas sabendo que temos mais de um cliente por dia, qual seria a probabilidade de 5 clientes escolherem um Sundae a cada 30 vendas? Bem, agora nosso problema poderia ser um pouco mais complicado de calcular, mas n√£o √©, pois usaremos o teste Binomial no R Studio.O teste Binomial √© muito simples de executar. Voc√™ pode usar a fun√ß√£o a seguir, onde o primeiro par√¢metro ser√° o n√∫mero de sucessos que voc√™ est√° medindo (x); o size aqui √© o n√∫mero de vezes que o evento vai acontecer, o n√∫mero de tentativas (n√£o deve ser confundido com o tamanho da amostra); e a probabilidade de sucesso que voc√™ tem.dbinom(x= n√∫mero de sucessos,       size = n√∫mero de eventos ou tentativas,       prob = probabilidade de sucesso)Ent√£o, resumindo:Problema 1: Qual √© a probabilidade de 5 em 30 clientes escolherem Sundae no menu?M√©todo: Teste binomial = Escolha Sundae ou N√ÉO Sundae.Probabilidade de sucesso: escolha 1 entre 15 op√ß√µes de menu.Quantidade de eventos: 30 clientes = 30 transa√ß√µes de vendas.Teste de sucesso: 5 pessoas escolheram sundae no menu.dbinom(x= 5, size= 30, prob= 1/15)[1] 0.03  # 3% de chance.E tem mais. Se quisermos testar a probabilidade acumulada de 5 ou mais pessoas de escolher um Sundae (5, 6, 7,‚Ä¶ .30), h√° uma fun√ß√£o para isso tamb√©m. Podemos usar pbinom, que √© bastante semelhante ao dbinom, mas nos traz o par√¢metro lower.tail, usado como TRUE quando voc√™ deseja verificar um determinado n√∫mero de sucessos ou menos (q inclusivo) e como FALSE quando voc√™ deseja mais de um certo n√∫meros de sucessos (q exclusivo).# Informa√ß√£o: 5 pessoas ou mais, 30 vendas, prob 1/15pbinom(q=4, size= 30, prob= 1/15, lower.tail= F)[1] 0.0464 # 4.6% de chance.Nota: eu sei que voc√™ provavelmente est√° pensando agora ‚ÄúMas a escolha de um produto por um cliente √© muito mais complexa do que um simples teste estat√≠stico‚Äù. E realmente √©. Envolve pre√ßo, promo√ß√£o, valor, ponto de vendas e muitas outras coisas. Mas a ideia neste artigo √© apenas mostrar como realizar os testes e t√™-los como uma nova ferramenta para sua an√°lise.Distribui√ß√£o de PoissonA Distribui√ß√£o de Poisson (descoberta por Sim√©on Denis Poisson) est√° relacionada a eventos em um per√≠odo de tempo.Voc√™ usa a distribui√ß√£o de Poisson quando deseja saber qual √© a chance de algo acontecer ‚Äôn‚Äô vezes durante um per√≠odo de tempo.Para usar esse teste, voc√™ pode digitar dpois no R Studio. No entanto, voc√™ precisar√° ter as seguintes informa√ß√µes para continuar:dpois(x= n√∫mero a ser testado,      lambda = taxa m√©dia em que o evento ocorre)Mais uma vez, trazendo para o nosso doce exemplo do sorvete, no conjunto de dados apresentado no in√≠cio deste artigo, vemos as colunas m√™s, vendas e clientes. Portanto, sabemos que nosso per√≠odo de tempo √© de um m√™s. E se chamarmos a fun√ß√£o summary em nossa coluna de vendas, teremos a taxa m√©dia de vendas por um m√™s, correto?summary(ice_cream$sales)  Min.   1st Qu.  Median  Mean    3rd Qu.    Max.   103.0   123.5   189.5   245.2   374.2     477.0Nosso lambda √©, portanto, 245.2 vendas por m√™s. Agora s√≥ precisamos saber o que queremos testar.Quero aumentar em 5% minha m√©dia de vendas. Qual a probabilidade de isso acontecer, apenas ao acaso?Problema 2: Aumente a m√©dia de vendas / m√™s em 5%, para aprox. 257M√©todo: teste de Poisson = 12 vendas a mais por m√™sM√©dia atual: 245.2 (lambda)dpois(x= 257, lambda)[1] 0.0188  # 1.8% de chance√â.. melhor eu come√ßar a trabalhar mais em a√ß√µes de marketing, certo? Porque se eu deixar tudo ao acaso, terei que contar com min√∫sculos 1,8% de probabilidade de que os clientes comecem a aparecer na minha loja e comprar mais.Da mesma forma que os outros testes de distribui√ß√£o, o Poisson tamb√©m traz o ppois que calcula a probabilidade acumulada. A diferen√ßa √© apenas o nome da fun√ß√£o come√ßando com a letra p a inclus√£o do par√¢metro lower.tail.Agora vou calcular a chance acumulada de aumentar minhas vendas em qualquer n√∫mero entre 1% e 5%.# Calculando a prob acumulada de 5% de aumento ou menos e subtraindo a prob. de 1% ou menos. Dessa forma, obtenho apenas o intervalo exato entre 1% e 5%, nada acima ou abaixo dele.ppois(257, lambda = 245.2) - ppois(247, lambda = 245.2)[1] 0.2226  # 22% de chance!Lembre-se de que essa √© a soma das chances. Portanto, aumentar 5% cont√©m a soma das chances de aumentar 1% + 2% + 3% + 4% + 5% ou quaisquer casas decimais entre eles. Dessa forma, voc√™ deve ter muito cuidado ao plotar e ler um gr√°fico como o mostrado abaixo. Veja que ele mostra 56% de chance de aumentar 1% de nossas vendas. T√° brincando?! O que isso significa?Onde a imagem mostra 247 (ou aumento de aproximadamente 1%), estamos na verdade calculando a probabilidade acumulada (somando probabilidades) das vendas irem da m√©dia de 245.2 para qualquer n√∫mero at√© 247 ‚Äî ou seja, variar entre 0 a 1%. Mesmo uma pequena altera√ß√£o como 245.2 para 245.21 √© considerada. Deste modo, olhando para a primeira barra, n√£o √© correto dizer que voc√™ ficar√° sentado o dia todo e √© 56% prov√°vel que suas vendas aumentem 1%.√â 56% prov√°vel que suas vendas aumentem para qualquer valor entre 245.2 e 247 se voc√™ continuar fazendo o que faz. Pode aumentar 0.1% ou 0.45% ou 0.87%‚Ä¶ Existe 58% de chance do n√∫mero se mover dentro da faixa de 245.2 a 248 e por a√≠ vai.Portanto, tenha cuidado ao interpretar este gr√°fico!Distribui√ß√£o acumulada Poisson para probabilidade de aumento de vendas de sorvetes ao acaso.Distribui√ß√£o normalFinalmente, a distribui√ß√£o normal √© o tipo mais comum que existe. Muitos conceitos e teorias estat√≠sticas baseiam-se nesta distribui√ß√£o.A distribui√ß√£o normal √© a famosa ‚Äòcurva em forma de sino‚Äô onde os dados s√£o distribu√≠dos em torno da m√©dia. Se voc√™ plotar os valores em um gr√°fico, a m√©dia ser√° o centro da curva.Conhecer as qualidades dessa curva nos permite fazer muitas suposi√ß√µes sobre os dados que s√£o normalmente distribu√≠dos e calcular as probabilidades para muitas coisas de nossa vida di√°ria. Extrair uma amostra de uma popula√ß√£o e usar as estat√≠sticas dessa amostra para entender o todo √© uma das vantagens surpreendentes da distribui√ß√£o normal.Acredito que a melhor analogia que conhe√ßo para isso √© com a comida. Quando voc√™ est√° explorando um novo alimento ou sabor, geralmente voc√™ n√£o vai l√° e d√° aquela baita mordida. Primeiro voc√™ pega um pedacinho e experimenta para saber o sabor. Isso porque voc√™ assume que o todo ter√° o mesmo sabor daquele pedacinho. O mesmo √© verdadeiro para distribui√ß√µes normais e voc√™ pode aprender mais sobre isso pesquisando sobre o Teorema do Limite Central.Al√©m disso, a √°rea sob a curva em forma de sino ter√° 100% dos valores. Como as Distribui√ß√µes Normais est√£o centradas em sua m√©dia, √© correto dizer que 50% ser√° maior e 50% ser√° menor que a m√©dia, assim como a maior parte dos valores est√° concentrada em torno da m√©dia. Se voc√™ calcular quanto os valores podem estar distantes do centro e depois dividir a curva em 6 partes iguais chamadas de desvio padr√£o ‚Äî 3 abaixo da m√©dia e 3 acima da m√©dia -, cada unidade de desvio padr√£o adicionada conter√° mais valores que explicam os seus dados.Agora fica mais f√°cil dividir a amostra em intervalos de probabilidade. Isso √© conhecido como a regra 68‚Äì95‚Äì99. Continue comigo: olhando para a curva normal abaixo, fica f√°cil ver que minha m√©dia √© o centro, os valores est√£o a 3 pontos do centro e sabemos que a √°rea sob a curva compreende 100% dos valores da minha amostra. Portanto, se eu pegar um desvio padr√£o para mais e um para menos do que a m√©dia, terei cerca de 68% dos valores de um determinado atributo. Se eu pegar dois desvios padr√£o, terei 95% dos valores. E tr√™s me d√° 99% dos valores. E isso explica o intervalo de confian√ßa que voc√™ deve ter ouvido muitas vezes, principalmente em per√≠odos eleitorais. Saiba mais neste √≥timo v√≠deo do Simple Learning Pro.O quanto voc√™ pode explicar usando 1, 2 ou 3 desvios padr√£o: regra 68‚Äì95‚Äì99.Seguindo adiante e trazendo o problema para o √∫ltimo exemplo de sorvete. Vamos pegar o m√™s de mar√ßo para nosso teste. Aqui est√° a distribui√ß√£o das 427 vendas daquele m√™s.Distribui√ß√£o normal das vendas. M√©dia = 14, desvio padr√£o = 1427 vendas em 30 dias nos d√° aproximadamente 14 vendas por dia. O desvio padr√£o √© 1 (por exemplo, poderia ter acontecido 13 ou 15 vendas em vez disso).Queremos saber a probabilidade de termos 16 vendas em um dia. Estou multiplicando a opera√ß√£o por 100, ent√£o j√° vemos a porcentagem final. Logo, vemos 5% de chance de ter 16 vendas em um dia. Isso cai para apenas 0,44% se testarmos 17 e 0,01% para o teste de 18 vendas.# 16 vendas em um dia dnorm(16, mean=14, sd=1)*100[1] 5.399097# 17 vendas em um diadnorm(17, mean=14, sd=1)*100[1] 0.4431848# 18 vendas em um diadnorm(18, mean=14, sd=1)*100[1] 0.01338302Conclus√£oOs testes estat√≠sticos s√£o muito √∫teis para neg√≥cios e ci√™ncia de dados se soubermos como aplic√°-los.Devemos ter cuidado ao mostrar os n√∫meros e as probabilidades aos tomadores de decis√£o, uma vez que podem ser facilmente mal interpretados. Certifique-se de sempre incluir explica√ß√µes detalhadas para cada probabilidade e gr√°ficos.√â f√°cil cometer um erro e colocar a culpa em estat√≠sticas ‚Äúruins‚Äù. Mas o problema n√£o est√° nos n√∫meros, o problema est√° nas pessoas que interpretam esses n√∫meros.if data:   data.science()GusData HackersBlog oficial da comunidade Data HackersFollow7 EstatisticaRCiencia De DadosData Science7¬†claps7¬†clapsWritten byGustavo SantosFollowFinancial Data Analyst | Data Scientist. I extract insights from data to help people and companies to make better and data driven decisions.FollowData HackersFollowBlog oficial da comunidade Data HackersFollowWritten byGustavo SantosFollowFinancial Data Analyst | Data Scientist. I extract insights from data to help people and companies to make better and data driven decisions.Data HackersFollowBlog oficial da comunidade Data HackersMore From MediumSete princ√≠pios para pipelines de dados confi√°veisRicardo Pinto in Data HackersTomada de decis√£o com Teste A/BGuilherme Reis Mendes in Data HackersData Visualization: Uma Arte ObjetivaLauro Oliveira in Data HackersComo categorizar textos usando o LDAGustavo Santos in Data HackersDados, Design e UX Research‚Ää‚Äî‚ÄäData Hackers Podcast 31Paulo Vasconcellos in Data HackersGradientes Descendentes na pr√°tica‚Ää‚Äî‚Äämelhor jeito de entenderArthur Lamblet Vaz in Data HackersA maneira eficiente de filtrar um data frame‚Ää‚Äî‚ÄäPandasArthur Lamblet Vaz in Data HackersEu li 3942 textos da New Order! Parte02‚Ää‚Äî‚ÄäTopic ModellingGiovani Ferreira in Data HackersLearn more.Medium is an open platform where 170 million readers come to find insightful and dynamic thinking."
Let it shiny: Um sorteador (√∫til de verdade),https://medium.com/rladiesbh/let-it-shiny-um-sorteador-%C3%BAtil-de-verdade-1bbb5e8b5b25?source=tag_archive---------1-----------------------,"R,Rladies,R Shiny","Dentre as muitas coisas que aprendi quando comecei a trabalhar com a Intelig√™ncia para produtos digitais √© focar na resolu√ß√£o de um problema real do cliente final. √â a dor a ser resolvida. Por mais √≥bvio que pare√ßa, o processo de descobrir essa demanda e de ater-se a ela com foco no seu processo de desenvolvimento √© super desafiador.Vi que em diversas comunidades, inclusive na minha favorita, costumam existir (com a ben√ß√£o de patrocinadores) sorteios. E que nesses momentos t√£o esperados, √© comum usar sorteadores que n√£o tem o melhor fit com o problema. Muitas vezes os sorteios ideais querem excluir pessoas j√° sorteadas para dar chance de mais participantes ganharem algo de um modo incremental mas com alguns sorteios especiais que todes podem participar.Adaptando a sabedoria popular: se a sua comunidade quiser um sorteio‚Ä¶ Fa√ßa um sorteador (em R, claro!)Se voc√™ quiser usar o sorteador que o Rladies Belo Horizonte vai adotar para os seus sorteios (dedos cruzados para que sejam muitos!) basta acessar: https://larissasayurifcs.shinyapps.io/rladiesBHsampler .A solu√ß√£o descrita nesse post √© a vers√£o basicona do sorteador:Input: uma lista com os nomes das pessoas a serem sorteadas e o tamanho da amostra.Output: a lista de pessoas sorteadas sendo o sorteio aleat√≥rio simples sem reposi√ß√£o (cada vez que aperta-se o bot√£o run um novo sorteio √© feito excluindo os indiv√≠duos amostrados anteiormente).Feito com o pacote shiny que permite intera√ß√£o do usu√°rio e pacote dplyr para manipula√ß√£o de dados.Sem os detalhes escritos em HTML.Com os c√≥digos descritos aqui fazemos o shiny app que permite um sorteio tipo esse:Suponha que voc√™ como organizador do sorteio tem uma lista de 300 participantes da comunidade, descritos em uma planilha (esses nomes eu tirei do pacote babynames):A ideia √© que voc√™ copie a lista de nomes e cole no shiny app do sorteador, caixa a esquerda da tela:Suponha que s√£o 05 premia√ß√µes diferentes e que vamos come√ßar com o sorteio de 05 camisetas do Rladies BH:Em seguida, sorteiam-se 04 canecas do Rladies BH (apenas para quem n√£o ganhou a camiseta):Depois, sorteiam-se 03 ingressos para trilhas do TDC online (apenas para quem n√£o ganhou a camiseta ou a caneca):Segue o sorteio de 02 ingressos para trilhas do She‚Äôs tech (apenas para quem n√£o foi sorteado at√© esse momento):E existe um sorteio ultra especial: Um ingresso com passagem e hospedagem para o pr√≥ximo rstudio::Conf. Nesse todos os participantes podem concorrer de modo que o sorteador √© reiniciado:Para permitir que o sorteador fosse usado pela maior quantidade de usu√°rios diferentes poss√≠vel tentei conceber a experi√™ncia de uso mais simples.Como eu queria uma tela com as funcionalidades do R rodando ‚Äúpor tr√°s‚Äù eu optei por usar o pacote shiny. Para os que n√£o s√£o familiariados com ele eu sugiro fortemente assistir os v√≠deos em: https://shiny.rstudio.com/tutorial/.Comecei criando tr√™s scripts b√°sicos de Shiny app (nomeados exatamente como ui.R, server.R e global.R e todos no mesmo diret√≥rio do projeto ):ui.R: destinado ao layout do app, para coletar dados que servem como input e como mostrar os objetos output para o usu√°rio come√ßando com:ui <- fluidPage(  )server.R: onde ficam os trechos de c√≥digo destinados a manipula√ß√£o de dados input para gerar os objetos output come√ßando com:server <- function(input, output){  }global.R: script auxiliar que para o contexto desse sorteador, vai ter uma lista inicial de indiv√≠duos a serem sorteados. Nas vezes que o usu√°rio colar uma lista pr√≥pria de indiv√≠duos a serem sorteados o objeto lista desse script n√£o ser√° usado. Mas ainda assim ele ser√° necess√°rio para carregar o pacote dplyr:library(dplyr)lista <- paste(c(""Layla Comparin"",                 ""Numi√° Gomes"",                 ""Ana Carolina Dias"",                 ""Larissa Sayuri Santos"",                 ""Numi√° Comparin"",                 ""Ana Carolina Comparin"",                 ""Larissa Comparin"",                 ""Layla Gomes"",                 ""Ana Carolina Gomes"",                 ""Larissa Sayuri Gomes"",                 ""Layla Dias"",                 ""Numi√° Dias"",                 ""Larissa Sayuri Dias"",                 ""Layla Santos"",                 ""Numi√° Santos"",                 ""Ana Carolina Santos""                 ), collapse = ""\n"")A interface com o usu√°rio (‚Äúas the top of the iceberg‚Äù)Come√ßando pela parte vis√≠vel do app:No script ui.R eu dividi a tela em duas partes de mesmo tamanho. A tela toda de um app shiny tem 12 como unidade de medida. Logo, √© como se eu tivesse criado duas colunas, cada uma com 06 unidades de medida. Acrescentei a fun√ß√£o wellPanel que cria um painel com borda e fundo cinza claro s√≥ para deixar mais bonitinho.ui <- fluidPage(  column(width = 6,         wellPanel(                    )  ),  column(width = 6,         wellPanel(                    )  ))A metade √† esquerda da tela foi pensada para que o usu√°rio cole a lista de indiv√≠duos para amostrar.A metade √† direita foi pensada para ter alguns bot√µes de input na parte superior e a tabela com a rela√ß√£o dos indiv√≠duos amostrados.No script ui.R vamos sub-dividir a tela da direita. Apesar de usar a fun√ß√£o column estou criando linhas:A 1¬™ e 3¬™ linhas ter√£o toda a extens√£o horizontal da coluna a direita (width = 12) para receber o bot√£o do tamanho amostral e a tabela com os indiv√≠duos amostrados. A 2¬™ linha ficar√° no meio da coluna direita, com dois bot√µes: (01) que usa a fun√ß√£o de amostragem sem reposi√ß√£o no indiv√≠duos dispon√≠veis para amostragem e (02) que reinicia o processo considerando todos os indiv√≠duos como dispon√≠veis para amostragem.ui <- fluidPage(  column(width = 6,         wellPanel(                    )        ),  column(width = 6,         wellPanel(           column(width = 12,                   # Sample size button           ),           column(width = 4, offset = 4,                   #  Run or Restart button           ),           column(width = 12,                  # Table output             )          )  ))Cola os nomes dos indiv√≠duos e ‚Äòprinta‚ÄôNo script ui.R vamos colocar na coluna da esquerda uma caixa para receber texto como input usando a fun√ß√£o textAreaInput.No script ui.R vamos colocar na coluna da direita uma tabela com o resultado das manipula√ß√µes e instru√ß√µes dadas no server.R usando a fun√ß√£o tableOutput.ui <- fluidPage(  column(width = 6,         wellPanel(           textAreaInput(inputId = ""list"",                          label = ""Enter your list"",                           height = ""80vh"",                          value = lista)         )        ),  column(width = 6,         wellPanel(           column(width = 12,                   # Sample size button           ),           column(width = 4, offset = 4,                   #  Run or Restart button           ),           column(width = 12,                  # Table output                    tableOutput(""sample"")           )          )  ))No script server.R vou criar um objeto chamado dataInput que corresponder√° √† lista de nomes de indiv√≠duos a serem amostrados associado a um id sequencial.dataInput <- eventReactive(input$list, {    input_names <- input$list    input_names_split <- strsplit(input_names, ""\n"")        if(length(input_names_split) > 0){      df <- as.data.frame(x = input_names_split)      names(df) <- ""name""      df <- df %>% mutate(id = seq(nrow(df))) %>%        select(id, name)    }  })Note que o objeto dataInput ser√° atualizado a toda altera√ß√£o feita em input$list. Em geral, shiny apps s√£o concebidos de modo que as funcionalidades descritas em server.R s√£o acionadas a toda e qualquer altera√ß√µes de inputs (dizemos que s√£o reativos, de reactive). A fun√ß√£o eventReactive ‚Äòisola‚Äô a obten√ß√£o do dataInput para que dependa apenas das altera√ß√µes feitas no input$list.Inclu√≠ tamb√©m uma condi√ß√£o l√≥gica de modo que manipula-se o dado para chegar no tibble/data.frame inicial apenas se o usu√°rio tiver colado uma lista de caracteres efetivamente (ou se estivermos usando o default do app, a lista de global.R).Tamb√©m no script server.R vou criar o objeto que o usu√°rio vai ver, o output$sample, obtido com a fun√ß√£o renderTable. Para come√ßar, ele vai retornar as primeiras linhas da tabela (fun√ß√£o head()) com os indiv√≠duos a serem amostrados.output$sample <- renderTable({    data <- dataInput()    if(nrow(data) > 0) {      return(head(data))      } else {      return(NULL)    }  })Em resumo, o script server.R fica assim, por enquanto:server <- function(input, output){    dataInput <- eventReactive(input$list, {    input_names <- input$list    input_names_split <- strsplit(input_names, ""\n"")        if(length(input_names_split) > 0){      df <- as.data.frame(x = input_names_split)      names(df) <- ""name""      df <- df %>% mutate(id = seq(nrow(df))) %>%        select(id, name)    }  })    output$sample <- renderTable({    data <- dataInput()    if(nrow(data) > 0) {      return(head(data))      } else {      return(NULL)    }  })  }Clicando em Runapp vemos o app desse jeito:Criando bot√µes (tamanho da amostra e Run) e amostrando efetivamenteNo script ui.R vamos criar os inputs sample_size e run. Os nomes s√£o auto-explicativos. S√≥ vou refor√ßar que o shiny app a princ√≠pio √© atualizado a cada altera√ß√£o de qualquer um dos par√¢metros input (input$list e input$sample_size). Como nos passos futuros a amostragem ser√° sem reposi√ß√£o eu optei por criar o bot√£o run que acionar√° o processo de amostragem. Usando o actionButton a ideia √© que o server.R s√≥ ser√° acionado com a interven√ß√£o expl√≠cita do usu√°rio.ui <- fluidPage(  column(width = 6,         wellPanel(           textAreaInput(inputId = ""list"",                          label = ""Enter your list"",                           height = ""80vh"",                          value = lista)         )        ),  column(width = 6,         wellPanel(           column(width = 12,                   # Sample size button                  numericInput(inputId = ""sample_size"",                                label = ""Sample size"",                                value = 3,                               min = 1,                               max = 1000000,                                width = ""33%"")           ),           column(width = 4, offset = 4,                   #  Run or Restart button                  actionButton(inputId = ""run"",                                label = ""Run!"",                                icon(""play-circle-o""))           ),           column(width = 12,                  # Table output                    tableOutput(""sample"")           )          )  ))Para fazer a amostragem propriamente dita vamos adicionar duas fun√ß√µes no script server.R: reactiveValues e observeEvent. Nesse post estamos construindo o shiny app juntes ent√£o pode parecer pouco intuitivo mas √© o uso das duas fun√ß√µes (com eventReactive e renderTable j√° apresentadas) que flexibilizam o c√≥digo para permitir a amostragem sem reposi√ß√£o.Em linhas gerais vamos:1. Criar um objeto no shiny para guardar os ids dos indiv√≠duos amostrados com a fun√ß√£o reactiveValues. 2. Quando apertarmos o bot√£o run uma nova amostra ser√° coletada (se poss√≠vel) e o objeto que guarda os ids dos indiv√≠duos amostrados ser√° ATUALIZADO com a cole√ß√£o mais recente de ids. Essa atualiza√ß√£o acontece com a fun√ß√£o observeEvent.No come√ßo do script server.R eu adicionei:all_ids_sampled <- reactiveValues(ids = c())√â como se eu estivesse criando um objeto global no shiny app chamado all_ids_sampled, com uma entrada chamada ids e inicializada com vazio.Abaixo do chunk para leitura de dados (objeto dataInput) adicionei o chunk do observeEvent:observeEvent(input$run, {    data <- dataInput()        ids_sampled <- sample(x = data$id,                          size = input$sample_size,                          replace = FALSE)        all_ids_sampled[[""ids""]] <- ids_sampled  })Note que:1. esse √© um chunk que ser√° executado se o bot√£o run for acionado. 2. observeEvent √© uma fun√ß√£o que n√£o produz output, ou seja, n√£o pode ser usada para atribuir uma quantidade a uma vari√°vel no shiny app. 3. Vamos usar o observeEvent para atualizar a lista com os ids amostrados, a √∫ltima linha do chunk. Logo, no observeEvent estamos atualizando o objeto global do shiny app criado com a fun√ß√£o reactiveValues no passo anterior.Umas dicas do fundo do cora√ß√£o de quem apanhou para entender:Eu associo o observeEvent ao escopo local de uma fun√ß√£o enquanto o eventReactive corresponderia a uma fun√ß√£o no escopo global do shiny app.Quando √© √∫til usar o observeEvent? Quando a atividade a ser feita √© de atualiza√ß√£o. Um uso comum dessa fun√ß√£o pode ser no script ui.R quando um input est√° condicionado ao outro. Por exemplo: se o usu√°rio precisa entrar com o estado e cidade de resid√™ncia o app pode pedir primeiro o estado e dada a resposta do usu√°rio apresentar apenas os munic√≠pios pertencentes ao estado reportado.Ao atualizar o output final incluindo a rela√ß√£o de ids amostrados no chunk de outputTable o script server.R fica:server <- function(input, output){    all_ids_sampled <- reactiveValues(ids = c())      dataInput <- eventReactive(input$list, {    input_names <- input$list    input_names_split <- strsplit(input_names, ""\n"")        if(length(input_names_split) > 0){      df <- as.data.frame(x = input_names_split)      names(df) <- ""name""      df <- df %>% mutate(id = seq(nrow(df))) %>%        select(id, name)    }  })      observeEvent(input$run, {    data <- dataInput()        ids_sampled <- sample(x = data$id,                          size = input$sample_size,                          replace = FALSE)        all_ids_sampled[[""ids""]] <- ids_sampled  })      output$sample <- renderTable({    data <- dataInput()    sampled_ids <- all_ids_sampled[[""ids""]]        if(length(sampled_ids) > 0) {      names_sampled <- data[sampled_ids, ]      return(names_sampled)      } else {      return(NULL)    }  })  }Execute o app nessa etapa! O meu t√° do jeitinho abaixo, com um sorteio de tamanho 3.Alterando o c√≥digo para uma amostragem sem reposi√ß√£oAt√© esse momento a amostragem toma os nomes dos invid√≠duos e o tamanho de amostra como input. Queremos que as pessoas sorteadas para ganhar camisetas, por exemplo, saiam da lista dos indiv√≠duos dispon√≠veis para fazer um novo sorteio de canecas. Basicamente nesse passo vamos trabalhar para incrementar o objeto all_ids_sampled. Essa √© uma mudan√ßa no script server.R mais especificamente no chunk observeEvent, aquele que atualiza o objeto all_ids_sampled.observeEvent(input$run, {    already_sampled <- all_ids_sampled[[""ids""]]        if(is.null(already_sampled)){      available <- dataInput()    }else{      available <- dataInput() %>%        filter(!(id %in% already_sampled))    }        ids_sampled <- sample(x = available$id,                          size = input$sample_size,                          replace = FALSE)        all_ids_sampled[[""ids""]] <- c(all_ids_sampled[[""ids""]], ids_sampled)      })Note que:1. No escopo local de observeEvent cria-se um objeto already_sampled que vai receber a lista de ids que j√° foram amostrados e, portanto, est√£o no objeto all_ids_sampled, output da fun√ß√£o reactiveValues.2. Criamos um objeto com os indiv√≠duos dispon√≠veis para serem amostrados, chamado de available, nas linhas do if/else. No contexto desse sorteador, already_sampled e available s√£o conjuntos complementares.3. A amostragem √© feita com a fun√ß√£o sample para o conjunto de ids do objeto available, com tamanho especificado pelo usu√°rio e sem reposi√ß√£o.4. Ao t√©rmino do amostragem o objeto all_ids_sampled √© atualizado de modo a receber o conjunto de ids que acabou de ser sorteado (ids_sampled).E fica assim:Note como a lista de pessoas sorteadas vai aumentando. Mas temos um problema! Olha esse sorteio simplificado (10 nomes e tamanho 5) abaixo:Com 10 pessoas √© poss√≠vel fazer dois sorteios de 05 pessoas sem reposi√ß√£o. Depois dos dois primeiros sorteios‚Ä¶ Na aus√™ncia de pessoas dispon√≠veis para mais um sorteio o shiny app ‚Äúquebra‚Äù aparecendo sombreado para o usu√°rio. Vamos criar caixas de di√°logo para o usu√°rio de modo a orient√°-lo melhor.Comunicando que o tamanho de amostra √© maior que o n√∫mero de pessoas dispon√≠veis e/ou que todos foram sorteadosNo script server.R, chunk observeEvent, vamos adicionar testes l√≥gicos com rela√ß√£o a quantidade de ids dispon√≠veis para sorteio (objeto available):Se N√ÉO h√° elementos dispon√≠veis para amostragem (length(available$id) == 0)Vamos usar as fun√ß√µes showModal e modalDialog para comunicar ao usu√°rio que todas as pessoas cujos nomes foram colados na tela da esquerda j√° foram sorteadas (est√£o na tela da direita).showModal(modalDialog(        title = ""Stop!"",        ""There are no available elements to be sampled!"",         footer = modalButton(label = ""Ok! I'm done!""),        easyClose = TRUE      ))2. Caso contr√°rio (length(available$id) != 0):2.1) Se o tamanho da amostra √© MAIOR ou IGUAL que o n√∫mero de pessoas dispon√≠veis para sorteio (tem mais brinde que gente que ganhou nada)Todas as pessoas dispon√≠veis para sorteios ser√£o obrigatoriamente sorteadas.Na verdade, vai sobrar brinde! O que possivelmente implica o recome√ßo dos sorteios, com todo mundo apto a ser amostrado.Parachamar a aten√ß√£o do organizador para isso vamos usar as fun√ß√µes showModal e modalDialog para comunicar o ocorrido.if(input$sample_size >= length(available$id)) {                ids_sampled <- available$id        all_ids_sampled[[""ids""]] <- c(all_ids_sampled[[""ids""]], ids_sampled)                showModal(modalDialog(          title = ""Attention!"",          ""There were fewer elements than the desired sample size. You have sampled everyone."",           footer = modalButton(label = ""Ok""),          easyClose = TRUE        ))              }2.2) Caso contr√°rio (tem mais gente p√© frio do que brindes ‚Äî vida real)Nesse caso, o tamanho da amostra √© MENOR que o n√∫mero de pessoas dispon√≠veis para sorteio. Ou seja, √© o caso mais comum dos sorteios‚Ä¶ sendo que o trecho de c√≥digo n√£o vai ter mensagem para o organizador:ids_sampled <- sample(x = available$id,                              size = input$sample_size,                              replace = FALSE)                all_ids_sampled[[""ids""]] <- c(all_ids_sampled[[""ids""]], ids_sampled)A essa altura o script server.R est√° beeem mais complexo:server <- function(input, output){    all_ids_sampled <- reactiveValues(ids = c())      dataInput <- eventReactive(input$list, {    input_names <- input$list    input_names_split <- strsplit(input_names, ""\n"")        if(length(input_names_split) > 0){      df <- as.data.frame(x = input_names_split)      names(df) <- ""name""      df <- df %>% mutate(id = seq(nrow(df))) %>%        select(id, name)    }  })      observeEvent(input$run, {    already_sampled <- all_ids_sampled[[""ids""]]        if(is.null(already_sampled)){      available <- dataInput()    }else{      available <- dataInput() %>%        filter(!(id %in% already_sampled))    }            if(length(available$id) == 0){            showModal(modalDialog(        title = ""Stop!"",        ""There are no available elements to be sampled!"",         footer = modalButton(label = ""Ok! I'm done!""),        easyClose = TRUE      ))          }else{      if(input$sample_size >= length(available$id)) {                ids_sampled <- available$id        all_ids_sampled[[""ids""]] <- c(all_ids_sampled[[""ids""]], ids_sampled)                showModal(modalDialog(          title = ""Attention!"",          ""There were fewer elements than the desired sample size. You have sampled everyone."",           footer = modalButton(label = ""Ok""),          easyClose = TRUE        ))              } else {        ids_sampled <- sample(x = available$id,                              size = input$sample_size,                              replace = FALSE)                all_ids_sampled[[""ids""]] <- c(all_ids_sampled[[""ids""]], ids_sampled)      }    }      })      output$sample <- renderTable({    data <- dataInput()    sampled_ids <- all_ids_sampled[[""ids""]]        if(length(sampled_ids) > 0) {      names_sampled <- data[sampled_ids, ]      return(names_sampled)      } else {      return(NULL)    }  })  }Olha como essa nova implementa√ß√£o ficou, considerando que s√£o 11 pessoas e amostra de tamanho 5 nos gifs abaixo. Fazendo dois sorteios aleat√≥rios, 10 das 11 pessoas iniciais foram sorteadas. Com aten√ß√£o voc√™ notar√° que s√≥ falta a pessoa 8 na lista:Seguindo com o sorteio (o gif compreende todas as etapas) note como no terceiro ‚ÄúRun!‚Äù aparece a mensagem (tela sombreada) que deixa claro para o usu√°rio que a amostra basicamente retornou todo mundo que n√£o tinha sido sorteado. No √∫ltimo frame podemos ver que a pessoa 08 aparece na lista de indiv√≠duos sorteados (√∫ltima linha).Se a gente continuasse o sorteio acima clicando em Run mais uma vez apareceria, por fim a seguinte tela:Adicionando um bot√£o para recome√ßar os sorteios√â poss√≠vel que o organizador tenha interesse em recome√ßar o sorteio. Vamos adicionar o bot√£o restart, abaixo do bot√£o run no script ui.R:column(width = 4, offset = 4,                   #  Run or Restart button                  actionButton(inputId = ""run"",                                label = ""Run!"",                                icon(""play-circle-o"")),                  actionButton(inputId = ""restart"",                                label = ""Restart!"",                                icon(""play-circle-o""))           )E, claro, atualizar o server.R adicionando um trecho de c√≥digo com a fun√ß√£o observeEvent de modo que o objeto all_ids_sampled √© apagado/ reinicializado se o usu√°rio aperta no bot√£o restart (input$restart):  observeEvent(input$restart, {    all_ids_sampled[[""ids""]] <- c()  })A essa altura o script ui.R est√° assim:ui <- fluidPage(  column(width = 6,         wellPanel(           textAreaInput(inputId = ""list"",                          label = ""Enter your list"",                           height = ""80vh"",                          value = lista)         )        ),  column(width = 6,         wellPanel(           column(width = 12,                   # Sample size button                  numericInput(inputId = ""sample_size"",                                label = ""Sample size"",                                value = 5,                               min = 1,                               max = 1000000,                                width = ""33%"")           ),           column(width = 4, offset = 4,                   #  Run or Restart button                  actionButton(inputId = ""run"",                                label = ""Run!"",                                icon(""play-circle-o"")),                  actionButton(inputId = ""restart"",                                label = ""Restart!"",                                icon(""play-circle-o""))           ),           column(width = 12,                  # Table output                    tableOutput(""sample"")           )          )  ))E o script server.R (grand√£o):server <- function(input, output){    all_ids_sampled <- reactiveValues(ids = c())      dataInput <- eventReactive(input$list, {    input_names <- input$list    input_names_split <- strsplit(input_names, ""\n"")        if(length(input_names_split) > 0){      df <- as.data.frame(x = input_names_split)      names(df) <- ""name""      df <- df %>% mutate(id = seq(nrow(df))) %>%        select(id, name)    }  })      observeEvent(input$run, {    already_sampled <- all_ids_sampled[[""ids""]]        if(is.null(already_sampled)){      available <- dataInput()    }else{      available <- dataInput() %>%        filter(!(id %in% already_sampled))    }            if(length(available$id) == 0){            showModal(modalDialog(        title = ""Stop!"",        ""There are no available elements to be sampled!"",         footer = modalButton(label = ""Ok! I'm done!""),        easyClose = TRUE      ))          }else{      if(input$sample_size >= length(available$id)) {                ids_sampled <- available$id        all_ids_sampled[[""ids""]] <- c(all_ids_sampled[[""ids""]], ids_sampled)                showModal(modalDialog(          title = ""Attention!"",          ""There were fewer elements than the desired sample size. You have sampled everyone."",           footer = modalButton(label = ""Ok""),          easyClose = TRUE        ))              } else {        ids_sampled <- sample(x = available$id,                              size = input$sample_size,                              replace = FALSE)                all_ids_sampled[[""ids""]] <- c(all_ids_sampled[[""ids""]], ids_sampled)      }    }      })      observeEvent(input$restart, {    all_ids_sampled[[""ids""]] <- c()  })      output$sample <- renderTable({    data <- dataInput()    sampled_ids <- all_ids_sampled[[""ids""]]        if(length(sampled_ids) > 0) {      names_sampled <- data[sampled_ids, ]      return(names_sampled)      } else {      return(NULL)    }  })  }Tadam! Chegamos ent√£o em um shiny app que faz a seguinte sequ√™ncia de sorteios:Como eu disse l√° no come√ßo essa √© a vers√£o basicona, sem os c√≥digos HTML que deixam a interface linde. Os scripts ui.R, server.R, global. R e a base babynames.csv do app descrito nesse post est√£o nesse reposit√≥rio.Perfeito √© imposs√≠velMesmo a vers√£o com ajustes em HTML pode e deve passar por melhorias tipo:permitir que o usu√°rio do shiny app nomeie as etapas do sorteio deixando claro, por exemplo, que as primeiras cinco linhas da tabela descrevem os ganhadores das camisetas no exemplo do come√ßo do post.permitir que o usu√°rio do shiny app (o organizador do sorteio) fa√ßa download de uma planilha final com o nome das pessoas sorteadas, seguida da express√£o que identifica o seu pr√™mio.ajustar a fun√ß√£o sampler para que amostre o m√≠nimo de 1 elemento e o m√°ximo de elementos como sendo o n√∫mero de nomes distintos colados na tela da esquerda.permitir que o usu√°rio do shiny app (o organizador do sorteio) especifique a semente para o sorteio aleat√≥rio.E voc√™? Sugere alguma funcionalidade para tornar esse shiny app ainda mais √∫til? Tem alguma coisa do app https://larissasayurifcs.shinyapps.io/rladiesBHsampler que n√£o te parece intuitivo ou bem explicado? Sugest√µes e coment√°rios s√£o muito bem-vindos!!!Esse foi um dos projetos pessoais mais desafiadores que eu j√° peguei. Eu ainda sou imatura em programar um shiny app por exigir a compreens√£o clara dos 03 scripts ao mesmo tempo. Mas certamente foi o projeto que eu mais aprendi recentemente. Sobretudo na tarefa de escrever esse post, porque me fazer colocar defini√ß√µes e racioc√≠nios nas minhas pr√≥prias palavras costuma ser a melhor forma de me fazer aprender.Refer√™nciasPara esse post foram usados os seguintes pacotes:babynames: Hadley Wickham (2019). babynames: US Baby Names 1880‚Äì2017. R package version 1.0.0. https://CRAN.R-project.org/package=babynamesdplyr: Hadley Wickham, Romain Fran√ßois, Lionel Henry and Kirill M√ºller (2020). dplyr: A Grammar of Data Manipulation. R package version 1.0.0. https://CRAN.R-project.org/package=dplyrshiny: Winston Chang, Joe Cheng, JJ Allaire, Yihui Xie and Jonathan McPherson (2020). shiny: Web Application Framework for R. R package version 1.5.0. https://CRAN.R-project.org/package=shinyE como n√£o poderia faltar:O shiny app est√° dispon√≠vel na plataforma self-service https://www.shinyapps.io/, espec√≠fica para aplica√ß√µes do shiny.rladiesbhUma comunidade de mulheres interessadas em R e Data Science.¬†:)Follow6 Sign up for We R Ladies - BlogBy rladiesbhUma comunidade de mulheres interessadas em R e Data Science.¬†:)¬†Take a lookGet this newsletterBy signing up, you will create a Medium account if you don‚Äôt already have one. Review our Privacy Policy for more information about our privacy practices.Check your inboxMedium sent you an email at  to complete your subscription.RRladiesR Shiny6¬†claps6¬†clapsWritten byLarissa Sayuri Futino Castro Dos SantosFollowStatistician, Data Scientist, stationery lover, cooker wannabe.FollowrladiesbhFollowUma comunidade de mulheres interessadas em R e Data Science.¬†:)FollowWritten byLarissa Sayuri Futino Castro Dos SantosFollowStatistician, Data Scientist, stationery lover, cooker wannabe.rladiesbhFollowUma comunidade de mulheres interessadas em R e Data Science.¬†:)More From MediumInterpreta√ß√£o de Modelos de Machine Learning no R‚Ää‚Äî‚ÄäParte 2J√©ssica Ramos in rladiesbhViol√™ncia contra a Mulher: dados do Governo Federal, formattable e leafletLarissa Sayuri Futino Castro Dos Santos in rladiesbhInterpreta√ß√£o de Modelos de Machine Learning no RJ√©ssica Ramos in rladiesbhO que √© Sports Analytics e por que voc√™ pode se interessar pela √°reaMariana Pasqualini in rladiesbhComo contribuir no blog do R-Ladies Beag√°?RLadies Belo Horizonte in rladiesbhMachine Learning‚Ää‚Äî‚ÄäBibliotecas √∫teis para an√°lise no R.Gisele G Brito in rladiesbhComo fazer um mapa animado com dados do IBGE ‚ù§Mar√≠lia Melo Favalesso in rladiesbhRMarkdown: o m√≠nimo que voc√™ precisa saberLarissa Sayuri Futino Castro Dos Santos in rladiesbhLearn more.Medium is an open platform where 170 million readers come to find insightful and dynamic thinking."
Transformando colunas em linhas e outras loucuras com tidyr,https://medium.com/psicodata/transformando-colunas-em-linhas-com-tidyr-a649f287a238?source=tag_archive---------2-----------------------,"Ci,R,Psicodata,Tutorial,Psicometria","Cada c√©lula √© um valor √∫nico.Cada coluna √© uma vari√°vel.Cada linha √© uma observa√ß√£o.O pacote tidyr foi criado com esse prop√≥sito.Quando temos um banco de dados, uma das primeiras preocupa√ß√µes deve estar relacionada √† qualidade desses dados. Eles precisam estar organizados e limpos para que as an√°lises sejam feitas.J√° falamos aqui sobre a manipula√ß√£o de um banco de dados em Python e em R (com o pacote dplyr). Al√©m de manipular dados, √© importante verificar se eles est√£o organizados. A equipe do tidyverse.org preparou o checklist acima para entender se um banco de dados est√° organizado.Esse √© um tutorial para te ensinar a fazer as seguintes transforma√ß√µes:1. Separar uma coluna em duas ou mais colunas2. Transformar colunas em linhas3. Transformar linhas em colunas4. Unir colunasA gente vai usar esse banco de dados aqui. Esse banco cont√©m dados fict√≠cios do n√∫mero de telefonemas e cartas que passaram por essas cidades pequenas.Banco de dados a ser utilizado.Atualmente o banco tem 20 linhas e 10 colunas.Ao final, ele vai ter 70 linhas e 6 colunas.A coluna id representa o c√≥digo de identifica√ß√£o da cidade. A coluna cidade cont√©m o nome das cidades. A informa√ß√£o sobre as cartas s√£o sempre pegas no mesmo dia do ano, j√° as colunas hora_coleta e minutos_coleta representa o hor√°rio em que as informa√ß√µes sobre a cidade foram coletadas naquele ano ‚Äî sempre entre 11h da manh√£ e 13h da tarde.Os anos est√£o separados por colunas, sendo que as √∫ltimas colunas, infelizmente, tiveram seu valor colocado em uma mesma coluna‚Ä¶ Uma pena, embora seja conveniente para o que queremos aprender.Transforma√ß√£o inicialAntes de come√ßar, vamos apenas carregar o banco de dados e retirar a primeira coluna que carrega com ele.data_url <- ""https://raw.githubusercontent.com/GabrielReisR/R/master/estrutura%20de%20dados/dados/untidy.csv""untidy <- read.csv(data_url)untidy <- untidy[,-c(1)] # retirando primeira colunaVamos come√ßar?Separar uma coluna em duas ou mais colunasVamos lembrar da primeira regra: cada c√©lula deve ser um valor √∫nico.Para ter a certeza de que nosso banco cumpra isso, utilizaremos a fun√ß√£o separate() do tidyr.O primeiro grande erro do banco, como j√° citado, est√° na computa√ß√£o da coluna ano_1994_1995_1996.Vamos usar a fun√ß√£o separate() para separar os valores da coluna ano_1994_1995_1996.separate() √© bem simples, ent√£o vamos exemplificar fazendoBasta adicionar o nome da coluna atual (""ano_1994_1995_1996""), a separa√ß√£o que existe entre as vari√°veis daquela coluna (nesse caso, um espa√ßo sep = "" ""), e quais as novas colunas que devem ser criadas (into = c(""ano_1994"", ""ano_1995"", ""ano_1996"")). Pronto!O que antes estava contido em uma s√≥ coluna agora est√° separado por coluna. V√°rios valores que estavam em uma s√≥ c√©lula agora est√£o em outras.Transformar colunas em linhasAgora a gente vai entrar nas fun√ß√µes pivot. Vamos lembrar: Cada coluna deve ser uma vari√°vel.Para ter a certeza de que nosso banco cumpra isso, utilizaremos pivot_longer(). Nos livraremos de colunas repetitivas e alongaremos o banco de dados ao acrescentar mais linhas nele.A parte mais f√°cil passou. Era √≥bvio que os dados dos anos precisavam ser separados.Entretanto, nem tudo √© √≥bvio quando se trata de dados organizados. √Äs vezes pode ser dif√≠cil enxergar que precisamos deixar apenas uma coluna para cada vari√°vel.Todos os anos de coleta dizem respeito a apenas uma vari√°vel, a vari√°vel ano. Como sabemos disso? Pois isso √© algo que varia em cada observa√ß√£o. Ao inv√©s de termos uma coluna para cada um dos anos, o ideal seria criar uma coluna ano para armazenar todos esses anos.Quais colunas entrar√£o na nova coluna ano?A nossa nova coluna ano armazenar√° todos os nomes das colunas que representam o ano. J√° os valores dessas colunas ser√£o armazenados em uma nova coluna chamada de casos.Como escrever isso? Usaremos a fun√ß√£o pivot_longer().Transformar linhas em colunasOlhando o banco acima podemos perceber que existem duas observa√ß√µes para cada um dos 6 anos coletados. Isso porque h√° uma divis√£o entre o tipo ‚Äúcartas‚Äù e ‚Äútelefonemas‚Äù.Isso quebra nossa terceira regra. Vamos lembr√°-la: cada linha √© uma observa√ß√£o s√≥.Para ter a certeza de que nosso banco cumpra isso, utilizaremos pivot_wider(). Nos livraremos de linhas repetitivas e alargaremos o banco de dados ao acrescentar mais colunas nele.Agora que temos o nosso banco quase pronto, vamos refletir sobre duas colunas importantes: tipo e casos.tipo informa qual meio de comunica√ß√£o essa cidade usou.casos informa quantas vezes essa cidade usou esse meio.O problema aqui √© que temos duas linhas para uma observa√ß√£o s√≥! Em 1990, Butte, √†s 11h27, desde o ano anterior, havia enviado 4275 cartas e feito 3450 telefonemas.A quest√£o √© que ‚Äúcartas‚Äù e ‚Äútelefonemas‚Äù s√£o coisas diferentes, elas n√£o s√£o vari√°veis contidas em ‚Äútipo‚Äù (assim como os anos est√£o contidos ano, telefonemas e cartas n√£o est√£o contidas em tipo). Estamos alongando o banco sem necessidade, j√° que estamos falando de coisas diferentes e poder√≠amos ter duas colunas para cada uma dessas vari√°veis.Torna-se interessante alargar esses dados (essa √© uma escolha, n√£o uma regra) ‚Äî ou seja, criar colunas para cartas e telefonemas. Para isso, usamos pivot_wider().Agora sim!Perceba que agora temos apenas uma linha para cada ano, ao inv√©s de 2 linhas para cada ano.Da mesma forma, ‚Äòcasos‚Äô n√£o √© mais necess√°rio, e nosso banco se tornou mais f√°cil de ser lido.Unir colunasUma coluna para cada vari√°vel e uma c√©lula para um caso √∫nico. Aqui, a gente vai se preocupar com duas colunas, utilizando unite() para resolver um probleminha que temos.Algo que ainda n√£o conversamos sobre √© a redund√¢ncia das colunas hora_coleta e minutos_coleta. Essa pode ser apenas uma coluna, n√£o precisa ser duas. Vamos criar uma nova coluna que vai se chamar horario_coleta.Vamos juntar as duas com unite().Vamos entender:Iniciamos com a primeira coluna que queremos. O valor dessa coluna ir√° na frente (hora_coleta).A segunda coluna informa o valor que ir√° em segundo lugar (minutos_coleta).Definimos uma separa√ß√£o para nossas vari√°veis. Nesse caso, tratando-se de hor√°rios, preferi sep = "":"".√â poss√≠vel definir sem separa√ß√£o: nesse caso, sep = """".Em col = ""horario_coleta"", definimos o nome da nossa coluna como horario_coleta.Como prometido, passamos de 20 linhas para 70 linhas, de 10 colunas para 6 colunas.Espero que tenha ajudado, de alguma forma, a entender um pouco mais sobre dados organizados usando tidyr.Como procurar ajuda?O tutorial apresentado nesse post tamb√©m pode ser encontrado aqui.F√≥runs: https://pt.stackoverflow.com/Documenta√ß√£o: https://cran.r-project.org/web/packages/tidyr/tidyr.pdfP√°gina no tidyverse: https://tidyr.tidyverse.org/index.htmlCheatsheet (folha de c√≥digos): https://github.com/rstudio/cheatsheets/blob/master/data-import.pdfContatoQualquer d√∫vida, observa√ß√£o ou coment√°rio s√£o muito bem-vindos!Fique √† vontade para se manifestar e vamos aprender juntos :)Fale comigo pelo LinkedIn ou pelo email reisrgabriel@gmail.com üòÅPsicoDatapsicodataFollow2 Sign up for PsicoData NewsletterBy PsicoDataFique por dentro das nossas publica√ß√µes!¬†Take a lookGet this newsletterBy signing up, you will create a Medium account if you don‚Äôt already have one. Review our Privacy Policy for more information about our privacy practices.Check your inboxMedium sent you an email at  to complete your subscription.Ci√™ncia De DadosRPsicodataTutorialPsicometria2¬†claps2¬†clapsWritten byGabriel RodriguesFollowSou um psic√≥logo que adora Ci√™ncia de Dados ‚Äî busco criar e compartilhar conte√∫do sobre esses dois assuntos. linktr.ee/gabrielrrFollowPsicoDataFollowUma plataforma colaborativa em portugu√™s sobre ci√™ncia de dados e psicologia.FollowWritten byGabriel RodriguesFollowSou um psic√≥logo que adora Ci√™ncia de Dados ‚Äî busco criar e compartilhar conte√∫do sobre esses dois assuntos. linktr.ee/gabrielrrPsicoDataFollowUma plataforma colaborativa em portugu√™s sobre ci√™ncia de dados e psicologia.More From MediumUsando R para avaliar a normalidadeGabriel Rodrigues in PsicoDataT√° tudo junto: m√©dia, vari√¢ncia e desvio-padr√£oGabriel Rodrigues in PsicoDataDados e Psico: 5 li√ß√µes que eu gostaria de ter ouvido no in√≠cioPaula Costa in PsicoDataO que √© uma vari√°vel latente e como medi-la?‚Ää‚Äî‚ÄäParte 2Gabriel Rodrigues in PsicoDataSimples e Direto: um guia de visualiza√ß√£o de dados com PythonDalton Costa in PsicoDataBaixando e processando dados do DATASUS sobre suic√≠dio com PythonDalton Costa in PsicoDataO que √© uma vari√°vel latente e como medi-la?‚Ää‚Äî‚ÄäParte 1Gabriel Rodrigues in PsicoDataUm gr√°fico vale mais que mil palavras!Marcela Alves Sanseverino in PsicoDataLearn more.Medium is an open platform where 170 million readers come to find insightful and dynamic thinking."
Predicting Adoption Speed for PetFinder,https://towardsdatascience.com/predicting-adoption-speed-for-petfinder-bb4d5befb78c?source=tag_archive---------0-----------------------,"R,Pets,Data Modeling,Classification","Our team is currently acting as a consulting agency who works on behalf of PetFinder, a non-profit organization, contains a database of animals and aims to improve the animal welfare through collaborations with related parties. The core task of this project is to predict how long it will take for a pet to be adopted.Business UnderstandingCredit: Petfinder FoundationAccording to PetFinder‚Äôs 2018 Financial Report, approximately 70% of its total public support gained and revenues earned will be spent on its public welfare programs, including this animal adoption program. Our defined business problem is ‚ÄúWhen new pets come in, what would be the estimated time for new pets to be adopted.‚Äù Based on different traits each pet has, we could estimate how long it takes for the pet to be adopted according to the model we created. Therefore, by adopting our model, the company can guide shelters and rescuers who post information about animals through the channel on estimating the adoption speed for new animals. Once the pet adoption speed is predicted well, more efficient resource allocation can be implemented to improve the overall adoption performance, and subsequently, reduce the costs of sheltering and fostering. Eventually PetFinder will receive more financial support and make a greater contribution to the global animal benefit.Data PreparationThis dataset originally consists of 14,993 observations and 24 variables. All the data are based in the Malaysia area so our analysis is country-specific and culture-oriented. We removed the variable Name since we assume it is irrelevant to the pet‚Äôs adoption speed. Other than Name, we removed State, PetID, and RescuerID for the same reason. In addition, some columns such as Breed1 and Type are coded as numbers with an additional reference spreadsheet provided. For clarification reasons, we convert Type and Gender into strings: we coded ‚Äú1‚Äù as ‚ÄúDog‚Äù and ‚Äú2‚Äù as ‚ÄúCat‚Äù; For Gender, we coded ‚Äú1‚Äù as ‚ÄúMale‚Äù, ‚Äú2‚Äù as ‚ÄúFemale‚Äù, and ‚Äú3‚Äù as ‚ÄúMixed‚Äù. The mixed represents the gender of a profile of pets. We also changed categorical variables into dummy variables. Besides, we conduct a chi-squared test to test the correlation among Color1, Color 2, and Color3, the result shows that they are dependent on each other. Therefore, we plan to remain Color1 and remove the other two. Finally, considering only 0.38% of records have Breed2, we remove Breed2 for simplicity reasons.Besides, considering that the dataset contains a Description column, we applied text analytics to this column. Based on the analytics, we select the 10 most frequently occurring words and create an additional 10 columns based on these hot words. We check if each pet‚Äôs description contains these 10 words individually. If any word is matched in the description, we assign ‚Äú1‚Äù under that column. We assign ‚Äú0‚Äù if it does not. These 10 words are: love, kitten, puppi, rescu, healthi, activ, cute, train, mother, kind. Finally, we all agree that the length of a description may be an important factor as well. Since the more detailed the description is, the less time it takes adopters to pick the pet. Logically, adopters tend to make decisions quicker if the available information is richer. Therefore, our final dataset consists of 14,993 records and 29 variables.Data ExplorationText AnalysisBased on the text analytics, we first created a text cloud to capture the most frequent word in the Description column. It turns out that ‚Äúadopt‚Äù appears most frequently, followed by words ‚Äúkitten‚Äù, ‚Äúpuppi‚Äù and ‚Äúcute‚Äù. From the word cloud, we can summarize the major characteristics of those pets in our dataset.Next, we created bigrams to capture more patterns in the text. The most frequent bigram is ‚Äúmonth old‚Äù, and we pick 10 most intriguing words and plot them in a bar chart.As we can see from the two graphs, rescuers and shelters tend to use a lot of descriptive words in the description such as active, good, cute etc. As mentioned above, to test the effectiveness of these words, we picked 10 hot words and converted them into 10 categorical variables. The reason that we did not choose bigram was that bigrams were made of stemming words, so it may be hard for the software to detect these bigrams in description.Breed vs. AdoptionSpeedAdditionally, from Perro Market Website, one of the most well-known pet websites in Malaysia, we learned the 10 most popular cat and dog breeds in the country. We picked Maine and Persian for Cat, and Poodle for Dog for the purpose of data exploration.The mean of AdoptionSpeed for cats is 2.40 with a standard deviation of 1.21. By comparison, the average Adoption Speed for Maine is 1.97 and the average Adoption Speed for Persian is 1.95. Even though their mean is lower than the average, they are still in the first standard deviation range. Based on this fact, we could not conclude that popular cat breeds such as Maine and Persian tend to have shorter adoption period than other breeds.On the other hand, dogs have an average AdoptionSpeed of 2.62 with a standard deviation of 1.14. Noticeably, Poodle has an average adoption speed of 1.97. Again, even though the average adoption speed is smaller than the average of the whole group, it still falls within 1 standard deviation range, so we could not conclude that popular dog breeds lead to shorter adoption period.However, from the graph below, we find that the number of pets under popular breeds is much higher than the rest. Breed 179 is Poodle, and Breed 285 is Persian while Breed 276 is Maine.Based on this preliminary analysis, our conclusion is that popular breed determines the number of pets under this breed but do not necessarily mean lower adoption speed.Vaccinated vs. AdoptionSpeedVaccinated as ‚Äú1‚Äù means the pet is vaccinated, ‚Äú2‚Äù means the pet is not vaccinated, and ‚Äú3‚Äù means not sure. The pet who has been vaccinated counterintuitively shows higher median of time span than the one not got vaccinated, and for those reported unknown they have higher time span waiting to be adopted. There‚Äôs also no clear correlation (-0.06) between the AdoptionSpeed and Vaccinated across the species.ModelingSince the target variable AdoptionSpeed is a categorical variable with 4 levels, we decided the core task of this project to be classification: when a new pet comes in, the model would determine how long it will be adopted based on which level of adoption speed it is classified into. Because of its multi-class characteristic, we choose three models to achieve the goal, including Random Forest, SVM, and Multinomial logistic regression. All three models help solve our business performance by predicting the adoption speed for new animals based on their provided attributes. By sharing our predictions with the company, the company can have a better plan about the allocation of current resources, especially allocating more resources to the animals with longer predicted adoption period to accelerate their adoption speed.Random ForestRandom Forest consists of a large number of individual decision trees that operate as an ensemble. Each individual tree in the random forest spits out a class prediction and the class with the most votes becomes our model‚Äôs prediction. One pro with Random Forest is that its decorrelation when dealing with highly correlated variables. One con is its limitation on the number of categorical variables could be included in the predictors. In our project, we excluded the Description since there are more than 14000 levels in one variable and it has already segmented into words with high frequency and description length.Support Vector Machines (SVM)Another model we used is the Support Vector Machines (SVM) model, to create the optimal hyperplane to separate classes and classify new records in our test dataset. We include all the variables from the cleaned training dataset, except Description. SVM is easily adopted in solving multiclass classification problems by transforming complex data into high dimensional analysis and thus having classes easily separable. The model itself has good computational properties, but it does not attempt to model probability. Comparing with Random Forest, SVM can only be used to predict the class instead of estimating the probability of different classes. From this point of view, Random Forest may offer us a more comprehensive understanding of the prediction of classes of the test dataset.Multinomial Logistic Regression with Lasso and Post LassoWe also used multinomial logistic regression to predict our target variable. However, because of the limitations on the number of variables could be included in multinomial logistic regression, we used Lasso to select related variables for each class of adoption speed. We built 4 models of multinomial logistic regression with Lasso and post-Lasso based on minimum Œª and 1 standard error Œª. After selecting the variables, the models eliminated overfitting problems and may make better predictions. However, the variables selected by Lasso may differ in different bootstrapped models, and they are hard to be interpreted.EvaluationIn order to evaluate the performance of each model, we conducted a 3-fold cross-validation and selected the Confusion Matrix Balanced Accuracy as our Out of Sample accuracy measurement. Based on the chart above, all of our six models have a similar level of accuracy performance, ranging from 0.5 to 0.6. Random forest achieved the highest level of OOS accuracy of 59%. Multinomial Logistic Regression with Lasso using minimum Œª is the second highest with 56% OOS accuracy. Considering OOS accuracy as an important indicator for the classification models, we chose Random Forest as the optimal model to predict a newly coming pet‚Äôs adoption speed. To investigate specific classification accuracy, we further evaluated the confusion matrix of Random Forest and SVM, one has the highest and one has the lowest OOS accuracy.From the confusion matrix, we can conclude that Random Forest predicts more accurately when pets are actually adopted within 1 month (AdoptionSpeed = 2) and no adoption after 100 days being listed (AdoptionSpeed = 4). Random Forest does not show a tendency towards one specific class. In contrast, SVM is leaning toward classifying pets into AdoptionSpeed level of 2.One possible limitation with the model is that there are too few data points with 0 AdoptionSpeed and therefore it‚Äôs hard to evaluate its classification accuracy. On the other hand, with the above-mentioned limitation of Random Forest, if the PetFinder adds a new categorical variable that has more than 100 levels, Multinomial Logistic Regression with Lasso using minimum Œª, instead of Random Forest, would be a better choice to classify newly coming pet‚Äôs adoption speed.When we try to apply our model to real-life business, we should develop a business case to identify if the prediction is accurate and if the business can utilize the prediction to optimize its resource planning and thus improve the adoption performance. The business can compare its previous adoption performance with its future adoption performance by having resources allocated based on our prediction model. The improvement in adoption performance can be proved by the increase in the number of animals adopted within the same period of time. In addition, the business should analyze if the animals with long predicted adoption time can be adopted within a shorter period of time, to see if the change in resource allocation is efficient.DeploymentBy evaluating each model with its OOS accuracy, we reached a conclusion that Random Forest, which generated the highest OOS accuracy, would be the optimal choice for PetFinder. Whenever a new pet comes in, PetFinder is capable to predict how long it will take for a pet to be adopted by simply inputting its features. Estimating the adoption time more accurately will enable us to improve the animal‚Äôs profile, which is crucial to expedite the adoption speed, and subsequently save the costs of sheltering.In order to have more insights on what specific features have a significant impact on pets that take a longer time to be adopted, we examined the variables selected by Multinomial Logistic Regression with Lasso using minimum Œª. Since the variable selection differs in each fold of data, we took the intersection of the variable selections in 3 folds and found that across the four levels of adoption speed, Age, Breed, Amount of photos uploaded, and Amount of Video Uploaded are important. Specifically, for pets with slower adoption speed, people pay more attention to their health: whether they are vaccinated, dewormed, and sterilized. Also, the length of the description plays a more important role in the profile of pets with slower adoption speed. If pets are classified as possibly taking a longer time to be adopted, PetFinder should encourage the shelters to prioritize the health of the pets and increase the length of the description on profile.One important ethical consideration is that if the pets should be treated differently based on their predicted adoption speed. Without the prediction model, we assume that all pets are receiving an equal level of treatment. However, if we suggest the business to allocate more resources to animals with lower predicted adoption speed, we are resulting in an unequal distribution of resources. If our prediction is not accurate, some pets will be negatively influenced and more unlikely to be adopted. From this perspective, the business‚Äô goal of improving global animal benefits may not be fully achieved.One possible risk for our model is that we are currently having an inaccurate prediction for pets in the actual adoption speed class of 0. If those pets have predicted class of 3 or 4, the business will allocate more resources to promote them and thus results in a waste of limited resources. To alleviate the risk, we should always update and improve our model as we have more records, especially records with 0 actual adoption speed.Written byEileen Z.Follow35 1 Sign up for The Daily PickBy Towards Data ScienceHands-on real-world examples, research,  tutorials, and cutting-edge techniques delivered Monday to Thursday. Make learning your daily ritual.¬†Take a lookGet this newsletterBy signing up, you will create a Medium account if you don‚Äôt already have one. Review our Privacy Policy for more information about our privacy practices.Check your inboxMedium sent you an email at  to complete your subscription.35¬†35¬†1 RPetsData ModelingClassificationMore from Towards Data ScienceFollowA Medium publication sharing concepts, ideas, and codes.Read more from Towards Data ScienceMore From Medium5 YouTubers Data Scientists And ML Engineers Should Subscribe ToRichmond Alake in Towards Data Science7 Must-Haves in your Data Science CVElad Cohen in Towards Data Science30 Examples to Master PandasSoner Yƒ±ldƒ±rƒ±m in Towards Data Science21 amazing Youtube channels for you to learn AI, Machine Learning, and Data Science for freeJair Ribeiro in Towards Data ScienceThe Roadmap of Mathematics for Deep LearningTivadar Danka in Towards Data Science4 Types of Projects You Must Have in Your Data Science PortfolioSara A. Metwalli in Towards Data ScienceAn Ultimate Cheat Sheet for Data Visualization in PandasRashida Nasrin Sucky in Towards Data ScienceHow to Get Into Data Science Without a DegreeTerence S in Towards Data ScienceAboutHelpLegalGet the Medium app"
"<strong class=""cc"">KNN Algorithm in R</strong>",https://medium.com/nerd-for-tech/knn-algorithm-in-r-30dff0c8d094?source=tag_archive---------1-----------------------,"Machine Learning,Algorithms,Knn Algorithm,R,Supervised Learning","The huge amount of data that we‚Äôre generating every day, has led to an increase in the need for advanced Machine Learning Algorithms. One such well-performed algorithm is the K Nearest Neighbour algorithm.In this blog on KNN Algorithm In R, we will understand what is KNN algorithm in Machine Learning and its unique features including the pros and cons, how the KNN algorithm works, an essay example of it, and finally moving to its Implementation of KNN using the R Language.It is quite essential to know Machine Learning basics. Here‚Äôs a brief introductory section on what is Machine Learning and its types.Machine learning is a subset of Artificial Intelligence that provides machines the power to find out automatically and improve from their gained experience without being explicitly programmed.There are mainly three types of Machine Learning discussed briefly below:1. Supervised Learning: It is that part of Machine Learning in which the data provided for teaching or training the machine is well labeled and so it becomes easy to work with it.2. Unsupervised Learning: It is the training of information using a machine that is unlabelled and allowing the algorithm to act on that information without guidance.3. Reinforcement Learning: It is that part of Machine Learning where an agent is put in an environment and he learns to behave by performing certain actions and observing the various possible outcomes which it gets from those actions.Now, moving to our main blog topic what is KNN Algorithm?KNN which stands for K Nearest Neighbour is a Supervised Machine Learning algorithm that classifies a new data point into the target class, counting on the features of its neighboring data points.Let‚Äôs attempt to understand the KNN algorithm with an essay example. Let‚Äôs say we want a machine to distinguish between the sentiment of tweets posted by various users. To do this we must input a dataset of users‚Äô sentiment (comments). And now, we have to train our model to detect the sentiments based on certain features. For example, features such as labeled tweet sentiment i.e., as positive or negative tweets accordingly. If a tweet is positive, it is labeled as 1 and if negative, then labeled as 0.Features of KNN algorithm:KNN is a supervised learning algorithm, based on feature similarity.Unlike most algorithms, KNN is a non-parametric model which means it does not make any assumptions about the data set. This makes the algorithm simpler and effective since it can handle realistic data.KNN is considered to be a lazy algorithm, i.e., it suggests that it memorizes the training data set rather than learning a discriminative function from the training data.KNN is often used for solving both classification and regression problems.Disadvantages of the KNN algorithm:After multiple implementations, it has been observed that the KNN algorithm does not work with good accuracy on taking large datasets because the cost of calculating the distance between the new point and each existing point is huge, and in turn, it degrades the performance of the algorithm.It has also been noticed that working on high dimensional data is quite difficult with this algorithm because the calculation of the distance in each dimension is not correct.It is quite needful to perform feature scaling i.e., standardization and normalization before actually implementing the KNN algorithm to any dataset. Eliminating these steps may lead to wrong predictions by the KNN algorithm.Sensitive to noisy data, missing values and outliers: KNN is sensitive to noise in the dataset. We need to manually impute missing values and remove outliers.KNN Algorithm ExampleIn order to make understand how KNN algorithm works, let‚Äôs consider the following scenario:In the image, we have two classes of data, namely class A and Class B representing squares and triangles respectively. The problem statement is to assign the new input data point to one of the two classes by using the KNN algorithmThe first step in the KNN algorithm is to define the value of ‚ÄòK‚Äô which stands for the number of Nearest Neighbors.In this image, let‚Äôs consider ‚ÄòK‚Äô = 3 which means that the algorithm will consider the three neighbors that are the closest to the new data point. The closeness between the data points is calculated either by using measures such as Euclidean or Manhattan distance. Now, at ‚ÄòK‚Äô = 3, two squares and 1 triangle are seen to be the nearest neighbors. So, to classify the new data point based on ‚ÄòK‚Äô = 3, it would be assigned to Class A (squares).Ways to measure the new data point and nearest data points:Euclidean Distance: It always gives the shortest distance between the two points.Manhattan Distance: To measure the similarity, we simply calculate the difference for each feature and add them up.Practical Implementation of KNN Algorithm in RProblem Statement: To study a bank credit dataset and build a Machine Learning model that predicts whether an applicant‚Äôs loan can be approved or not based on his socio-economic profile.Step 1: Import the dataset and then look at the structure of the dataset:loan <- read.csv(‚ÄúC:/Users/zulaikha/Desktop/DATASETS/knn dataset/credit_data.csv‚Äù)str(loan)Step 2: Data CleaningFrom the structure of the dataset, we can see that there are 21 predictor variables but some of these variables are not essential in predicting the loan. So, it‚Äôs better to filter down the predictor variables by narrowing down 21 variables to 8 predictor variables.loan.subset <- loan[c(‚ÄòCreditability‚Äô, ‚ÄòAge..years.‚Äô, ‚ÄòSex‚Ä¶Marital.Status‚Äô, ‚ÄòOccupation‚Äô, ‚ÄòAccount Balance‚Äô, ‚ÄòCredit.Amount‚Äô, Length.of.current.employment‚Äô)]head(loan.subset)Step 3: Data NormalizationIt‚Äôs very essential to always normalize the data set so that the output remains unbiased. In the below code snippet, we‚Äôre storing the normalized data set in the ‚Äòloan.subset.n‚Äô variable and also we‚Äôre removing the ‚ÄòCredibility‚Äô variable since it‚Äôs the response variable that needs to be predicted.normalize <- function(x){return ((x ‚Äî min(x)) / (max(x) ‚Äî min(x)))}loan.subset.n <- as.data.frame(lapply(loan.subset[,2:8], normalize))Step 4: Data SplicingIt basically involves splitting the data set into training and testing data set. Then, it is needful to create a separate data frame for the ‚ÄòCreditability‚Äô variable so that our final outcome can be compared with the actual value.set.seed(123)dat.d <- sample(1:nrow(loan.subset.n),size=nrow(loan.subset.n)*0.7,replace = FALSE) #random selection of 70% data.train.loan <- loan.subset[dat.d,] # 70% training datatest.loan <- loan.subset[-dat.d,] # remaining 30% testing datatrain.loan_labels <- loan.subset[dat.d,1]test.loan_labels <-loan.subset[-dat.d,1]Step 5: Building a Machine Learning modelAt this stage, we have to build a model by using the training data set. Since we‚Äôre using the KNN algorithm to build the model, we must first install the ‚Äòclass‚Äô package provided by R. Next, we‚Äôre going to calculate the number of observations in the training data set.install.packages(‚Äòclass‚Äô)library(class)NROW(train.loan_labels)knn.26 <- knn(train=train.loan, test=test.loan, cl=train.loan_labels, k=26)knn.27 <- knn(train=train.loan, test=test.loan, cl=train.loan_labels, k=27)Step 6: Model EvaluationAfter building the model, it is time to calculate the accuracy of the created models:ACC.26 <- 100 * sum(test.loan_labels == knn.26)/NROW(test.loan_labels)ACC.27 <- 100 * sum(test.loan_labels == knn.27)/NROW(test.loan_labels)As shown above, the accuracy for K = 26 is 67.66 and for K = 27 it is 67.33. So, from the output, we can see that our model predicts the outcome with an accuracy of 67.67% which is good since we worked with a small data set.The summarizing words‚Ä¶KNN proves to be a useful algorithm at a lot many areas such as in banking sectors to predict whether a loan will be approved by a manager for an individual or not, in credit rate calculation of an individual by comparing the rate with a person bearing similar traits, and also in politics to classify a potential voter. Other areas in which the KNN algorithm can be used are Speech Recognition, Handwriting Detection, Image Recognition, and Video Recognition.Check out more articles:https://medium.com/nerd-for-tech/keras-608df20e88fdPlease share this Article with all your friends and hit that üëè button below to spread it around even more. Also, add any points or maybe your valuable suggestions that you want to convey below in the comments üí¨!I would love to hear from you. Stay in touch by following me‚Ä¶Check out more articles @ https://medium.com/@Eshita_NandyNerd For TechFrom Confusion to ClarificationFollow51 Machine LearningAlgorithmsKnn AlgorithmRSupervised Learning51¬†claps51¬†clapsWritten byEshita NandyFollowTechnical Content Writer || Data Science Enthusiast || Intern @ IIT BHU || B Tech ‚Äî IT ||FollowNerd For TechFollowWe are tech nerds because we believe in reinventing the world with the power of Technology. Our articles talk about some of the most disruptive ideas, technology, and innovation.FollowWritten byEshita NandyFollowTechnical Content Writer || Data Science Enthusiast || Intern @ IIT BHU || B Tech ‚Äî IT ||Nerd For TechFollowWe are tech nerds because we believe in reinventing the world with the power of Technology. Our articles talk about some of the most disruptive ideas, technology, and innovation.More From Medium(My) Machine Learning WorkflowArian Prabowo in Data Driven InvestorEXAM‚Ää‚Äî‚ÄäState-of-The-Art Method for Text ClassificationNeuroHiveLearning a XOR Function with Feedforward Neural NetworksTojo Batsuuri in The StartupAnti-Patterns in Machine LearningAndrew Lamb in firemind.K-Means Algorithm: Dealing with Unlabeled DataSrijarko Roy in SRM MICRecommendation System: An IntroductionRuben Winastwan in The StartupNearest Neighbors with Keras and CoreMLS√∏ren L KristiansenPre-trained language model in any languagerohan.arora in AI n ULearn more.Medium is an open platform where 170 million readers come to find insightful and dynamic thinking."
Integrate R & JIRA using JIRA REST API,https://medium.com/@gowthamrajaram/integrate-r-jira-using-jira-rest-api-a3e4617b60e8?source=tag_archive---------2-----------------------,"Jira,Rest Api,API,R,Programming","This article provides introduction on how to integrate R and JIRA using httr package (https://cran.r-project.org/web/packages/httr/httr.pdf)Photo by Chris Ried on UnsplashBefore we get started a small introduction on REST API .A RESTful API is an application program interface (API) that uses HTTP requests to GET, PUT, POST and DELETE data. An API for a website is code that allows two software programs to communicate with each other.JIRA does provide some documentation on how to access their API.But it not specific to R.JIRA 7.13.0docs.atlassian.comwithin your .renviron file create two new variablesJIRA_USER: email address of JIRA associated accountJIRA_APIKEY: can be generated within your profile->security->API tokenHaving this within environmental variables ensures that you have access to these while running your R sessionLets dive into a sample R snippet that returns list of projects available:library(httr)library(jsonlite)url<-‚Äúhttp://host:port/rest/api/2/project""params <- list(httr::accept_json())params<-c(params,httr::authenticate(Sys.getenv(‚ÄòJIRA_USER‚Äô),  Sys.getenv(‚ÄòJIRA_API_KEY‚Äô)))json <-httr::GET(url =url,config = params)project_dataframe<-fromJSON(rawToChar(json$content))project_dataframe$nameparams -> to specify the username and jira api key as parameterproject_dataframe$name -> gives out list of projectsWe can do much more by integrating JIRA API with R .Including but not limited to create projects , Upload file , add comments,filters and so on .If you want to see more code snippets on how to read/write (GET/POST) data to/from JIRA using R let me know in comments belowWritten byGowtham RajaramI am a Data Science enthusiast and love working with data :) Because data is the footprint of our existenceFollow1 1 1¬†1¬†1 JiraRest ApiAPIRProgrammingMore from Gowtham RajaramFollowI am a Data Science enthusiast and love working with data :) Because data is the footprint of our existenceMore From MediumObject-Oriented JavaScript‚Ää‚Äî‚ÄäErrors and IterablesJohn Au-Yeung in JavaScript In Plain EnglishCopying to the clipboard in ReactFeargal WalshHow I made my portfolio website blazing fast with GatsbyMaribel Duran in freeCodeCamp.orgUnderstanding Cypress.io: An Automated Testing LibraryRahul Gupta in DSC RNGPITGetting Gatsby WrongChris Vibert in Better ProgrammingMy ‚ÄúWhoa, I didn‚Äôt know that!‚Äù moments with JestBriwa in The StartupHide files and folders in FinderDaniel Rot√§rmelMaterial-UI and React‚Ää‚Äî‚ÄäTheme customizationCyclic BarrierAboutHelpLegalGet the Medium app"
Essential R packages for data science projects,https://medium.com/@ericbonucci/essential-r-packages-for-data-science-projects-d79cb5698b96?source=tag_archive---------3-----------------------,"R,Data Science,Productivity,Community,Rstudio","Leverage genius work from R community in your projects!There are more than 16 000 packages on the Comprehensive R Archive Network (CRAN) that gather a lot of commonly used methods in data science projects. Time runs fast, and it may takes days to code functionalities for sometimes basic tasks‚Ä¶ Fortunately, we can leverage many packages to focus on what is essential for projects to be successful!Quick reminder: install and use packagesThe most common way is to install a package directly from CRAN using the following R command:# this command installs tidyr package from CRANinstall.packages(""tidyr"")Once the package is installed on your local machine, you don‚Äôt need to run this command again, unless you want to update the package with its latest version! If you want to check the version of a package you installed, you may use:# returns tidyr package versionpackageVersion(""tidyr"")RStudio IDE also provides a convenient way to check if any update is available for installed packages in Tools/Check for packages updates‚Ä¶Update all your packages in a few clicks using RStudioLast but not least: how to use a package now it is installed :) You may either specify the package name in front of its included method:stringr::str_replace(""Hello world!"", ""Hello"", ""Hi"")Or run the following command to load all the package‚Äôs functions at once:# load a package: it will throw an error if package is not installedlibrary(stringr)Now you‚Äôre ready to go!If you want to learn basically everything about R packages development, I highly recommend Hadley Wickham R packages book (free online version).Fetching dataFetching data is often the starting point of a data science project: data can be located in a database, an Excel spreadsheet, a comma-separated values (csv) file‚Ä¶ it is essential to be able to read it regardless of its format, and avoid headaches before even starting to work with the data!When data is located in a .csv files or any delimited-values fileThe readr package provides functions that are up to 10 times faster than base R functions to read rectangular data.Great R packages usually have a dedicated hex sticker: https://github.com/rstudio/hex-stickersConvenient methods exist for reading and writing standard .csv files as well as custom files with a custom values separation symbol:# read csv data delimited using comma (,)input_data <- readr::read_csv(""./input_data.csv"")# read csv data delimited using semi-colon (;)input_data <- readr::read_csv2(""./input_data.csv"")# read txt data delimited using whatever symbols (||)input_data <- readr::read_delim(""./input_data.txt"", delim = ""||"")In addition to good looking stickers, great R packages also have cheat sheets you can refer to!When data is located in an Excel fileMicrosoft Excel has its own file formats (.xls and .xlsx) and is very commonly used to store and edit data. The package readxl enables efficient reading of these files into R, you can even only read a specific spreadsheet:# read Excel spreadsheetsinput_data <- readxl::read_excel(""input_data.xlsx"", sheet = ""page2"")When data is located in a database or in the cloudWhen it comes to fetching data from databases, DBI makes it possible to connect to any server, as long as you provide the required credentials, and run SQL queries to fetch data. Because there are many different databases and ways to connect depending on your technical stack, I suggest that you refer to the complete documentation provided by RStudio to find the steps that suit your needs: Databases using R.Make sure to check if a package exists to connect to your favorite cloud services provider! For example, bigrquery enables fetching data from Google BigQuery platform.Wrangling dataYou may have noticed a lot of the previously mentioned packages are part of the tidyverse. This collection of packages forms a powerful toolbox that you can leverage throughout your data science projects. Mastering these packages is key to become super efficient with R.The pipe operator shipped with the magrittr package is a game changer https://github.com/tidyverse/magrittrData wrangling is made easy using the pipe operator, which goal is simply to pipe left-hand values into right-hand expressions:# without pipe operatorpaste(""Hello"", ""world!"")# with pipe operator""Hello"" %>% paste(""world!)It may not seem obvious in this example, but this is a life-changing trick when you need to perform several sequential operations to a given object, typically a data frame.Data frames usually contains your input data, making it the R object you probably work the most with. dplyr is a package that provides useful functions to edit, filter, rearrange or join data frames.library(dplyr)# mtcars is a toy data set shipped with base R# create a column mtcars <- mtcars %>% mutate(vehicle = ""car"")# filter on a columnmtcars <- mtcars %>% filter(cyl >= 6)# create a column AND filter on a columnmtcars <- mtcars %>%   mutate(vehicle = ""car"") %>%  filter(cyl >= 6)Now you should understand my point about the power of the pipe operator :)There is so much more to say about data wrangling that you can find entire books discussing the topic, such as Data Wrangling with R. In addition, a key work on leveraging tidyr functionalities is R for Data Science. A free online version of the latter can be found here. Please notice that these are Amazon affiliated links so I will receive a commission if you decide to buy the books.VisualizationOne of the main reason R is a very good choice for data science projects may be ggplot2. This package makes it easy and eventually fun to build visualizations that looks good and gather a lot of informations.You may find inspirations from this Top 50 ggplot2 visualisation article : http://r-statistics.co/Top50-Ggplot2-Visualizations-MasterList-R-Code.htmlggplot2 is also part of the tidyverse collection, that‚Äôs why it works perfectly with shapes of data you typically obtain after tidyr or dplyr data wrangling operations. Managing to plot histograms and scatter plots is rather quick. Then many additional elements can be used to enhance your plots.Machine learningAnother very convenient package is caret, that wraps up a lot of methods typically used in machine learning processes. From data preparation, to model training and performances assessment, you will find everything you need when working on predictive analytics tasks.I recommend reading the caret chapter about model training where this key task is discussed. Here is a very simple example of how to train a logistic regression:library(dplyr)# say we want to predict iris having a big petal widthobservations <- iris %>%   mutate(y = ifelse(Petal.Width >= 1.5, ""big"", ""small"")) %>%   select(-Petal.Width)# set up a a 10-fold cross-validationtrain_control <- caret::trainControl(method = ""cv"",                                      number = 10,                                      savePredictions = TRUE,                                      classProbs = TRUE)# make it reproducible and train the modelset.seed(123)model <- caret::train(y ~ .,                      data = observations,                      method = ""glm"",                      trControl = train_control,                      metric = ""Accuracy"")Final wordsThanks a lot for reading my very first article on Medium! I feel like there is so much more to say in each section, as I did not talk about other super useful packages such as boot, shiny, shinydashboard, pbapply‚Ä¶ Please share your thoughts in the comments, I am very interested in feedbacks on what you are willing to explore in future articles.Useful documentations and referencesreadr https://readr.tidyverse.org/index.htmlreadxl https://readxl.tidyverse.org/bigrquery https://github.com/r-dbi/bigrquerycaret http://topepo.github.io/caret/index.htmltidyr https://tidyr.tidyverse.org/All RStudio cheatsheets https://rstudio.com/resources/cheatsheets/Written byEric BonucciData scientist based in üìçParis, FranceFollow48 48¬†48¬†RData ScienceProductivityCommunityRstudioMore from Eric BonucciFollowData scientist based in üìçParis, FranceMore From Medium5 Simple Ways to Write Clean CodeShubham Pathania in Better ProgrammingA Practical Step-by-Step Guide to Understanding KubernetesRam Rai in Better ProgrammingIntroduction to Functional ProgrammingSerokell in The StartupApache Kafka Producer/Consumer using Spring BootOshan Fernando in The StartupRefactoring a Flutter Project‚Ää‚Äî‚Ääa Story About Progression and DecisionsGon√ßalo Palma in Flutter CommunityHow to use Java variables like a proTom HenricksenIntroduction to StanfordNLP: An NLP Library for 53 Languages (with Python code)Mohd Sanad Zaki Rizvi in Analytics VidhyaModeling Bowling in ElixirIan GrubbAboutHelpLegalGet the Medium app"
An√°lise de Im√≥veis com Estat√≠stica Descritiva,https://medium.com/@erick.vinicius81/an%C3%A1lise-de-im%C3%B3veis-com-estat%C3%ADstica-descritiva-b4271befd10b?source=tag_archive---------4-----------------------,"Estatistica,Ciencia De Dados,An,R","Esse texto tem objetivo apresentativo, os atributos t√©cnicos, tais como os c√≥digos gerados, podem ser acessados no meu perfil do GitHub:Er1ckAraujo/House-RocketProjeto proposto pelo Meigarom - Data Scientist que tem como objetivo mostrar o poder de uma boa An√°lise Explorat√≥ria‚Ä¶github.comImagine o seguinte cen√°rio: Uma empresa do ramo imobili√°rio chamada House Rocket deseja aumentar seus lucros, para isso, precisa de uma an√°lise para identificar quais s√£o as melhores oportunidades de neg√≥cio. Essa empresa compra e vende im√≥veis, o seu lucro √© a diferen√ßa entre o valor de venda, e o valor de compra. Como Analista, o nosso papel √© responder as seguintes perguntas:1. Quais casas o CEO da House Rocket deveria comprar e por qual pre√ßo de compra?2. Uma vez a casa em posse da empresa, qual o melhor momento para vend√™-las e qual seria o pre√ßo da venda?3. A House Rocket deveria fazer uma reforma para aumentar o pre√ßo da venda? Quais seriam as sugest√µes de mudan√ßas? Qual o incremento no pre√ßo dado por cada op√ß√£o de reforma?Esse projeto foi proposto pelo Meigarom ‚Äî Data Scientist, seu objetivo √© demonstrar como uma boa An√°lise Explorat√≥ria pode gerar bons insights e propor solu√ß√µes para um neg√≥cio. Para saber mais, o desafio pode ser conferido clicando aqui.Sem mais delongas, vamos entender esse banco de dados, e extrair o que ele tem a nos dizer‚Ä¶O House Sales King County, USA √© um banco de dados que cont√©m informa√ß√µes sobre a venda de im√≥veis em King County entre 02/05/2014 at√© 27/05/2015, possui 21613 observa√ß√µes e 21 vari√°veis sendo elas:id ‚Äî Um n√∫mero √∫nico de identifica√ß√£o para cada casa vendida;date ‚Äî Data de venda do im√≥vel no intervalo de um ano, como comentado acima;price ‚Äî Pre√ßo do im√≥vel vendido, que varia de $75,000 at√© $7,700,000, essas casas valem em m√©dia $540,088;bedrooms ‚Äî N√∫mero de quartos. A maioria das casas possuem de 1 a 6 quartos;bathrooms ‚Äî N√∫mero de banheiros, onde n√∫meros inteiros representam a quantidade de banheiros completos, e n√∫meros fracionados representam banheiros incompletos. ‚ÄúO que √© um banheiro incompleto?‚Äù ‚Äî Foi a pergunta que eu me fiz quando tentava entender essa vari√°vel. Banheiros incompletos s√£o como aqueles banheiros externos que temos no Brasil, geralmente encontrados em √°reas de churrasqueira, que possuem apenas vaso sanit√°rio, sem √°rea para banho. √â mais comum encontrar casas que possuem 2.5 banheiros, ou seja, dois banheiros completos, e um incompleto;sqft_living ‚Äî √â o espa√ßo interior da casa em m¬≤. Essas casas possuem entre 290m¬≤ e 13540m¬≤, com m√©dia: 2080m¬≤;sqft_lot ‚Äî Essa vari√°vel representa o tamanho do terreno onde o im√≥vel est√° localizado e varia de 520m¬≤ a 1651359m¬≤, com m√©dia: 15107m¬≤;floors ‚Äî N√∫mero de andares. A maior parte das propriedades possuem 1 andar. Essa vari√°vel, assim como, a vari√°vel bathrooms, recebe uma nota: 1, 1.5, 2, 2.5‚Ä¶ Aqui no Brasil, n√£o √© usual dizer ou descrever: ‚ÄúMeio andar‚Äù, ou ‚Äúdois andares e meio‚Äù. Um andar completo aqui na an√°lise √© considerado um andar que ocupa toda √°rea de base ocupada pelo primeiro;Waterfront ‚Äî Uma vari√°vel bin√°ria que diz se uma casa possui vista pro mar, ou n√£o. Apenas 0.75% das casas possuem vista pro mar;View ‚Äî Um √≠ndice de 0 a 4 que avalia o qu√£o boa √© a vista da propriedade;Condition ‚Äî Um √≠ndice de 1 a 5 da condi√ß√£o do apartamento;grade ‚Äî Um √≠ndice de 1 a 13, onde 1‚Äì3 fica aqu√©m da constru√ß√£o e design dos edif√≠cios, 7 tem um n√≠vel m√©dio de constru√ß√£o e design e 11‚Äì13 t√™m um alto n√≠vel de qualidade de constru√ß√£o e design.yr_built ‚Äî Ano de constru√ß√£o do im√≥vel. Varia entre 1900 e 2015;H√° ainda algumas vari√°veis que foram omitidas por mim nessa apresenta√ß√£o inicial por dois motivos, primeiro, para que a leitura fique menos densa, e segundo que dado o nosso objetivo de encontrar oportunidades de compra e venda de im√≥veis, algumas vari√°veis n√£o s√£o t√£o relevantes para o nosso prop√≥sito, como por exemplo, sqft_living15. Essa vari√°vel, descreve a metragem quadrada do espa√ßo habitacional para os 15 vizinhos mais pr√≥ximos. Imagine voc√™ na posse de um im√≥vel que deseja vender, antes de vender, voc√™ pretende fazer algumas reformas e modifica√ß√µes para que o seu im√≥vel fique mais atrativo para a venda, ainda que a localiza√ß√£o dessa propriedade tenha influ√™ncia sobre seu pre√ßo, n√£o √© vi√°vel pensar em melhorias que englobem uma fra√ß√£o do seu bairro. O que eu estou tentando dizer? Dado uma propriedade, algumas vari√°veis est√£o fora do nosso poder de modifica√ß√£o, como CEP por exemplo. N√£o h√° reforma que mude um im√≥vel de lugar. Voltando alguns passos, √© interessante observar tamb√©m que, as propriedades possuem, em m√©dia, 2080 m¬≤ de constru√ß√£o, e 15107m¬≤ de terreno, ou seja, estamos lidando com im√≥veis com grande potencial construtivo, visto que h√° espa√ßo dispon√≠vel.√â importante lembrar do objetivo da nossa an√°lise para que ela n√£o se perca no meio de tantas informa√ß√µes.O meu processo investigativo tem como objetivo encontrar vari√°veis que influenciam o pre√ßo da propriedade. Sendo assim, vamos avaliar a correla√ß√£o de algumas vari√°veis num√©ricas:Maiores correla√ß√µes: Price vs sqft_living (0.702); sqft_living vs sqft_above (0.877); sqft_above vs Price (0.606)A vari√°vel sqft_above √© uma das vari√°veis que foram omitidas na introdu√ß√£o, ela descreve a metragem quadrada da habita√ß√£o acima do n√≠vel do solo, ou seja, o tamanho total da propriedade. √â natural que essa vari√°vel apresente uma rela√ß√£o forte com o pre√ßo, e mais ainda com o tamanho da casa propriamente dito.Podemos concluir, a partir dessa tabela, que o tamanho da casa influencia fortemente seu pre√ßo.Agora, vamos analisar as vari√°veis categ√≥ricas, por√©m, gostaria de lembrar que os nossos dados ser√£o analisados sob duas perspectivas: Em um primeiro momento com o objetivo de compra, e posteriormente com o objetivo de venda.Sendo assim, nada melhor pra guiar a nossa an√°lise do que as perguntas propostas inicialmente:1- Quais casas o CEO House Rocket deveria comprar e por qual pre√ßo?Vamos analisar algumas caracter√≠sticas dos im√≥veis, e no fim, definir um perfil de compra.QuartosMinha hip√≥tese inicial era que o pre√ßo da casa aumentava a medida que o n√∫mero de quartos aumentava, por√©m, no gr√°fico acima vemos que n√£o necessariamente. Os maiores pre√ßos s√£o observados em propriedades com 5 e 6 quartos.Os menores pre√ßos, por sua vez, est√£o localizados em im√≥veis que possuem de 0 a 3 quartos. Apliquei um filtro sob o banco de dados e descobri que essas propriedades custam em m√©dia: $449,873.9. Adquirir im√≥veis com poucos ou nenhum quarto pode ser interessante.BanheirosAs casas aumentam de pre√ßo √† medida que o n√∫mero que banheiros aumenta. Aparentemente banheiros s√£o itens valiosos em um im√≥vel.Para selecionar im√≥veis com um pre√ßo mais atrativo, filtrei propriedades que possuem at√© 2.5 banheiros. Casas com essa caracter√≠stica custam em m√©dia $466,453.9.AndaresSe observarmos o gr√°fico, veremos que im√≥veis com 2.5 andares est√£o levemente acima dos outros, eles possuem uma amplitude maior, e s√£o sim, em m√©dia, mais caros que os outros. Um dos fatores que elevou essa m√©dia, √© o fato da propriedade mais cara da tabela possuir dois andares e meio. O ponto isolado na coluna 2.5 √© o im√≥vel mais caro do nosso banco de dados. Sem distrair do nosso objetivo, casas com 1 andar s√£o as mais baratas e custam em m√©dia: $442,180.6.Vista pro mar√â intuitivo presumir que propriedades com vista pro mar s√£o mais valiosas. Uma casa com vista pro mar √©, em m√©dia 212.63% mais cara que uma casa sem vista pro mar, e custam em m√©dia $1,661,876. √â um pre√ßo bem alto em compara√ß√£o aos valores que viemos observando anteriormente. Uma casa com vista pro mar, pode at√© ganhar uma valoriza√ß√£o generosa por causa de sua localiza√ß√£o, por√©m, podemos reparar que a propriedade mais valiosa n√£o possui vista para o mar. Ou seja, da casa mais barata at√© a mais cara, nenhuma possui vista para o mar, e essa dist√¢ncia entre as valoriza√ß√µes √© justamente a brecha que procuramos para maximizar os lucros da House Rocket. Portanto, casas sem vista pro mar, s√£o melhores investimentos, pois o pre√ßo de compra √© mais atrativo, custando em m√©dia $531,563.6.VistaIm√≥veis que possuem uma melhor avalia√ß√£o no quesito vista, tamb√©m possuem uma valoriza√ß√£o maior, essa vari√°vel est√° relacionada com a localiza√ß√£o do im√≥vel, e assim como a vista pro mar, essa √© uma vari√°vel que est√° fora do nosso alcance de modifica√ß√£o, ent√£o √© importante que a escolha nesse quesito seja feita com aten√ß√£o. Se observarmos, os maiores outiliers (pontos fora da curva), as duas propriedades mais caras, possuem nota 2 e 3, que podem ser consideradas notas media e boa respectivamente. Considerando im√≥veis, nota 2 e 3, encontraremos im√≥veis que custam em m√©dia $854,571.9.Condi√ß√£oA condi√ß√£o dos apartamentos n√£o parece ser um fator muito definitivo na sua valoriza√ß√£o, j√° que a m√©dia dos valores entre as notas parece n√£o variar muito. Logo, adquirir im√≥veis em condi√ß√µes 1 e 2, ou seja, em p√©ssimas condi√ß√µes n√£o parece ser um bom neg√≥cio.Im√≥veis em condi√ß√µes 3 e 4, consideradas m√©dia e boa, possuem as maiores valoriza√ß√µes fora da curva, e custam em m√©dia $536,016.NotasIm√≥veis com nota de 1 a 8, considerados de ruim a bom, custam em m√©dia $437,284. A partir da√≠, a valoriza√ß√£o come√ßa a aumentar quase de maneira exponencial. Im√≥veis acima de nota 8 custam em m√©dia $959,962.4Tendo essas considera√ß√µes em mente, vamos tra√ßar um perfil de casa ideais para compra:-Casas que possuem de 0 a 3 quartos;-0 a 2.5 banheiros;-1 andar;-sem vista pro mar;-nota da vista entre 2 e 3;-condi√ß√µes 3 e 4;-nota geral de 1 a 8;Im√≥veis com essas caracter√≠sticas possuem valores entre $154,000.0 e $1,500,000.0. Com m√©dia $502,808.0.Visando valoriza√ß√£o futura, esses s√£o os melhores im√≥veis para compra.2- Uma vez a casa em posse da empresa, qual o melhor momento para vend√™-las, e qual seria o pre√ßo da venda?A vari√°vel ‚Äúdate‚Äù, nos diz respeito a data de venda de um determinado im√≥vel. Para que a visualiza√ß√£o desses dados ficasse mais amig√°vel, classifiquei os meses em 12 fatores, que variam de 1 a 12, e cada n√∫mero representa o m√™s de mesma ordem. 1 para janeiro, 2 para fevereiro, e assim recursivamente‚Ä¶DataPodemos observar que existe um grande volume de negocia√ß√µes no m√™s de maio e nos meses vizinhos. Por√©m, se avaliarmos os pre√ßos, veremos que maio n√£o foi o m√™s onde ouveram as negocia√ß√µes mais valiosas. Abril, Junho, Agosto, Setembro e Outubro foram os meses mais aquecidos no que diz respeito ao pre√ßo das negocia√ß√µes.Vale a pena dar uma olhada detalhada nesses im√≥veis para observar quais caracter√≠sticas os deixaram t√£o valiosos. Como o objetivo da House Rocket √© compra e venda, a melhor maneira de agregar valor a essas propriedades √© por meio de uma reforma, sendo assim, n√£o vale a pena focar em atributos imut√°veis, como CEP, vista pro mar‚Ä¶Nosso objetivo √© observar atributos que possam ser modificados, e consequentemente agregar valor, como: Quantidade de quartos, banheiros, andares, e o tamanho geral da propriedade.Na tabela mostrada, podemos observar o perfil das propriedades mais valiosas referente aos meses citados. Na primeira linha, por exemplo, temos uma casa com 5 quartos, 5 banheiros, 8 mil m¬≤, 2 andares, avaliada em $5,350,000 e vendida no m√™s de Abril.A tabela √© um catalogo enxuto das principais caracter√≠sticas dos im√≥veis mais valiosos desse per√≠odo.Usando esse perfil como refer√™ncia, podemos ter uma no√ß√£o de qual ser√° o pre√ßo de revenda quando os im√≥veis em posse da House Rocket alcan√ßarem um padr√£o de qualidade fora da curva. Ou seja, casas que possuam de 4 a 5 quartos, 4 a 8 banheiros completos, dois andares, e que tenha no m√≠nimo 5 mil m¬≤. Aplicando um filtro com essas especifica√ß√µes no nosso banco de dados podemos observar que casas com esse perfil possuem um pre√ßo de venda entre $784,500 e $7,700,000, com m√©dia $2,346,534. Comprando im√≥veis com a m√©dia de compra que estimamos na primeira pergunta ($502,808.0), e vendendo pela m√©dia de venda estimada, temos uma possibilidade de retorno maior que 300%. Uau! Parece um ganho astron√¥mico, por√©m, essa simula√ß√£o n√£o leva em conta os gastos com as reformas e todas as despesas geradas em todo o contexto de um empreendimento imobili√°rio real.3- A House Rocket deveria fazer uma reforma para aumentar o pre√ßo da venda? Quais seriam as sugest√µes de mudan√ßas e qual o incremento no pre√ßo, dado por cada op√ß√£o de reforma?Sim. Como vimos na pergunta n√∫mero 2, as reformas s√£o o ponto chave no ganho da valoriza√ß√£o de um im√≥vel. Vamos conferir logo abaixo algumas sugest√µes de reformas:BanheirosOs banheiros s√£o um belo atrativo em termos de valoriza√ß√£o imobili√°ria. Cada banheiro completo, gera, em m√©dia, uma valoriza√ß√£o de 59% para o im√≥vel.QuartosCada quarto gera uma valoriza√ß√£o, em m√©dia, de 22% no pre√ßo total do im√≥vel.AndaresCada andar gera uma valoriza√ß√£o, em m√©dia, de 47% no pre√ßo total do im√≥vel.Se for parar pra pensar, o valor agregado de um andar, √© menor que o valor de um banheiro novo.Conclus√£o√â importante lembrar que as vari√°veis avaliadas n√£o s√£o independentes. Portanto, o tamanho do im√≥vel influencia na quantidade de banheiros, de quartos, andares e vice-versa. A an√°lise tamb√©m n√£o leva em considera√ß√£o despesas de nenhuma esp√©cie. √â um projeto de estudo, e agora o CEO da House Rocket j√° sabe onde investir!Written byErick Ara√∫joGraduando em Estat√≠stica pela Universidade Federal de Ouro Preto (UFOP)FollowEstatisticaCiencia De DadosAn√°lise Explorat√≥riaRMore from Erick Ara√∫joFollowGraduando em Estat√≠stica pela Universidade Federal de Ouro Preto (UFOP)More From MediumFast transition between dplyr and data.tableNata Berishvili in Towards Data ScienceVanilla Neural Networks in RChris Mahoney in Towards Data ScienceInterpret Regression Analysis Results using R: Biomedical DataSwayanshu Shanti Pragnya in Analytics VidhyaDonald Trump Won, No Matter What Happens NextJessica Wildfire in The Apeiron Blog(Why) There Was no Biden Landslideumair haque in Eudaimonia and CoThe Election Should Never Have Been This Closeumair haque in Eudaimonia and Co20 Things Most People Learn Too Late In LifeNicolas Cole in Better AdviceThis Is ‚ÄòI Wish a Motherf*cker Would‚Äô Week for Black PeopleMarley K. in ZORAAboutHelpLegalGet the Medium app"
Hierarchical Clustering (Bagian 1): Contoh Sederhana Hierarchical Clustering dengan R,https://medium.com/@ikaokay/hierarchical-clustering-bagian-1-contoh-sederhana-hierarchical-clustering-dengan-r-76bf489474a9?source=tag_archive---------5-----------------------,"Hierarchical Clustering,Clustering Analysis,R","Source: Module Doc ‚Äî Cluster MapAsslamu‚Äôalaikum sobat data..Pada kesempatan kali ini kita akan belajar terkait suatu metode pengelompokan yaitu hierarchical clustering yang juga digunakan sebagai salah satu teknik dalam data mining.Tapi sebelumnya, mari kita ketahui bersama apa yang dimaksud hierarchical clustering!Pengertian Hierarchical ClusteringHierarchical Clustering atau Pengelompokan hirarki adalah suatu metode pengelompokan data yang dimulai dengan mengelompokkan dua atau lebih objek yang memiliki kesamaan paling dekat. Kemudian proses diteruskan ke objek lain yang memiliki kedekatan kedua. Demikian seterusnya sehingga cluster akan membentuk semacam pohon dimana ada 6 hierarki (tingkatan) yang jelas antar objek, dari yang paling mirip sampai yang paling tidak mirip. Secara logika semua objek pada akhirnya hanya akan membentuk sebuah cluster. Dendogram biasanya digunakan untuk membantu memperjelas proses hierarki tersebut.Penerapan Hierarchical ClusteringBaiklah, sekarang kita akan membuat contoh sederhana dari pengelompokan hirarki. Berikut adalah contoh data yang digunakan:Source: Chapter 15 ‚Äî Cluster analysisLangkah pertama adalah meng-install package, tetapi jika di aplikasi R yang sobat miliki sudah tersedia package berikut, maka cukup panggil saja dengan perintah ‚Äúlibrary()‚Äù:install.packages(""cluster"")install.packages(""dendextend"")install.packages(""factoextra"")library(cluster)library(dendextend)library(factoextra)Selanjutnya meng-input data seperti berikut:X1=c(3,4,2,5,1,4)X2=c(2,1,5,2,6,2)data1=data.frame(X1,X2)data1Maka hasil data framenya adalah,karena dari data frame di atas belum memuat nama subjeknya maka akan kita tambahkan dengan cara berikut:colnames(data1)=c(‚ÄúX1‚Äù,‚ÄùX2"")rownames(data1)=c(‚Äúa‚Äù,‚Äùb‚Äù,‚Äùc‚Äù,‚Äùd‚Äù,‚Äùe‚Äù,‚Äùf‚Äù)data1Sehingga hasilnya adalah,Tahap selanjutnya, kita coba untuk membuat plot data menggunakan perintah:plot(data1)text(data1,rownames(data1))Dan hasilnya adalah,Berdasarkan plot di atas diketahui bahwa data menyebar dan membagi menjadi dua kelompok besar. Dimana kelompok satu yang terdiri dari empat kelompok observasi yaitu a, b, d, dan f. Sedangkan, kelompok dua terdiri dari dua kelompok observasi yaitu e dan c.Berikutnya dalam pengelompokan ini, kita perlu untuk menghitung jarak antar data. Perintahnya adalah sebagi berikut:dist(data1)Maka hasilnya adalah,Setelah mengetahui jaraknya, bisa kita lakuka peng-cluster-an dengan salah satu metode hirarki yaitu single:data1.hc=hclust(dist(data1), ‚Äúsingle‚Äù)plot(data1.hc,hang=-1)Sehingga hasilnya adalah sebagai berikut,Setelah diperoleh ouput dendogram, maka kita dapat membuat garis cluster-nya sehingga kita bisa melihat anggota dari masing-masing kelompok serta.Perintahnya adalah sebagi berikut:rect.hclust(data1.hc,k=2,border=2:3)data1.cut<-cutree(data1.hc,2)rownames(data1)[data1.cut==1]rownames(data1)[data1.cut==2]Maka hasilnya adalah,Dari dendogram di atas dapat diketahui bahwa dari data yang ada telah dibentuk 2 kluster dengan masing-masing anggota adalah a, b, d, dan f untuk kelompok 1, sedangkan untuk kelompok 2 anggotanya adalah c dan e.Dan berikut adalah hasil dari kelompok yang telah di cut pada dendogram yang menunjukkan output anggota yang sama:Demikian artikel kali ini terkaitcontoh sederhana hierarchical clustering.Sampai jumpa di artikel berikutnya.Terima Kasih dan semoga bermanfaat ya sobat! :)Wassalamu‚Äôalaikum Warrahmatullahi Wabarakatuh‚Ä¶Referensi:http://eprints.umpo.ac.id/3039/3/BAB%20II.pdfhttp://www.yorku.ca/ptryfos/f1500.pdfWritten byRizka KhairunnisaFollowHierarchical ClusteringClustering AnalysisRMore from Rizka KhairunnisaFollowMore From MediumClustering Techniques: Hierarchical and Non-HierarchicalM Bharathwaj in Towards Data ScienceFast transition between dplyr and data.tableNata Berishvili in Towards Data ScienceEconomics for Tech People‚Ää‚Äî‚ÄäEquilibrium (Part 3)Tyler Harris in Towards Data ScienceA Study of the Hierarchical Clustering: Unsupervised Machine LearningElias Hossain in Analytics VidhyaHierarchy ClusteringHamza Issa in AI In Plain EnglishHow to use R in Google ColabEd Adityawarman in Towards Data ScienceThree Different Lessons from Three Different Clustering Analyses: Data Science CapstoneMichio SuginooTiming matters in imposing measures for COVID-19Catherine Lopes Ph.D. in Towards Data ScienceAboutHelpLegalGet the Medium app"
N/A,https://medium.com/@h88129/r-sql%E5%B0%87data-frame-insert-ioto%E5%88%B0sql-653f733f01a4?source=tag_archive---------6-----------------------,"R,Sql,Dataframes","Á¥ÄÈåÑ‰∏ç‰ΩøÁî®PASTE‰æÜÂ∞ádata.frameÁõ¥Êé•ÂåØÂÖ•mariadb‰∏≠Âü∫Êú¨ÈÄ£Êé•mariadblibrary(DBI);library(RMariaDB)con <- dbConnect(RMariaDB::MariaDB(),username=‚Äùroot‚Äù,password=‚Äùtoyo123"",group=‚Äùmy-db‚Äù,dbname=‚Äùtest2"",host=‚Äù127.0.0.1"",port=3303)2.Âª∫Á´ãdata.framedf=data.frame(var1=1,var2=2)3.Âª∫Á´ãÂ§öÁ≠ÜË≥áÊñôdf=rbind(df,df)4.ÂØ´ÂÖ•DBdbWriteTable(con,‚Äùtest_r‚Äù, df, append=TRUE, row.names=FALSE)conÔºöDBÁöÑÈÄ£Êé•Âè£""test_r"":DBË°®Ê†ºÂêçÁ®±df:data.frameWritten byAllenÂÄã‰∫∫Á≠ÜË®ò(https://tw.linkedin.com/in/%E9%A7%BF%E5%AE%8F-%E5%BC%B5-10a847177)FollowRSqlDataframesMore from AllenFollowÂÄã‰∫∫Á≠ÜË®ò(https://tw.linkedin.com/in/%E9%A7%BF%E5%AE%8F-%E5%BC%B5-10a847177)More From Medium40 Best Ruby Gems We Can‚Äôt Live WithoutCodica Team in Codica JournalThe Knights of Functional Programming fight the Imperative Dragon.Sam FareAn Introduction to the Java Memory ModelPrashant Pandey in The StartupA Principled Approach to GraphQL Query Cost AnalysisJames Davis in Dev GeniusUnderstanding Window FunctionsGarrett Edel in The StartupA Combination of TimerTask and TimerHank Leeüë®üèª‚Äçüíªüáπüáºüá¶üá∫What Happens After Prisoners Learn to Code?The Atlantic in The AtlanticAWS Key Management Service: All You Need to KnowShilpi Gupta in Better ProgrammingAboutHelpLegalGet the Medium app"
Back-linking of terms in R scripts for use in Obsidian,https://medium.com/@kee.adrian/back-linking-of-terms-in-r-scripts-for-use-in-obsidian-99d43505b6a2?source=tag_archive---------0-----------------------,"Obsidian,Backlink,R","Problem: Although I was able to load my R scripts as markdown files for use in Obsidian (see my previous post), there was no back-linking tags and thus the power of Obsidian, which is in the linkage graphs, is not harnessed. It would be nice if all the terms of my interest in the related R scripts can be linked up in Obsidian fashion.Solution: Just loop through each markdown file and change the terms of interest to include the back-linked syntax. E.g., if the term of interest is mindfulness, changing all occurrences of it to [[mindfulness]] will do the trick. The result is that I can get a node called mindfulness (as seen below), and can check out the identified co-occurrences more easily.The R codes for achieving this is pretty simple.First, create a vector of the file names in the directory containing the Obsidian markdown files.all_files<-list.files(pattern = ‚Äò\\.md$‚Äô)Next, define the terms to change (term_to_change) and define how the edited term (edited_term) would look like. The edited_term has the extra square brackets that will make back-linking work. I am interested in the term mindfulness in the example below.term_to_change<-‚Äúmindfulness‚Äùedited_term<-‚Äú[[mindfulness]]‚ÄùThen, we are ready to loop through each markdown file, grab its content, replace the term_to_change to edited_term accordingly, before finally saving the markdown file with the new contents. Since all_files contains the markdown file names, the write function uses this information directly.for (i in 1:length(all_files)){ data<-readLines(all_files[i]) updated<-str_replace(data, term_to_change, edited_term) write(updated, all_files[i])}The result is that you now have an updated markdown file which has the square brackets added to the term of interest, as shown below. This will facilitate the back-linking in Obsidian.The entire script is shown below.all_files<-list.files(pattern = ‚Äò\\.md$‚Äô)term_to_change<-‚Äúmindfulness‚Äùedited_term<-‚Äú[[mindfulness]]‚Äùfor (i in 1:length(all_files)){ data<-readLines(all_files[i]) updated<-str_replace(data, term_to_change, edited_term) write(updated, all_files[i])}To use this script, place it in the folder where you keep the markdown files for Obsidian. As usual, take extra and necessary CAUTION, such as backing up your files before trying out this script. It changes your files content as the write function is used.Written byAdrian KeeResearcher on mindfulness, interested in coding. I tweet @keefellow. https://orcid.org/0000-0003-2839-0461FollowObsidianBacklinkRMore from Adrian KeeFollowResearcher on mindfulness, interested in coding. I tweet @keefellow. https://orcid.org/0000-0003-2839-0461More From MediumReversing an n-bit number in O(log n) timeEhud Tamir in HackerNoon.comIntro to Computer Science‚Ää‚Äî‚ÄäIn 5 Minutes (with python)Tyler WalkerSolve Async Callbacks with FutureBuilder!Sayan Mondal in Flutter CommunityStoring & retrieving PostgreSQL JSON data using GorpAlvin RizkiSet Up Varnish-Cache 4 for WordPressEni Sinanaj in Better Programming3 Ways to Explore a Python ObjectYang Zhou in TechToFreedomWhat‚Äôs coming to Rails 6.0?Guy Maliar in Ruby InsideReady, Steady, Connect. Help Your Organization to Appreciate KafkaSPOUD in The StartupAboutHelpLegalGet the Medium app"
Running backs importam? Analisando dados da NFL com R,https://medium.com/@riquecardoso_/running-backs-importam-analisando-dados-da-nfl-com-r-5398bc22456f?source=tag_archive---------0-----------------------,"Data Science,R,NFL,Football,Statistics","A an√°lise de dados √© uma tend√™ncia que veio pra ficar no mundo dos esportes, em especial no futebol americano. Nas √∫ltimas d√©cadas, vimos um crescimento do uso das estat√≠sticas, n√£o apenas nos programas de TV como tamb√©m com os fans, que utilizam esses dados para chegar em suas pr√≥prias conclus√µes. Fan-pages e tamb√©m servi√ßos focados em oferecer e tratar esses dados surgiram para aquecer a paix√£o do f√£ de esportes.Com o tempo fomos percebendo que algumas dessas m√©tricas de performance n√£o faziam muito sentido e acabavam distorcendo muito a real import√¢ncia e performance dos jogadores. Os torcedores se dividiram entre a turma que seguia rigorosamente esses dados para formar opini√£o, e os que preferiam confiar no ‚Äúteste do olho‚Äù: assistir o jogo e julgar criticamente sem mergulhar tanto nos stats.Quem seguia os dados cegamente caia na armadilha de acreditar que ‚Äúrunning backs n√£o importam‚Äù, que √© a ideia de que todos os running backs tem mais ou menos o mesmo valor, como um commodity, pois grande parte da performance dos corredores √© por conta do trabalho da linha ofensiva, esquema do ataque, alinhamento da defesa e outros fatores. √â verdade que tudo isso importa muito! Por√©m, quem for realizar a an√°lise pelo v√≠deo do jogo, observando cada jogada, percebe com facilidade que existe diferen√ßa de qualidade entre os RBs, e voc√™ ter um jogador de elite nessa posi√ß√£o pode aumentar a sua chance de vencer o jogo.Ent√£o os dados mentem?Como o futebol americano √© um esporte coletivo, infinitos fatores influenciam no resultado de uma jogada. Os dados simples (p√∫blicos) de uma jogada podem te informar se uma corrida deu certo para o ataque, mas n√£o √© poss√≠vel mensurar o quanto disso √© responsabilidade do corredor. E simplesmente presumir que todos os corredores s√£o igualmente bons para n√£o ter que lidar com esse problema, √© esconder uma dimens√£o enorme do jogo.A partir de 2018 a NFL passou a realizar o Big Data Bowl, que √© uma competi√ß√£o de analytics no Kaggle, para a cria√ß√£o de novas m√©tricas e novas conclus√µes serem tiradas do esporte, tendo em vista a quantidade enorme de dados que conseguimos extrair hoje em dia. O vencedor de 2019, Matt Ploenzke, acabou sendo contratado para o departamento de Data Science do San Francisco 49ers. A competi√ß√£o liberou dados avan√ßados, como a localiza√ß√£o exata, velocidade e acelera√ß√£o de todos os jogadores no campo, de uma determinada amostra de jogos. O tema para 2019 era criar um modelo preditivo do resultado de uma corrida, dado um retrato do exato momento do snap, e 1 segundo ap√≥s o snap.O trabalho de Ploenzke primeiro determinou a relev√¢ncia de cada feature para o resultado da jogada. A dist√¢ncia entre o corredor e o primeiro defensor desbloqueado foi considerado o fator de maior relev√¢ncia para o sucesso ou fracasso da jogada, assim como a acelera√ß√£o do corredor ainda no backfield.Por conta de limita√ß√µes nas regras da competi√ß√£o (n√£o poderiam haver features individuais da performance de cada jogador), o modelo alcan√ßou boa predi√ß√£o nas corridas que resultavam em poucas jardas (at√© +/- 10j), e perdeu vertiginosamente a capacidade preditiva nas maiores corridas. Isso me fez pensar se existe alguma forma coerente de avaliar running backs com os dados p√∫blicos, at√© mais simples, por meio de pacotes como o nflscrapR.Conhecendo o esporte, a conclus√£o que eu pude tirar ‚Äúde olho‚Äù √© que a corrida depende muito mais de fatores externos ao corredor nas primeiras jardas. Ou seja, do momento do snap, at√© ultrapassar o box defensivo, a jogada depende muito de fatores como o trabalho de bloqueios da linha ofensiva, as forma√ß√µes do ataque e da defesa, sorte, etc. J√° no momento em que o running back ultrapassou o box defensivo e tem apenas o √∫ltimo n√≠vel da defesa a sua frente, √© nesse momento, acredito eu, que conseguimos mensurar melhor a capacidade de um corredor em formar big plays (jogadas para enorme ganho de jardas). A capacidade do corredor ent√£o, se mensura n√£o nas estat√≠sticas da sua produ√ß√£o total, mas na capacidade de transformar ganhos de 8 jardas, em ganhos de 20 ou mais jardas, por exemplo.√â l√≥gico que existe sim muito m√©rito dos corredores no processo de ultrapassar o box defensivo. Nas primeiras jardas, s√£o importantes fundamentos como as leituras que um corredor faz, sua capacidade de quebrar tackles, ganhar velocidade, mudar de dire√ß√£o com rapidez, dentre outros. No entanto, esses fundamentos n√£o acabam sendo refletidos nas estat√≠sticas simples, que foram o meu objeto de estudo. Se eu fosse levar esses importantes fundamentos em considera√ß√£o, seria preciso avaliar cada jogada minuciosamente e ir adicionando essas features. Minha proposta era tentar chegar numa conclus√£o eficiente com dados abertos ao p√∫blico, ent√£o realizei algo mais simples.Para avaliar as estat√≠sticas simples, utilizei a linguagem R com o pacote nflscrapR, que retorna um play-by-play com literalmente centenas de informa√ß√µes p√∫blicas sobre cada jogada. Irei utilizar a temporada 2019 para essa an√°lise. No final desse artigo estar√° um link com um baita tutorial para brincar com essas ferramentas.library(tidyverse)library(dplyr)library(na.tools)library(ggimage)pbp <- read_csv(url(‚Äúhttps://github.com/ryurko/nflscrapR-data/raw/master/play_by_play_data/regular_season/reg_pbp_2019.csv""))Primeiro importamos algumas importantes bibliotecas e criamos uma vari√°vel ‚Äúpbp‚Äù com todas as jogadas de 2019. Em seguida, filtramos e criamos uma vari√°vel ‚Äúcorridas‚Äù com apenas as corridas e um n√∫mero limitado de colunas. N√≥s vamos solicitar um summary e um histograma das corridas para analisar esses n√∫meros melhor:corridas <- pbp %>% filter(rush_attempt==1, play_type==""run"") %>% select(desc, rush_attempt, yards_gained, rusher_player_name, rush_touchdown, epa, posteam, td_prob, rusher_player_id)summary(corridas[""yards_gained""])hist(corridas$yards_gained, breaks= 60)#summary output:  yards_gained     Min.   :-15.000   1st Qu.:  1.000   Median :  3.000   Mean   :  4.472   3rd Qu.:  6.000   Max.   : 91.000Histograma da ocorr√™ncia do ganho de jardas em todas as corridas da temporada 2019Como pude perceber, apenas 25% das corridas s√£o para 6 jardas ou mais.A seguir, crio uma variavel ‚ÄúcorridasMaisDe8‚Äù com apenas as corridas que resultaram em mais de 8 jardas. Uso dois tipos diferentes de summary, e ploto um histograma com essas corridas:corridasMaisDe8 <- corridas %>% filter(yards_gained >= 9) summary(corridasMaisDe8[""yards_gained""])hist(corridasMaisDe8$yards_gained, breaks = 60)corridasMaisDe8 %>% group_by(yards_gained) %>% summarize(Corridas = n()) %>% arrange(desc(Corridas))#output summary 1:  yards_gained   Min.   : 9.00   1st Qu.:10.00   Median :12.00   Mean   :15.56   3rd Qu.:17.00   Max.   :91.00Sum√°rio 2: A quantidade de corridas que resultou num determinado n√∫mero de jardas. (2019)Histograma contendo apenas as corridas que resultaram em pelo menos 9 jardas (2019).Em seguida, crio a minha primeira m√©trica. Estou rankeando os corredores pelo EPA M√©dio que eles conseguiram, apenas nas jogadas que resultaram em no m√≠nimo 9 jardas, e considerando apenas os corredores que tiveram pelo menos 20 dessas corridas. EPA significa ‚ÄúExpected Points Added‚Äù. Basicamente √© uma m√©trica da efici√™ncia do ataque, que mensura o quanto uma jogada contribuiu para aumentar as chances do ataque de pontuar em uma campanha. Como estamos presumindo que ap√≥s o passar o box defensivo a qualidade individual do corredor passa a ser um grande diferencial, podemos dizer que o EPA M√©dio nessas jogadas √© m√©rito do corredor.Se fossemos julgar o EPA M√©dio considerando todas as jogadas, inclusive aquelas que resultaram em poucas jardas, iriamos regredir todos os jogadores √† m√©dia, pois haveria uma interfer√™ncia muito maior daqueles outros fatores que discutimos. Seria menos influenciado pelo m√©rito individual, e mais pelo m√©rito coletivo dos dois times, e do acaso.Obs: Nesse ponto do estudo, ainda n√£o filtramos para retirar os Quarterbacks da lista. No √∫ltimo gr√°fico, plotaremos apenas os n√∫meros dos running backs.rank <- corridasMaisDe8 %>% group_by(rusher_player_id) %>% mutate(EpaMedio = mean(epa, na.rm = TRUE), TotalEpa = sum(epa, na.rm = TRUE), Time = posteam, TDs = sum(rush_touchdown), ""Jardas Por Carregada""=mean(yards_gained), Corridas=n()) %>% select (Time, rusher_player_name, EpaMedio, TotalEpa, Corridas,  TDs) %>% distinct %>% arrange(desc(EpaMedio),  na.rm = TRUE) %>% filter(Corridas >= 20)rankRanking por EpaMedio, p√°gina 1.Ranking por EpaMedio, p√°gina 2.Com raras surpresas, vemos uma lista com corredores bem conceituados, o que pode indicar que a minha premissa inicial estava correta. Vamos seguir com a an√°lise dos dados.O EpaMedio parece bem realista, mas tamb√©m criei uma m√©trica que pode contribuir mais ainda ao debate: O OpenFieldRatio.O OFR √© divis√£o que tenta responder a seguinte pergunta:Dessas corridas para pelo menos 9 jardas, quantos % delas resultaram em big plays?Para definir um conceito objetivo de big play, utilizei:Corridas que resultaram em no m√≠nimo 20 jardas, OU que resultaram em no m√≠nimo 9 jardas e foram touchdowns.corridas <- corridas %>%  mutate(    CL = if_else(yards_gained >= 9, 1 , 0), #clutch line    BPS = if_else(yards_gained >= 20 | (yards_gained >= 9 & rush_touchdown == 1), 1, 0) #sucesso na big play  )rank2 <- corridas %>% filter(yards_gained >= 9) %>% group_by(rusher_player_id) %>% mutate(EpaMedio = mean(epa, na.rm = TRUE), TotalEpa = sum(epa, na.rm = TRUE), Time = posteam, TDs = sum(rush_touchdown), ""Jardas Por Carregada""=mean(yards_gained), Corridas=n(), OpenFieldRatio = sum(BPS) / sum(CL), TotalBPS = sum(BPS), TotalCL = sum(CL)) %>% select (Time, rusher_player_name, OpenFieldRatio, TotalBPS, TotalCL, EpaMedio, TotalEpa, Corridas,  TDs) %>% distinct %>% arrange(desc(OpenFieldRatio),  na.rm = TRUE) %>% filter(Corridas >= 20)rank2OFR. P√°gina 1.OFR. P√°gina 2.Alguns nomes j√° conhecidos, com leves mudan√ßas em rela√ß√£o √† m√©trica anterior, mas uma coisa chama bastante aten√ß√£o:√â inadmiss√≠vel um ranking de Rb que tenha Cristian McCaffrey, discutivelmente o melhor running back da liga, apenas na 3¬™ p√°gina. Isso pode ser explicado pelo fato de ele ter muitas jogadas em que correu mais do que 9 jardas, mas ter ganho menos de 20 jardas na maioria delas.Para entender, olhem a linha do Josh Jacobs. TotalCL seria a quantidade de vezes em que ele correu pelo menos 9 jardas (32 vezes). TotalBPS seria a quantidade dessas vezes que o ganho foi de pelo menos 20 jardas ou um TD (10 vezes). OpenFieldRatio: 10 / 32 = 0.31.N√∫meros do McCaffrey: 7/38 = 0.18.√â, ficou bastante question√°vel se o OFR retrata a realidade. O ranking anterior por EPAMedio pareceu mais veross√≠mil.Resolvi ent√£o plotar um gr√°fico com essas duas m√©tricas, para que possamos visualizar melhor e tirar melhores conclus√µes. Antes disso, criei tamb√©m uma coluna que tenta ‚Äúsomar‚Äù essas duas m√©tricas. Chamei de SuperTrunfo (faltou criatividade, e eu estava com sono). Essa coluna priorizou os running backs que foram bem nas duas m√©tricas, mas n√£o corrigiu as distor√ß√µes como a do Cristian McCaffrey. Ah, dessa vez retiramos os nomes dos Qbs, pois a inten√ß√£o √© avaliar o trabalho dos running backs. Os Qbs atl√©ticos tem uma vantagem natural para ganhar jardas corridas, pois s√£o um jogador a mais para a defesa se preocupar no jogo terrestre. Muitas vezes acabam conseguindo mis-matches t√°ticos e arrumando jardas com facilidade. Bom, se voc√™ est√° lendo at√© aqui, provavelmente j√° sabe disso.rank3 <- corridas %>% filter(yards_gained >= 9) %>% group_by(rusher_player_id) %>% mutate(EpaMedio = mean(epa, na.rm = TRUE), TotalEpa = sum(epa, na.rm = TRUE), Time = posteam, TDs = sum(rush_touchdown), ""Jardas Por Carregada""=mean(yards_gained), Corridas=n(), SuperTrunfo = (sum(BPS) / sum(CL)) + (EpaMedio / 10), OpenFieldRatio = sum(BPS) / sum(CL), TotalBPS = sum(BPS), TotalCL = sum(CL)) %>% select (Time, rusher_player_name, SuperTrunfo, OpenFieldRatio, TotalBPS, TotalCL, EpaMedio, TotalEpa, Corridas,  TDs) %>% distinct %>% arrange(desc(OpenFieldRatio),  na.rm = TRUE) %>% filter(Corridas >= 20, !(rusher_player_name %in% c(""K.Murray"", ""M.Ingram"", ""L.Jackson"", ""D.Watson"", ""J.Allen"", ""Jos.Allen"")), !(rusher_player_name == ""K.Drake"" & Time == ""MIA""))rank3Considerando as duas m√©tricas. P√°gina 1.Considerando as duas m√©tricas. P√°gina 2.Para plotar o gr√°fico:ggplot(rank3, aes(x=EpaMedio, y=OpenFieldRatio)) +   geom_text(aes(label=rusher_player_name), size= 3) +  labs(x = ""EPA M√©dio (corridas acima de 8 jardas)"",  y = ""Open Field Ratio"",  caption = ""Dados de nflscrapR (Feito por Henrique Cardoso @5adescida)"",  title = ""Rbs nas jogadas acima de 8 jardas"",  subtitle = ""2019"")  ggsave('GraficoRBs.png', dpi=3000)Quais as maiores surpresas? Podemos ver alguns jogadores como Gus Edwards em boa posi√ß√£o nas duas m√©tricas! Eu n√£o esperava. Vou olhar com aten√ß√£o para a temporada do Baltimore Ravens esse ano. A excelente posi√ß√£o do Miles Sanders reflete bem o qu√£o subestimado ele realmente √©.No entando, √© estranho ver um excelente jogador como Ezequiel Elliott sendo o pior running back do grupo. Zec √© elite. Quebra tackles e luta por jardas adicionais com muita facilidade. Olhando os n√∫meros, ele foi o jogador que mais conseguiu ter ganhos de pelo menos 9 jardas: 49! Dessas 49 vezes, apenas 5 ele conseguiu um TD ou correu pra mais de 20 jardas. A conclus√£o que eu chego √©: Zec √© elite, mas talvez n√£o nessas big plays. Ele √© muito elite correndo as primeiras jardas das corridas (aquelas que essas m√©tricas simples jamais ir√£o conseguir mensurar). De todos os running backs da liga, ele foi o que mais teve corridas de pelo menos 9 jardas.A conclus√£o que podemos chegar √© que os n√∫meros mostram muita coisa, mas √© preciso contextualizar e entender o que houve. √â preciso entender como s√£o calculadas as m√©tricas, entender quais s√£o os seus pontos cegos, quais os fundamentos mais importantes do esporte e quais podemos mensurar com os dados.Running backs importam sim! Possuem caracteristicas muito diferentes, e s√£o qualitativamente diferentes. Essas m√©tricas que eu produzi est√£o longe de decretar qualquer veredicto ou de serem verdades absolutas, mas j√° s√£o bem melhores do que simplesmente considerar o n√∫mero total de jardas (o que acaba sendo um stat completamente enviezado) ou o pr√≥prio EPA M√©dio, mas considerando todas as jogadas, inclusive as que resultaram em poucas jardas.Se voc√™ quiser aprender mais sobre o nflscrapR, e como realizar esses estudos, esse tutorial √© √≥timo. Se quiser aprender mais sobre o que fazer, e principalmente o que N√ÉO fazer com stats de futebol americano, o NoFlags tem tr√™s artigos excelentes sobre o tema (1, 2 e 3).Voc√™ consegue baixar todos os arquivos que precisa para reproduzir esse estudo no meu GitHub.Seria incr√≠vel debater sobre essas m√©tricas! Discorde de mim mandando uma mensagem no meu Linkedin. Se existir uma forma melhor de calcular tudo isso, eu adoraria saber.Continue em casa, use m√°scara e sempre tenha √°lcool em gel.Forever Faithfull.Written byHenrique CardosoEstudante de Machine Learning e Intelig√™ncia ArtificialFollow1 1 1¬†1¬†1 Data ScienceRNFLFootballStatisticsMore from Henrique CardosoFollowEstudante de Machine Learning e Intelig√™ncia ArtificialMore From MediumThe Military Is Back in BrazilForeign Policy in Foreign PolicyLewagon‚Ää‚Äî‚ÄäLearning how to code in Rio de JaneiroStuart BoyleWhat Does a Developer Look Like?Allison Chow in Code Like A GirlLab 2‚Äî Javascript IntroductionChristian Grewell in applab 2.04 Habits That Make You an Inefficient DeveloperDaan in Better ProgrammingVanilla Neural Networks in RChris Mahoney in Towards Data ScienceFast transition between dplyr and data.tableNata Berishvili in Towards Data ScienceFake News Classification with Recurrent Convolutional Neural NetworksAmol Mavuduru in Towards Data ScienceAboutHelpLegalGet the Medium app"
Build Stock Price Database With R Website Crawling & MYSQL,https://medium.com/data-room/build-stock-price-database-with-r-website-crawling-mysql-2948dec80c5a?source=tag_archive---------1-----------------------,"Stocks,Database,Data Analytics,Crawling,R","Why build my own stock price database?Limitations of the environmentCompared with American stock price research, there are no R packages to get Taiwanese stock price data. If you want to do research without many open data sources, this article might provide you an idea.On the other hand, we query the stock price data from the TWSE website (Taiwan Stock Exchange Corporation) which treats website crawling strictly. Collecting historical data on local is necessary because few requests per minute and per day are allowed.2. The historical stock price for backtestingBacktesting is the most important thing for quantitative financial analysis.Backtesting requires lots of historical data to test your model performances and make further predictions. The database is the best choice to store and sort the huge amount of data.3. Combine with other data to do deep analysisMost investors analyze stock prices in three perspectives, technical, fundamental, and chip analysis respectively.Multiple data sources can optimize the model performances. Doing fundamental analysis requires monthly revenue, but chip analysis demands the institutional investors‚Äô trading records. The database can store multiple data sources at the same place.The structure of my databaseThe current database contains six tables.Stocks_Categories: Including stock Id and their industries categories.History_Prices: Data from Yahoo Finance API. Including open, high, low, close (OHLC), adjusted close prices, and volume.Monthly_Revenue: Including the monthly revenue of each stock.History_Prices_TWSE: The OHLC, last offered prices(which didn‚Äôt make a deal), and the amounts of deals.Institutional_Investor_Trading_TWSE: The institutional investors‚Äô trading records of each stockHistory_Index_TWSE: Including Taiwanese industrial indexes, and indexes of top tier Taiwanese companies‚Äô performances which share similar concepts with S&P 500.There are two stock prices from Yahoo Finance API and TWSE site respectively. Although there is huge homogeneity between two data sources, they are complementary to each other. It‚Äô difficult to get the whole historical prices from the TWSE site, due to the restricted limit of requests. On the other hand, yahoo finance lacks the price data of some stocks, but the TWSE site has all stocks‚Äô today prices.There are rooms for improvements for this database. I kept almost all the columns from the original sources, but some calculated metrics are redundant in the database. Besides, adding more different data about stocks is important to consummate models.Using R crawls the data & writes into the databaseData Roomdata analysisFollowStocksDatabaseData AnalyticsCrawlingRWritten bylalaLyn TheaterFollow‰ªãÁ¥π‰∏ñÁïåÂêÑÂú∞ÁöÑËàûÂè∞ÂäáËàáÈõªÂΩ±„ÄÇÊúÄËøë‰∏ªÈ°åÁÇ∫Ê≠åËàû‰ºé„ÄÇFollowData RoomFollowLearning on data and sharing my projectsFollowWritten bylalaLyn TheaterFollow‰ªãÁ¥π‰∏ñÁïåÂêÑÂú∞ÁöÑËàûÂè∞ÂäáËàáÈõªÂΩ±„ÄÇÊúÄËøë‰∏ªÈ°åÁÇ∫Ê≠åËàû‰ºé„ÄÇData RoomFollowLearning on data and sharing my projectsMore From MediumLevels of MeasurementsRiteshpratap A. Singh in The StartupHow people talk about marijuana on Reddit: a natural language analysisSara Robinson in HackerNoon.comSave a Neural Net, Use a Linear ModelODSC - Open Data Science in PredictCheck out My MixtapeHarmit Sampat in VisUMDMinimally Sufficient PandasTed Petrou in Dunder DataWish your team paid more attention to data?Atlantic 57 in Insights from Atlantic 57How to Manage Big Data With 5 Python LibrariesSeattleDataGuy in Better ProgrammingMismatch Between Academic and Real-world Data Science ProjectsBenjamin Obi Tayo Ph.D. in Towards AILearn more.Medium is an open platform where 170 million readers come to find insightful and dynamic thinking."
Creating Visualization with R ggplot2,https://medium.com/@putik.dhira/creating-visualization-with-r-ggplot2-f6584b400774?source=tag_archive---------2-----------------------,"R,Ggplot,Ggplot2,Charts,Visualization","Photo by Markus Winkler on UnsplashIn the previous article about automating your report with R, you can add some charts on your report to make even more comprehensive visualization. This article will give some examples on how to use the ggplot2 package to create some useful charts. First thing first, you need to load ggplot2 package to your workspace. Similar to previous articles, we will use the financial sample dataset.Library(ggplot2)Library(dplyr)Library(scales)Line Chart / Time SeriesFirst, we need to summarize the data to become a daily data view.sales_day <- financial_sample %>% group_by(Date) %>% summarise(daily_sales = sum(Sales))Then, we can use below script to plot the line chart.theme_set(theme_classic())ggplot(sales_day, aes(x=Date)) + geom_line(aes(y=daily_sales)) + labs(title=‚ÄùDaily Sales Chart‚Äù, subtitle=‚ÄùTotal of Daily Sales‚Äù, caption=‚ÄùSource: Financial Sample‚Äù, y=‚ÄùSales‚Äù) + scale_y_continuous(name=‚ÄùSales‚Äù, labels = comma)2. Bar ChartFirst, we need to summarize the data to become a daily data view per product.sales_day_p <- financial_sample %>% group_by(Date, Product) %>% summarise(daily_sales = sum(Sales))Then, we can use below script to plot the stacked bar chart.product_plot <- ggplot(sales_day_p, aes(Date,Product))product_plot + geom_bar(aes(fill=Product,x = Date, y = daily_sales), stat=‚Äùidentity‚Äù, width = 20) + labs(title=‚ÄùFinancial Sample‚Äù, subtitle=‚ÄùDaily Sales of Products‚Äù, caption=‚ÄùSource: Financial Sample‚Äù) + theme(axis.text.x = element_text(angle=65, vjust=0.6)) + scale_y_continuous(name=‚ÄùSales‚Äù, labels = comma)3. Pie ChartWhen we try to visualize a categorical product share, a pie chart is one of the best ways to visualize it. First, we need to summarize the data to total profit per product, and add a column in order to put them as label.profit_product <- financial_sample %>% group_by(Product) %>% summarise(total_profit = sum(Profit))profit_product$Label <-paste(round(profit_product$total_profit/1000000,2),‚ÄùM‚Äù)Then, we can use below script to plot the pie chart.p <- ggplot(profit_product, aes(x = 1, y = total_profit, fill = Product)) + geom_bar(stat = ‚Äúidentity‚Äù)p + coord_polar(theta = ‚Äòy‚Äô) + theme_void() + geom_text(aes(label = Label), position = position_stack(vjust = 0.5)) + labs(fill=‚ÄùProduct‚Äù,title=‚ÄùPie Chart of Profit by Product‚Äù,caption=‚ÄùSource: Financial Sample‚Äù)Written byPutik DhiraramantiBusiness Intelligence ‚Äî Product ManagementFollowRGgplotGgplot2ChartsVisualizationMore from Putik DhiraramantiFollowBusiness Intelligence ‚Äî Product ManagementMore From MediumHow to use Iterrows in PandasHamilton Chang in Python In Plain EnglishHEARTCOUNT Quick GuidelineSidney @HEARTCOUNT in HEARTCOUNTPrinciples of PlotlyJenny Dcruz in The StartupAutomated e-Learning Content Creation with Web Scraping and NLPErdem IsbilenThe Data Visualization PolarityMike Raper in NightingaleYour Microsoft Excel Skills Need WorkPendora in High FinanceHow Did I Get Started With Machine Learning?Moeedlodhi in The StartupFast.ai Practical Data Ethics lesson 5.1 notes-The problem with metricsRisto Hinno in The InnovationAboutHelpLegalGet the Medium app"
Predicting the Nominal GDP using Economic Indicators: A Data Science Approach,https://towardsdatascience.com/predicting-the-nominal-gdp-using-economic-indicators-a-data-science-approach-7c56cded782?source=tag_archive---------0-----------------------,"Data Science,Economics,Machine Learning,Artificial Intelligence,R","Photo by Kevin Ku on UnsplashThis article talks about forecasting nominal GDP (Gross Domestic Product) using data present over the web. The problem statement was to gather data from authentic sources, perform an Exploratory Data Analysis (EDA), train a model and predict the Nominal GDP (Canada). As an IT person, I had little knowledge about these economics-associated terms. That is why the first thing to do was to get familiar with these concepts. Next was to learn and execute this project in R programming language and get acquainted with popular R packages such as tidyverse, ggplot, caret and others. In practice, good use of online resources makes it easy to get accustomed to R and its syntax.Stepping into the task, I came across various economic statistics aka indicators, classified as lagging and leading indicators. Economic indicators are something which may have a direct impact on the Nominal GDP of a country. Our approach here was to use these indicators to predict the Nominal GDP of Canada. After researching these indicators from various online blogs, articles and papers, the following indicators were finalized:PopulationRefugee PopulationReal Interest RateNumber of Domestic CompaniesTravel Services ( % of Import Services -BoP)Tax RevenueHousing MarketLabor ProductivityGovernment Bond Yield 10yr ratePersonal Remittances (Received & Paid in USD)Passengers carried by RailwaysPassengers carried by Air TransportsInflationIncome GrowthUnemployment RateGovernment DeficitConsumer Price IndexCommodity Price IndexEUR to CAD conversion rateUSD to CAD conversion rateToronto Stock Exchange traded valueToronto Stock Exchange traded volumeIncoming International TouristsData was fetched from sources like Statistics Canada, World Bank, Statista and others. These indicators were assumed as features and data was collected for these features between from year 2009 to 2018. The data set was divided into two parts, a traning set and a testing set.Training Set ‚Äî 2009 to 2016esting Set ‚Äî 2017 and 2018Exploratory Data AnalysisAfter the dataset was assembled, cleaned and neatly arranged, exploratory data analysis for the data set was started. Data normalization was done using the probability density dnorm function for a standard normal distribution. To check whether the data is normal, Shapiro-Wilk Normality Test was applied on each feature. All variables except 3 were having their p-value greater than 0.05, which indicated that they are normal. In order to analyze the correlation between the dataset features, findCorrelation function of caret package was utilized and highly correlated attributes were removed. As the data points were less i.e. 8 (2009‚Äì2016), the dimensionality of the data had to be reduced. This was done by applying Principal Component Analysis (PCA) using prcomp function. At this point, EDA was concluded. Let's proceed with modelling now! Refer the GitHub link‚Ä¶Click here!Linear Regression ModellingIn this type of modelling, PCA values were used to train the simple linear regression model using the lm() function. The resultant model was summarized using the summary function. The key observation was that the p-value was less than the significance level (< 0.05). This means that I could safely reject the null hypothesis as co-efficient Œ≤ of the predictor is zero. Furthermore, the F-statistic value of the model was 428.6 (more the better). Hence, it was concluded that the model was statistically significant. To validate the model, I tested it on the values of 2017 and 2018. The overall accuracy of the model was 98.31%. A table at the end of the article provides more details about the experiments.Random Forest ModellingFor this modelling, randomForest library of R language makes it easier to select the features and create a random forest model. The importance() function finds and displays the importance of each feature according to the data leaf node impurity. These were stored in a new dataset which was the sorted on basis of the importance values. The trick here is not to select the features whose importance values are only higher or lower, but to select values to neutralize the spectrum i.e. combination of higher, lower and mid importance values to maintain the balance. Six features were picked using trial and error method.Personal Remittances (Received) Real Interest Rate Travel Services Imports  Government Bond Yield 10yr rateGovernment DeficitNumber of Domestic CompaniesThe model was trained by using the randomForest function. mtry parameter was set to 6 (no of features) and number of trees (ntree) to 1000. The model forecasted the 2017 and 2019 NGDP with an average accuracy of 95.68%.Support Vector Machine ModellingThis was the final one on our experiment list. In order to utilize the SVM modelling in our use, we used the e1071 package of R that provides the svn function to model. As our use-case was not a classification one, the regression option had to be applied. For this, the svm() function provides a type parameter which enables us to opt in our choice. Accordingly, the eps-regression type was opted and kernel parameter was set to radial. For more details on kernels, click here. The average accuracy forecasted here was 98.18%. The graph and table below provide more details about the experiment.Chart displaying the actual vs predicted valuesTo conclude, despite having a small data set, all models predict with 95% + accuracy. As the linear model is trained using the PCA values, the resulting output is better than other. Whereas, the Random Forest can be considered to be least reliable as it uses minimal features of the dataset. If not the accuracy or reliability, the results at least advocate the significance of economic indicators in the prediction of Nominal GDP.PS: This experiment was a part of the submission for Statistics Canada Business Data Scientist Challenge 2019/2020.Written byDamian Diago D‚ÄômonteMaster's in Computer Science - University of New BrunswickFollowSign up for The Daily PickBy Towards Data ScienceHands-on real-world examples, research,  tutorials, and cutting-edge techniques delivered Monday to Thursday. Make learning your daily ritual.¬†Take a lookGet this newsletterBy signing up, you will create a Medium account if you don‚Äôt already have one. Review our Privacy Policy for more information about our privacy practices.Check your inboxMedium sent you an email at  to complete your subscription.Data ScienceEconomicsMachine LearningArtificial IntelligenceRMore from Towards Data ScienceFollowA Medium publication sharing concepts, ideas, and codes.Read more from Towards Data ScienceMore From Medium5 YouTubers Data Scientists And ML Engineers Should Subscribe ToRichmond Alake in Towards Data Science7 Must-Haves in your Data Science CVElad Cohen in Towards Data Science21 amazing Youtube channels for you to learn AI, Machine Learning, and Data Science for freeJair Ribeiro in Towards Data Science30 Examples to Master PandasSoner Yƒ±ldƒ±rƒ±m in Towards Data ScienceThe Roadmap of Mathematics for Deep LearningTivadar Danka in Towards Data ScienceAn Ultimate Cheat Sheet for Data Visualization in PandasRashida Nasrin Sucky in Towards Data ScienceHow to Get Into Data Science Without a DegreeTerence S in Towards Data Science4 Types of Projects You Must Have in Your Data Science PortfolioSara A. Metwalli in Towards Data ScienceAboutHelpLegalGet the Medium app"
Merubah Tema di RStudio,https://medium.com/@sumadireja/merubah-tema-di-rstudio-dca8acf1435?source=tag_archive---------1-----------------------,"Rstudio,R Shiny,R,2020,Indonesia","Assalamualaikum..Hallo temen-temen semua, apa kabar nih kalian semua? Saya harap di situasi sekarang ini temen-temen selalu dalam keadaan baik,ya! Tetap jaga kesehatan. Jika harus keluar rumah tetap gunakan masker, ya, jangan lupa untuk selalu cuci tangan juga setelah berpergian.Kali ini saya akan membuat tutorial sederhana tentang bagaimana merubah tema pada R Studio.Source From rstudio(dot)comRstudio itu sendiri merupakan Integrated Development Environment (IDE) open souce dan gratis untuk R, bahasa pemrograman untuk komputasi statistik dan grafik. Rstudio didirikan oleh JJ Allaire, pencipta bahasa pemrograman ColdFusion.Rstudio tersedia dalam dua versi: Rstudio desktop, dimana program dapat dijalankan secara lokal sebagai aplikasi desktop biasa; dan Rstudio server, yang memungkinkan mengakses Rstudio menggunakan browser web saat sedang berjalan di server Linux jarak jauh.Rstudio tersedia dalam edisi open source dan komersial serta dapat berjalan pada OS (Windows, macOS, dan Linux) atau di browser yang terhubung ke Rstudio Server atau Rstudio Server Pro. Rstudio sebagian ditulis dalam bahasa pemrograman C++.Kali ini saya akan menjelaskan Rstudio dengan versi open source yang berjalan pada OS Windows 10. Berikut tutorialnya..Secara default Rstudio memiliki tema dengan cenderung berwarna putih, bagi sebagain orang termasuk saya kurang suka dengan tampilan yang cenderung berwarna putih, karena tidak begitu nyaman untuk mata. Maka dari itu saya merubahnya menjadi warna yang enak untuk dipandang dan tidak membuat perih mata. Hal pertama yang perlu dilakukan ialah buka Rstudio kamu. Setelah itu klik bagian Tools > Global Options‚Ä¶Setelah itu klik bagian Appearance dan pilih Editor theme. Disana banyak pilihan tema yang dapat kamu gunakan. Namun kali ini saya ingin menggunakan tema night owl. Untuk caranya kita pergi ke link berikut ini.Setelah itu download yang bagian night-owlish.rstheme. Jika sudah balik lagi pada Rstudio. Lalu, klik button add dan pilih file night-ownlish yang telah kamu download tadi.Jika sudah langkah terakhir tinggal pilih apply > ok. Dan tadaaa sudah terganti temanya menjadi yang lebih nyaman dipandang.Cukup sekian, terimakasih temen-temen sudah mampir. Semoga lebih semangat ya ngoding menggunakan Rstudionya.üòäJika menurutmu ini bermanfaat tolong share ke temen-temen kamu juga, ya. Semoga membantu!Written byImron SumadirejaHallo! Terimakasih sudah mampir.üòäFollow2 2¬†2¬†RstudioR ShinyR2020IndonesiaMore from Imron SumadirejaFollowHallo! Terimakasih sudah mampir.üòäMore From MediumExploring Undernourishment: Part 6‚Ää‚Äî‚ÄäResearch Area 3: Surprising TrendsChris Mahoney in The StartupExploring Undernourishment: Part 7‚Ää‚Äî‚ÄäResearch Area 4: Most Influential IndicatorChris Mahoney in The StartupFast transition between dplyr and data.tableNata Berishvili in Towards Data SciencePowerBI vs. R Shiny: Two Popular Excel Alternatives ComparedDario Radeƒçiƒá in Towards Data ScienceThere is an R in ReproducibilityDr Andreas Ochs in Towards Data ScienceWeb Scraping With R‚Ää‚Äî‚ÄäEasier Than PythonDario Radeƒçiƒá in Towards Data ScienceInterpret Regression Analysis Results using R: Biomedical DataSwayanshu Shanti Pragnya in Analytics VidhyaTableau vs. R Shiny: Which Excel Alternative Is Right For You?Dario Radeƒçiƒá in Towards Data ScienceAboutHelpLegalGet the Medium app"
N/A,https://medium.com/psicodata/saia-do-spss-e-venha-para-o-r-7cdfdeac8a24?source=tag_archive---------2-----------------------,"R,Newsletter,Psicodata","No m√™s de agosto a linguagem R dominou o PsicoData!Voc√™ perdeu alguma coisa?! N√£o fique triste, eu vou resumir para voc√™ todos os conte√∫dos:Se voc√™ n√£o sabe por onde come√ßar, respira e vem conferir comigo esse post sobre como iniciar no R!Se voc√™ √© daqueles mais adiantados e quer aprender a manipular dados com R, tamb√©m temos conte√∫dos para voc√™:Transformando colunas em linhas e outras loucuras com tidyrValores missing ‚Äî Parte 1Valores missing ‚Äî Parte 2Ganhe liberdade aprendendo a ler fun√ß√µesOk, voc√™ j√° sabe muitas coisas que d√° para fazer com o R, vamos tentar algum mais avan√ßado? Que tal corrigirmos uma escala psicom√©trica e obter insights incr√≠veis sobre os seus dados? üòçPreparamos um tutorial de como analisar uma escala psicom√©trica utilizando o R, clique aqui.Esse m√™s de agosto foi incr√≠vel! E o que ser√° que vem por ai no PsicoData? üëÄConvide seus amigos a assinar a nossa newsletter pelo link abaixo:PsicoDatapsicodataFollow3 Sign up for PsicoData NewsletterBy PsicoDataFique por dentro das nossas publica√ß√µes!¬†Take a lookGet this newsletterBy signing up, you will create a Medium account if you don‚Äôt already have one. Review our Privacy Policy for more information about our privacy practices.Check your inboxMedium sent you an email at  to complete your subscription.Thanks to Gabriel Rodrigues.¬†RNewsletterPsicodata3¬†claps3¬†clapsWritten byDalton CostaFollowEstudante de psicologia e ci√™ncia de dadosFollowPsicoDataFollowUma plataforma colaborativa em portugu√™s sobre ci√™ncia de dados e psicologia.FollowWritten byDalton CostaFollowEstudante de psicologia e ci√™ncia de dadosPsicoDataFollowUma plataforma colaborativa em portugu√™s sobre ci√™ncia de dados e psicologia.More From MediumT√° tudo junto: m√©dia, vari√¢ncia e desvio-padr√£oGabriel Rodrigues in PsicoDataDados e Psico: 5 li√ß√µes que eu gostaria de ter ouvido no in√≠cioPaula Costa in PsicoDataO que √© uma vari√°vel latente e como medi-la?‚Ää‚Äî‚ÄäParte 2Gabriel Rodrigues in PsicoDataSimples e Direto: um guia de visualiza√ß√£o de dados com PythonDalton Costa in PsicoDataBaixando e processando dados do DATASUS sobre suic√≠dio com PythonDalton Costa in PsicoDataO que √© uma vari√°vel latente e como medi-la?‚Ää‚Äî‚ÄäParte 1Gabriel Rodrigues in PsicoDataUm gr√°fico vale mais que mil palavras!Marcela Alves Sanseverino in PsicoDataValores missing‚Ää‚Äî‚ÄäParte 2Gabriel Rodrigues in PsicoDataLearn more.Medium is an open platform where 170 million readers come to find insightful and dynamic thinking."
N/A,https://medium.com/nuances-of-programming/%D0%BF%D1%80%D0%B8%D0%BA%D0%BB%D1%8E%D1%87%D0%B5%D0%BD%D0%B8%D1%8F-%D0%B0%D0%BD%D0%B0%D0%BB%D0%B8%D1%82%D0%B8%D0%BA%D0%B0-%D0%B2-%D1%81%D1%82%D1%80%D0%B0%D0%BD%D0%B5-%D0%BA%D0%BE%D0%B4%D0%B0-%D0%BF%D1%80%D0%BE%D0%B1%D1%83%D0%B6%D0%B4%D0%B5%D0%BD%D0%B8%D0%B5-%D1%81%D0%B8%D0%BB%D1%8B-415488c5ce04?source=tag_archive---------0-----------------------,"Learn To Code,R,Sql,Spreadsheets,Nuances Of Programming","–ü—Ä–æ–≥—Ä–∞–º–º—ã –¥–ª—è —ç–ª–µ–∫—Ç—Ä–æ–Ω–Ω—ã—Ö —Ç–∞–±–ª–∏—Ü, —Ç–∞–∫–∏–µ –∫–∞–∫ Microsoft Excel –∏ Google Sheets, –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–Ω—ã. –û–Ω–∏ –≤–Ω–µ –∫–æ–Ω–∫—É—Ä–µ–Ω—Ü–∏–∏, –∫–æ–≥–¥–∞ –¥–µ–ª–æ –∫–∞—Å–∞–µ—Ç—Å—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –ø—Ä–æ—Å—Ç—ã—Ö –≤—ã—á–∏—Å–ª–µ–Ω–∏–π –∏–ª–∏ –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏—è —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π.–û–¥–Ω–∞–∫–æ –Ω–∞—Å—Ç—É–ø–∞–µ—Ç –º–æ–º–µ–Ω—Ç, –∫–æ–≥–¥–∞ –≤—ã –¥–æ–ª–∂–Ω—ã –æ—Ç–∫–∞–∑–∞—Ç—å—Å—è –æ—Ç —ç–ª–µ–∫—Ç—Ä–æ–Ω–Ω—ã—Ö —Ç–∞–±–ª–∏—Ü –∏ –Ω–∞–π—Ç–∏ –¥—Ä—É–≥–æ–µ —Ä–µ—à–µ–Ω–∏–µ. –í—ã –Ω–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç–µ, —Å–∫–æ–ª—å–∫–æ —Ä–∞–∑–Ω—ã—Ö –æ—Ç–≤—Ä–∞—Ç–∏—Ç–µ–ª—å–Ω—ã—Ö –æ—Ç—á–µ—Ç–æ–≤ –ø–æ —ç–ª–µ–∫—Ç—Ä–æ–Ω–Ω—ã–º —Ç–∞–±–ª–∏—Ü–∞–º –ø–æ–ø–∞–¥–∞–ª–æ—Å—å –º–Ω–µ –≤ —Å–≤–æ–µ –≤—Ä–µ–º—è! –ö–∞–∫ —Ç–æ–ª—å–∫–æ –≤—ã –Ω–∞—á–∏–Ω–∞–µ—Ç–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —Ç–∞–±–ª–∏—Ü—É –≤ –∫–∞—á–µ—Å—Ç–≤–µ –æ—Ç—á–µ—Ç–∞, –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö –∏ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞ –¥–ª—è –∏—Ö –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è, —ç—Ç–æ —Å–∏–≥–Ω–∞–ª ‚Äî –Ω–∞–¥–æ –æ—Å—Ç–∞–Ω–æ–≤–∏—Ç—å—Å—è! –í—ã –∑–∞—à–ª–∏ —Å–ª–∏—à–∫–æ–º –¥–∞–ª–µ–∫–æ. –¢–∞–∫–∏–µ —ç–ª–µ–∫—Ç—Ä–æ–Ω–Ω—ã–µ —Ç–∞–±–ª–∏—Ü—ã –Ω–µ —á—Ç–æ –∏–Ω–æ–µ, –∫–∞–∫ —Å—Ç—Ä–∞—à–Ω—ã–π —Å–æ–Ω –¥–ª—è –æ–±—Å–ª—É–∂–∏–≤–∞–Ω–∏—è –∏ –æ—Ç–ª–∞–¥–∫–∏.–ê –µ—Å—Ç—å –ª–∏ –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–∞? –ö–æ–Ω–µ—á–Ω–æ ‚Äî –Ω–∞—É—á–∏—Ç—å—Å—è –ø–∏—Å–∞—Ç—å –∫–æ–¥ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –¥–∞–Ω–Ω—ã—Ö. –ù–µ—Ç –Ω–∏–∫–∞–∫–∏—Ö –ø—Ä–∏—á–∏–Ω –±–æ—è—Ç—å—Å—è –∫–æ–¥–∞. –ï—Å–ª–∏ –≤—ã —É–∂–µ –≤ –∫—É—Ä—Å–µ, –∫–∞–∫ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ø—Ä–æ–≥—Ä–∞–º–º—É –¥–ª—è —Ç–∞–±–ª–∏—Ü—ã, —Ç–æ –Ω–µ —Å–æ—Å—Ç–∞–≤–∏—Ç —Ç—Ä—É–¥–∞ –ø—Ä–æ–≤–µ—Å—Ç–∏ –∞–Ω–∞–ª–æ–≥–∏–∏ –∏ –ø–æ–Ω—è—Ç—å —Å—É—Ç—å —ç—Ç–æ–≥–æ –ø—Ä–æ—Ü–µ—Å—Å–∞. –Ø —É–∂–µ –∫—É—Ä–∏—Ä–æ–≤–∞–ª –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –∞–Ω–∞–ª–∏—Ç–∏–∫–æ–≤ –Ω–∞ –Ω–∞—á–∞–ª—å–Ω—ã—Ö —ç—Ç–∞–ø–∞—Ö –∏—Ö —Ä–∞–∑—Ä–∞–±–æ—Ç–æ–∫ –∏ —É–±–µ–¥–∏–ª—Å—è, —á—Ç–æ –ø–æ–¥—Ö–æ–¥ –∏–∑ —ç—Ç–æ–π —Å—Ç–∞—Ç—å–∏ —Ä–∞–±–æ—Ç–∞–µ—Ç —Å—Ç–∞–±–∏–ª—å–Ω–æ. –ù–∞—á–Ω–µ–º!–ü—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–µ —É—Å–ª–æ–≤–∏–µ ‚Äî –∏–∑—É—á–∏—Ç—å —ç–ª–µ–∫—Ç—Ä–æ–Ω–Ω—ã–µ —Ç–∞–±–ª–∏—Ü—ã –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –¥–∞–Ω–Ω—ã—Ö–ï—Å–ª–∏ –≤—ã –µ—â–µ –Ω–µ –∑–Ω–∞–µ—Ç–µ, –∫–∞–∫ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ø—Ä–æ–≥—Ä–∞–º–º—É –¥–ª—è —ç–ª–µ–∫—Ç—Ä–æ–Ω–Ω—ã—Ö —Ç–∞–±–ª–∏—Ü, —Ç–æ —ç—Ç–æ –Ω–∞–¥–æ –∏—Å–ø—Ä–∞–≤–∏—Ç—å. –ò–º–µ–Ω–Ω–æ –Ω–∞ –¥–∞–Ω–Ω–æ–º —ç—Ç–∞–ø–µ –≤—ã –ø–æ–∑–Ω–∞–∫–æ–º–∏—Ç–µ—Å—å —Å –ø–æ–Ω—è—Ç–∏—è–º–∏, —Å–≤—è–∑–∞–Ω–Ω—ã–º–∏ —Å –¥–∞–Ω–Ω—ã–º–∏, –∏ –±–µ–∑–±–æ–ª–µ–∑–Ω–µ–Ω–Ω–æ –≤–æ–ª—å–µ—Ç–µ—Å—å –≤ –ø—Ä–æ—Ü–µ—Å—Å –Ω–∞–ø–∏—Å–∞–Ω–∏—è –∫–æ–¥–∞.–°—Ç–∞—Ä—Ç —Å –Ω—É–ª–µ–≤—ã–º –æ–ø—ã—Ç–æ–º–ï—Å–ª–∏ —É –≤–∞—Å –Ω—É–ª–µ–≤–æ–π –æ–ø—ã—Ç –≤ —ç—Ç–æ–π –æ–±–ª–∞—Å—Ç–∏, —Ç–æ —Å–æ–≤–µ—Ç—É—é –ø—Ä–æ–π—Ç–∏ –±–µ—Å–ø–ª–∞—Ç–Ω—ã–π –∫—É—Ä—Å –ø–æ —ç–ª–µ–∫—Ç—Ä–æ–Ω–Ω—ã–º —Ç–∞–±–ª–∏—Ü–∞–º –≤ YouTube. –£—á–∏—Ç—ã–≤–∞—è –∏—Ö —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ, –ø—Ä–æ—Å—Ç–æ –≤—ã–±–µ—Ä–∏—Ç–µ –æ–¥–∏–Ω –∏ –ø–æ—Å–º–æ—Ç—Ä–∏—Ç–µ –ø–æ–ª–Ω–æ—Å—Ç—å—é.–°—Ç–∞—Ä—Ç —Å –ø–æ–∑–∏—Ü–∏–π –æ–ø—ã—Ç–∞–ï—Å–ª–∏ —É –≤–∞—Å –µ—Å—Ç—å –∫–∞–∫–æ–π-–ª–∏–±–æ –æ–ø—ã—Ç, —Ç–æ–≥–¥–∞ –Ω—É–∂–Ω–æ –ø—Ä–∞–∫—Ç–∏–∫–æ–≤–∞—Ç—å—Å—è –¥–æ —Ç–µ—Ö –ø–æ—Ä, –ø–æ–∫–∞ –Ω–µ –æ–±—Ä–µ—Ç–µ—Ç–µ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å –≤ —Ä–∞–±–æ—Ç–µ —Å —ç–ª–µ–∫—Ç—Ä–æ–Ω–Ω—ã–º–∏ —Ç–∞–±–ª–∏—Ü–∞–º–∏. –í –∏–Ω—Ç–µ—Ä–Ω–µ—Ç–µ –¥–æ—Å—Ç—É–ø–Ω–æ –º–Ω–æ–∂–µ—Å—Ç–≤–æ –Ω–∞–±–æ—Ä–æ–≤ –¥–∞–Ω–Ω—ã—Ö —Å –æ—Ç–∫—Ä—ã—Ç—ã–º –∏—Å—Ö–æ–¥–Ω—ã–º –∫–æ–¥–æ–º. –ü—Ä–∏–¥—É–º–∞–π—Ç–µ 5 –¥–µ–π—Å—Ç–≤–∏–π —Å –¥–∞–Ω–Ω—ã–º–∏ –∏ –≤—ã–ø–æ–ª–Ω–∏—Ç–µ –∏—Ö, –∏—Å–ø–æ–ª—å–∑—É—è —ç—Ç–∏ –Ω–∞–±–æ—Ä—ã. –ü—Ä–∏–º–µ—Ä—ã –≤–æ–∑–º–æ–∂–Ω—ã—Ö –¥–µ–π—Å—Ç–≤–∏–π:‚Äú–ö–∞–∫–æ–µ —Å—Ä–µ–¥–Ω–µ–µ –∑–Ω–∞—á–µ–Ω–∏–µ –≤ —Å—Ç–æ–ª–±—Ü–µ X?‚Äù‚Äú–°–∫–æ–ª—å–∫–æ –ø—É—Å—Ç—ã—Ö –∫–ª–µ—Ç–æ–∫/—Å–æ–¥–µ—Ä–∂–∞—Ç –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è –≤ —Å—Ç–æ–ª–±—Ü–µ X?‚Äù‚Äú–û—Ç—Å–æ—Ä—Ç–∏—Ä—É–π—Ç–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –≤ —Å—Ç–æ–ª–±—Ü–µ X –≤ –ø–æ—Ä—è–¥–∫–µ —É–±—ã–≤–∞–Ω–∏—è‚Äù.‚Äú–°–æ–∑–¥–∞–π—Ç–µ –ª–∏–Ω–µ–π–Ω—É—é –¥–∏–∞–≥—Ä–∞–º–º—É —Å—Ç–æ–ª–±—Ü–∞ X‚Äù.‚Äú–ö–∞–∫–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ —Å—Ç–æ–ª–±—Ü–∞ X –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –∑–Ω–∞—á–µ–Ω–∏—è –≤ —Å—Ç–æ–ª–±—Ü–µ Y?‚Äù–ò–∑—É—á–∞–µ–º—ã–µ –ø–æ–Ω—è—Ç–∏—è–ù–µ–∑–∞–≤–∏—Å–∏–º–æ –æ—Ç –∏–º–µ—é—â–µ–≥–æ—Å—è –æ–ø—ã—Ç–∞, –≤–∞—à–∏ –ø—Ä–∏–∫–ª—é—á–µ–Ω–∏—è –≤ —Å—Ç—Ä–∞–Ω–µ —ç–ª–µ–∫—Ç—Ä–æ–Ω–Ω—ã—Ö —Ç–∞–±–ª–∏—Ü –±—É–¥—É—Ç —Å–æ–ø—Ä–æ–≤–æ–∂–¥–∞—Ç—å—Å—è –≤–∞–∂–Ω—ã–º–∏ –ø–æ–Ω—è—Ç–∏—è–º–∏, –æ—Ç–Ω–æ—Å—è—â–∏–º–∏—Å—è –∫ –ø—Ä–æ—Ü–µ—Å—Å—É –Ω–∞–ø–∏—Å–∞–Ω–∏—è –∫–æ–¥–∞ –¥–ª—è –¥–∞–Ω–Ω—ã—Ö.–§–æ—Ä–º—É–ª—ã –º–æ–≥—É—Ç –ø—Ä–∏–º–µ–Ω—è—Ç—å—Å—è –∫ –æ—Ç–¥–µ–ª—å–Ω—ã–º —è—á–µ–π–∫–∞–º –∏–ª–∏ –∫ –≥—Ä—É–ø–ø–µ —è—á–µ–µ–∫. –≠—Ç–æ –ø–æ–¥–æ–±–Ω–æ –ø–æ–Ω—è—Ç–∏—é ‚Äú–≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏‚Äù, –∫–æ–≥–¥–∞ –º—ã –ø—Ä–∏–º–µ–Ω—è–µ–º –æ–ø–µ—Ä–∞—Ü–∏–∏ –∫–æ –≤—Å–µ–º –º–∞—Å—Å–∏–≤–∞–º –¥–∞–Ω–Ω—ã—Ö.–î–∞–Ω–Ω—ã–µ –º–æ–≥—É—Ç –æ–±–æ–±—â–∞—Ç—å—Å—è –≤ –ø—Ä–æ—Ü–µ—Å—Å–µ –∞–≥—Ä–µ–≥–∏—Ä–æ–≤–∞–Ω–∏—è. –°–≤–æ–¥–Ω—ã–µ —Ç–∞–±–ª–∏—Ü—ã –Ω–∞—É—á–∞—Ç –Ω–∞—Å, —á—Ç–æ –∑–Ω–∞—á–∏—Ç –ø–æ–¥—Å—á–∏—Ç—ã–≤–∞—Ç—å –∏–ª–∏ —Å—É–º–º–∏—Ä–æ–≤–∞—Ç—å —Å—Ç–æ–ª–±—Ü—ã –ø–æ –≥—Ä—É–ø–ø–∞–º, –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω—ã–º –≤ –¥—Ä—É–≥–∏—Ö —Å—Ç–æ–ª–±—Ü–∞—Ö.–í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏ ‚Äî –º–æ—â–Ω—ã–π –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç. –°–≤–æ–¥–Ω—ã–µ –¥–∏–∞–≥—Ä–∞–º–º—ã –ø–æ–∑–≤–æ–ª—è—é—Ç –≤–∞–º –ø–æ–Ω—è—Ç—å, –∫–∞–∫–∏–µ —Ç–∏–ø—ã –¥–∏–∞–≥—Ä–∞–º–º –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è—é—Ç –∫–∞–∫–∏–µ —Ç–∏–ø—ã –¥–∞–Ω–Ω—ã—Ö.–î–≤–∞ –Ω–∞–±–æ—Ä–∞ –¥–∞–Ω–Ω—ã—Ö –º–æ–≥—É—Ç –±—ã—Ç—å ‚Äú–æ–±—ä–µ–¥–∏–Ω–µ–Ω—ã‚Äù. –≠—Ç–æ –º—ã —É–∑–Ω–∞–µ–º, –∏—Å–ø–æ–ª—å–∑—É—è —Å–∏–ª—É —Ñ–æ—Ä–º—É–ª, —Ç–∞–∫–∏—Ö –∫–∞–∫ ‚ÄòVLOOKUP‚Äô (—Ñ—É–Ω–∫—Ü–∏—è –í–ü–†).–ö–∞–∫ —Ç–æ–ª—å–∫–æ –≤—ã –Ω–∞–±–∏–ª–∏ —Ä—É–∫—É –≤ —Ä–∞–±–æ—Ç–µ —Å –ø—Ä–æ–≥—Ä–∞–º–º–∞–º–∏ –¥–ª—è —ç–ª–µ–∫—Ç—Ä–æ–Ω–Ω—ã—Ö —Ç–∞–±–ª–∏—Ü, –∑–Ω–∞—á–∏—Ç –ø—Ä–∏—à–ª–∞ –ø–æ—Ä–∞ –ø–µ—Ä–µ–º–µ—Å—Ç–∏—Ç—å—Å—è –≤ —Å—Ç—Ä–∞–Ω—É –∫–æ–¥–∞.–û—Ç —ç–ª–µ–∫—Ç—Ä–æ–Ω–Ω–æ–π —Ç–∞–±–ª–∏—Ü—ã –∫ –∫–æ–¥—É–í—ã–±–∏—Ä–∞–µ–º —è–∑—ã–∫ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏—è–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–∏ Microsoft Office –º–æ–≥—É—Ç –ø–æ–¥—É–º–∞—Ç—å:‚Äú–Ø –∞–∫—Ç–∏–≤–Ω–æ –ø–æ–ª—å–∑—É—é—Å—å Excel, –∫ —Ç–æ–º—É –∂–µ —Å–ª—ã—à–∞–ª –ø—Ä–æ ‚ÄòVBA‚Äô. –ì–æ–≤–æ—Ä—è—Ç, –µ–≥–æ –∫–æ–¥ –º–æ–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –æ—Ç—á–µ—Ç–æ–≤ Excel.–ù–∏ –≤ –∫–æ–µ–º —Å–ª—É—á–∞–µ! –ó–∞–±—É–¥—å—Ç–µ –ø—Ä–æ VBA. –ü–æ–≤–µ—Ä—å—Ç–µ –Ω–∞ —Å–ª–æ–≤–æ —Ç–æ–º—É, –∫—Ç–æ –∫–æ–≥–¥–∞-—Ç–æ –Ω–∞–ø–∏—Å–∞–ª –Ω–∞ –Ω–µ–º —Ç–æ–Ω–Ω—ã —Å—Ç—Ä–æ–∫ –∫–æ–¥–∞. –õ—É—á—à–µ –æ—Å–≤–æ–∏—Ç—å ‚Äú–ø–µ—Ä–µ–¥–∞–≤–∞–µ–º—ã–π –Ω–∞–≤—ã–∫‚Äù, —Ç.–µ. –Ω–∞–≤—ã–∫, –∫–æ—Ç–æ—Ä—ã–º –≤—ã —Å–º–æ–∂–µ—Ç–µ –∑–∞–∏–Ω—Ç–µ—Ä–µ—Å–æ–≤–∞—Ç—å —Å–ª–µ–¥—É—é—â–µ–≥–æ —Ä–∞–±–æ—Ç–æ–¥–∞—Ç–µ–ª—è, –Ω–µ–∑–∞–≤–∏—Å–∏–º–æ –æ—Ç —Ç–æ–≥–æ, –ø–æ–ª—å–∑–æ–≤–∞–ª—Å—è –ª–∏ –æ–Ω Microsoft Office –∏–ª–∏ –Ω–µ—Ç.–£—á–∏—Ç—ã–≤–∞—è, —á—Ç–æ —É –≤–∞—Å –Ω–µ—Ç –æ–ø—ã—Ç–∞ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏—è, –ø—Ä–µ–¥–ª–∞–≥–∞—é —Å–æ—Å—Ä–µ–¥–æ—Ç–æ—á–∏—Ç—å—Å—è –Ω–∞ SQL –∏ R.–î–ª—è –∏–∑—É—á–µ–Ω–∏—è SQL –Ω–µ –Ω–∞–¥–æ –±–æ–ª—å—à–æ–≥–æ —É–º–∞. –≠—Ç–æ —à–∏—Ä–æ–∫–æ —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω–µ–Ω–Ω—ã–π —è–∑—ã–∫ —Ä–µ–ª—è—Ü–∏–æ–Ω–Ω–æ–π –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö. –í–æ—Ç –µ–≥–æ –≤—ã –∏ –¥–æ–ª–∂–Ω—ã –≤—ã—É—á–∏—Ç—å.–ê –ø–æ—á–µ–º—É R? –ü–æ—á–µ–º—É –Ω–µ Python? –ü–æ—Ç–æ–º—É —á—Ç–æ –æ–Ω —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–Ω –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –¥–∞–Ω–Ω—ã—Ö! –¢–∞–∫ –∫–∞–∫ —ç—Ç–æ—Ç —è–∑—ã–∫ –ø—Ä–æ–µ–∫—Ç–∏—Ä–æ–≤–∞–ª—Å—è —Å –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ–π —Ü–µ–ª—å—é, –æ–Ω –ø–æ–∑–≤–æ–ª—è–µ—Ç –±–æ–ª—å—à–µ –≤—Ä–µ–º–µ–Ω–∏ —É–¥–µ–ª—è—Ç—å –∏–º–µ–Ω–Ω–æ –∞–Ω–∞–ª–∏–∑—É –¥–∞–Ω–Ω—ã—Ö, –∞ –Ω–µ –±–æ—Ä—å–±–µ —Å –∞–±—Å—Ç—Ä–∞–∫—Ç–Ω—ã–º–∏ –∞—Å–ø–µ–∫—Ç–∞–º–∏ —è–∑—ã–∫–æ–≤ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏—è. –ù–∞–ø—Ä–∏–º–µ—Ä, R –ø–æ—Å—Ç–∞–≤–ª—è–µ—Ç—Å—è —Å–æ –≤—Å—Ç—Ä–æ–µ–Ω–Ω—ã–º–∏ –Ω–∞–±–æ—Ä–∞–º–∏ –¥–∞–Ω–Ω—ã—Ö, –∫–æ—Ç–æ—Ä—ã–µ —Å—Ä–∞–∑—É –º–æ–∂–Ω–æ –∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å. CSV-—Ñ–∞–π–ª—ã –º–æ–≥—É—Ç –±—ã—Ç—å –∏–º–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω—ã –≤ –æ–¥–Ω–æ–π —Å—Ç—Ä–æ–∫–µ –ø—Ä–∏ –ø–æ–º–æ—â–∏ ‚Äòread.csv()‚Äô. –í—ã –º–æ–∂–µ—Ç–µ —Å–æ–∑–¥–∞–≤–∞—Ç—å (–Ω–µ–ø—Ä–∏–≥–ª—è–¥–Ω—ã–µ) –≥–∏—Å—Ç–æ–≥—Ä–∞–º–º—ã, –∏—Å–ø–æ–ª—å–∑—É—è ‚Äòhist()‚Äô, –∞ —Ç–∞–∫–∂–µ –¥–∏–∞–≥—Ä–∞–º–º—ã —Ä–∞—Å—Å–µ—è–Ω–∏—è –∏ –º–Ω–æ–≥–æ–µ –¥—Ä—É–≥–æ–µ –ø—Ä–∏ –ø–æ–º–æ—â–∏ ‚Äòplot()‚Äô. –î–µ–ª–æ –≤ —Ç–æ–º, —á—Ç–æ –≤—Å–µ —ç—Ç–∏ –≤—Å—Ç—Ä–æ–µ–Ω–Ω—ã–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ R –ø–æ–∑–≤–æ–ª—è—é—Ç –ø—Ä–∏—Å—Ç—É–ø–∏—Ç—å –∫ –∞–Ω–∞–ª–∏–∑—É –¥–∞–Ω–Ω—ã—Ö —Å—Ä–∞–∑—É –∂–µ –ø–æ—Å–ª–µ –µ–≥–æ —É—Å—Ç–∞–Ω–æ–≤–∫–∏.–•–æ—á—É, —á—Ç–æ–±—ã –≤—ã –º–µ–Ω—è –ø—Ä–∞–≤–∏–ª—å–Ω–æ –ø–æ–Ω—è–ª–∏. –Ø –Ω–µ —É—Ç–≤–µ—Ä–∂–¥–∞—é, —á—Ç–æ –≤—ã –Ω–µ –¥–æ–ª–∂–Ω—ã —É—á–∏—Ç—å Python. –°–∞–º —è –µ–≥–æ –æ–±–æ–∂–∞—é –∏ –∫–æ–≥–¥–∞-—Ç–æ —Å–¥–µ–ª–∞–ª –≤—ã–±–æ—Ä –≤ –µ–≥–æ –ø–æ–ª—å–∑—É. –ò –≤–∞–º –æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ –Ω–∞–¥–æ –µ–≥–æ –≤—ã—É—á–∏—Ç—å, —Ç–æ–ª—å–∫–æ –Ω–µ —Å–µ–π—á–∞—Å. –Ø —Ä—É–∫–æ–≤–æ–¥—Å—Ç–≤—É—é—Å—å —Ç–µ–º, —á—Ç–æ, –æ—Å–≤–æ–∏–≤ —Å–Ω–∞—á–∞–ª–∞ R, –≤—ã —Å—Ä–∞–∑—É –∂–µ –ø–æ–ª—É—á–∏—Ç–µ —É—Å–ø–µ—à–Ω—ã–π –æ–ø—ã—Ç —Ä–∞–±–æ—Ç—ã —Å –∫–æ–¥–æ–º. –ê —ç—Ç–∏ –º–∞–ª–µ–Ω—å–∫–∏–µ –¥–æ—Å—Ç–∏–∂–µ–Ω–∏—è –±—É–¥—É—Ç –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞—Ç—å –≤ –≤–∞—Å —Ä–µ—à–∏–º–æ—Å—Ç—å, –º–æ–∏ ‚Äú—é–Ω—ã–µ –ø–∞–¥–∞–≤–∞–Ω—ã‚Äù, –æ–≤–ª–∞–¥–µ—Ç—å —Å–≤–µ—Ç–ª–æ–π —Å—Ç–æ—Ä–æ–Ω–æ–π —Å–∏–ª—ã –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º –Ω–∞–ø–∏—Å–∞–Ω–∏–µ –∫–æ–¥–∞ –¥–ª—è –¥–∞–Ω–Ω—ã—Ö.–ü–æ —ç—Ç–∏–º —è–∑—ã–∫–∞–º —Ç–∞–∫ –º–Ω–æ–≥–æ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏. –ù–∞ –∫–∞–∫–∏—Ö –∞—Å–ø–µ–∫—Ç–∞—Ö —Å–æ—Å—Ä–µ–¥–æ—Ç–æ—á–∏—Ç—å—Å—è?–ü–æ–π–¥–µ–º –Ω–∞ –ø–æ–≤–æ–¥—É –Ω–∞—à–µ–≥–æ –ø—Ä–∞–≥–º–∞—Ç–∏–∑–º–∞ –∏ —Å–æ—Å—Ä–µ–¥–æ—Ç–æ—á–∏–º—Å—è –Ω–∞ –∏–∑—É—á–µ–Ω–∏–∏ —Ç–µ—Ö –∞—Å–ø–µ–∫—Ç–æ–≤, –∫–æ—Ç–æ—Ä—ã–µ –ø—Ä–∏–≥–æ–¥—è—Ç—Å—è –≤ —Ä–∞–±–æ—Ç–µ –∞–Ω–∞–ª–∏—Ç–∏–∫–∞. –ê —á—Ç–æ–±—ã –∏—Ö –æ–±–æ–∑–Ω–∞—á–∏—Ç—å, –±—É–¥–µ–º —Å–ª–µ–¥–æ–≤–∞—Ç—å –ø—Ä–æ—Å—Ç–æ–π –ª–æ–≥–∏–∫–µ:1. –°–æ—Å—Ç–∞–≤—å—Ç–µ —Å–ø–∏—Å–æ–∫ –≤—Å–µ—Ö –æ—Ç—á–µ—Ç–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ —ç–ª–µ–∫—Ç—Ä–æ–Ω–Ω—ã—Ö —Ç–∞–±–ª–∏—Ü, –∫–æ—Ç–æ—Ä—ã–µ –≤—ã —Ä–µ–≥—É–ª—è—Ä–Ω–æ –æ–±–Ω–æ–≤–ª—è–µ—Ç–µ. –î–æ–±–∞–≤—å—Ç–µ –∫ –Ω–µ–º—É —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ —Ñ—Ä–∞–≥–º–µ–Ω—Ç—ã –∞–Ω–∞–ª–∏–∑–∞, –≤—ã–ø–æ–ª–Ω–µ–Ω–Ω–æ–≥–æ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ç–∞–±–ª–∏—Ü –∑–∞ –ø–æ—Å–ª–µ–¥–Ω–∏–µ –ø–æ–ª–≥–æ–¥–∞.2. –û—Ç–¥–æ—Ö–Ω–∏—Ç–µ 5 –º–∏–Ω—É—Ç –ø–æ—Å–ª–µ —Ç–∞–∫–æ–π —É—Ç–æ–º–∏—Ç–µ–ª—å–Ω–æ–π —Ä–∞–±–æ—Ç—ã!3. –û—Ç–∫—Ä–æ–π—Ç–µ –æ–¥–Ω—É –∏–∑ —Ç–∞–±–ª–∏—Ü –≤ –≤–∞—à–µ–º —Å–ø–∏—Å–∫–µ.4. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ —Ç–∞–π–º–µ—Ä –Ω–∞ 10 –º–∏–Ω—É—Ç.5. –í–æ –≤—Ç–æ—Ä–æ–º —Å–ø–∏—Å–∫–µ —Å–Ω–∞—á–∞–ª–∞ –æ—Ç–º–µ—Ç—å—Ç–µ —Ñ–æ—Ä–º—É–ª—ã, –∫–æ—Ç–æ—Ä—ã–µ –≤—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–ª–∏ –≤ –æ—Ç—á–µ—Ç–µ, –∑–∞—Ç–µ–º —Å–æ–∑–¥–∞–Ω–Ω—ã–µ –≤–∞–º–∏ –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏, –∞ —Ç–∞–∫–∂–µ —á–∏—Å–ª–∞, –ø–æ–ª—É—á–µ–Ω–Ω—ã–µ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Å–≤–æ–¥–Ω—ã—Ö —Ç–∞–±–ª–∏—Ü. –ú—ã—Å–ª–∏—Ç–µ –∫–∞–∫ –º–æ–∂–Ω–æ —à–∏—Ä–µ! –ù–µ –Ω—É–∂–Ω–æ –∫–æ–Ω–∫—Ä–µ—Ç–∏–∑–∏—Ä–æ–≤–∞—Ç—å, –∫–∞–∫ –≤—ã –≤—ã–ø–æ–ª–Ω—è–ª–∏ ‚ÄòVLOOKUP‚Äô, –∏—Å–ø–æ–ª—å–∑—É—è —Ç–æ—á–Ω—ã–µ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏—è –≤ —Å—Ç–æ–ª–±—Ü–µ B –∏ –≤–æ–∑–≤—Ä–∞—â–∞—è –∑–Ω–∞—á–µ–Ω–∏—è –≤ —Å—Ç–æ–ª–±–µ—Ü F. –ü—Ä–æ—Å—Ç–æ –æ—Ç–º–µ—Ç—å—Ç–µ —Ñ–∞–∫—Ç –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è —ç—Ç–æ–π —Ñ—É–Ω–∫—Ü–∏–∏. –†—è–¥–æ–º —Å –∫–∞–∂–¥–æ–π —Ñ–æ—Ä–º—É–ª–æ–π –Ω–∞–ø–∏—à–∏—Ç–µ, —Å–∫–æ–ª—å–∫–æ —Ä–∞–∑ –æ–Ω–∞ –≤—Å—Ç—Ä–µ—á–∞–ª–∞—Å—å –≤ —Ç–∞–±–ª–∏—Ü–∞—Ö. –ü—Ä–æ–¥–æ–ª–∂–∞–π—Ç–µ —Ä–∞–±–æ—Ç—É –¥–æ –∏—Å—Ç–µ—á–µ–Ω–∏—è —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–Ω–æ–≥–æ –≤—Ä–µ–º–µ–Ω–∏ –∏–ª–∏ –ø–µ—Ä–µ—Ö–æ–¥–∏—Ç–µ –∫ —Å–ª–µ–¥—É—é—â–µ–π —Ç–∞–±–ª–∏—Ü–µ, –µ—Å–ª–∏ 10 –º–∏–Ω—É—Ç –∏—Å—Ç–µ–∫–ª–∏.6.–í–æ–∑—å–º–∏—Ç–µ –ø—è—Ç–∏–º–∏–Ω—É—Ç–Ω—ã–π –ø–µ—Ä–µ—Ä—ã–≤ –∏ –ø–µ—Ä–µ—Ö–æ–¥–∏—Ç–µ –∫ —Å–ª–µ–¥—É—é—â–µ–π —Ç–∞–±–ª–∏—Ü–µ –≤ —Å–ø–∏—Å–∫–µ.–ê–Ω–∞–ª–∏–∑–∏—Ä—É–π—Ç–µ —Ç–∞–∫–∏–º —Å–ø–æ—Å–æ–±–æ–º –≤—Å–µ —Ç–∞–±–ª–∏—Ü—ã –≤ —Å–ø–∏—Å–∫–µ –¥–æ —Ç–µ—Ö –ø–æ—Ä, –ø–æ–∫–∞ –Ω–µ –∑–∞–∫–æ–Ω—á–∏—Ç–µ –ø–æ–¥—Å—á–µ—Ç—ã. –û—Ç—Å–æ—Ä—Ç–∏—Ä—É–π—Ç–µ –≤–∞—à —Å–ø–∏—Å–æ–∫ –≤ –ø–æ—Ä—è–¥–∫–µ —É–±—ã–≤–∞–Ω–∏—è –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Ç–æ–≥–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ —Ä–∞–∑, –∫–æ—Ç–æ—Ä–æ–µ –∫–∞–∂–¥–∞—è —Ñ–æ—Ä–º—É–ª–∞/–≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è/–∑–Ω–∞—á–µ–Ω–∏–µ —Å–≤–æ–¥–Ω–æ–π —Ç–∞–±–ª–∏—Ü—ã –ø–æ—è–≤–ª—è–ª–∏—Å—å –≤ –æ—Ç—á–µ—Ç–∞—Ö –∏ –∞–Ω–∞–ª–∏–∑–∞—Ö. –≠—Ç–æ—Ç –ø–æ—Ä—è–¥–æ–∫ –æ–±–æ–∑–Ω–∞—á–∏—Ç –≥–ª–∞–≤–Ω—ã–µ –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏—è, –∫–æ—Ç–æ—Ä—ã–º–∏ –≤—ã –±—É–¥–µ—Ç–µ —Ä—É–∫–æ–≤–æ–¥—Å—Ç–≤–æ–≤–∞—Ç—å—Å—è –ø—Ä–∏ –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–∏ —Å–≤–æ–µ–≥–æ –¥–∞–ª—å–Ω–µ–π—à–µ–≥–æ –æ–±—É—á–µ–Ω–∏—è!–ö–∞–∫ –∂–µ –æ—Ä–≥–∞–Ω–∏–∑–æ–≤–∞—Ç—å –æ–±—É—á–µ–Ω–∏–µ?–î–∞–≤–∞–π—Ç–µ —Ä–∞–∑–¥–µ–ª–∏–º –ø—Ä–æ—Ü–µ—Å—Å –æ–±—É—á–µ–Ω–∏—è –Ω–∞ –¥–≤–∞ –¥–≤–∞–¥—Ü–∞—Ç–∏–º–∏–Ω—É—Ç–Ω—ã—Ö –∑–∞–Ω—è—Ç–∏—è (–¥–≤–µ ‚Äú–ø–æ–º–∏–¥–æ—Ä–∫–∏‚Äù —Å–æ–≥–ª–∞—Å–Ω–æ —Ç–µ—Ä–º–∏–Ω–æ–ª–æ–≥–∏–∏ –ø—Ä–∞–∫—Ç–∏–∫—É—é—â–∏—Ö –º–µ—Ç–æ–¥ ‚Äú–ø–æ–º–∏–¥–æ—Ä–∞‚Äù). –ó–∞–∫–æ–Ω—á–∏—Ç–µ —ç—Ç–∏ –¥–≤–∞ –∑–∞–Ω—è—Ç–∏—è –ø–µ—Ä–µ–¥ —Ä–∞–±–æ—Ç–æ–π! –í–µ–¥—å –≤—Å–µ –º—ã —É—Å—Ç–∞–µ–º –ø–æ—Å–ª–µ —Ç—è–∂–µ–ª–æ–≥–æ —Ä–∞–±–æ—á–µ–≥–æ –¥–Ω—è. –ï—Å–ª–∏ –ø–æ—Å—Ç–æ—è–Ω–Ω–æ –ø–µ—Ä–µ–Ω–æ—Å–∏—Ç—å –∑–∞–Ω—è—Ç–∏—è –Ω–∞ –±–æ–ª–µ–µ –ø–æ–∑–¥–Ω–µ–µ –≤—Ä–µ–º—è –≤ —Ç–µ—á–µ–Ω–∏–µ –¥–Ω—è, —Ç–æ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å –∏—Ö –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –±—É–¥–µ—Ç —Å—Ç—Ä–µ–º–∏—Ç—å—Å—è –∫ –Ω—É–ª—é.–ü–µ—Ä–≤—ã–µ 25 –º–∏–Ω—É—Ç –º—ã —Å–æ—Å—Ä–µ–¥–æ—Ç–∞—á–∏–≤–∞–µ–º—Å—è –Ω–∞ –∏–∑—É—á–µ–Ω–∏–∏ R:–û–±—Ä–∞—Ç–∏—Ç–µ –≤–Ω–∏–º–∞–Ω–∏–µ –Ω–∞ –ø–µ—Ä–≤—ã–π –æ–±—ä–µ–∫—Ç –≤ —Å–ø–∏—Å–∫–µ –∏ –∏–∑—É—á–∏—Ç–µ, –∫–∞–∫ –µ–≥–æ —Ä–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å –≤ R. –°–Ω–∞—á–∞–ª–∞ –Ω–∞—É—á–∏—Ç–µ—Å—å –¥–µ–ª–∞—Ç—å —ç—Ç–æ –ø—Ä–∏ –ø–æ–º–æ—â–∏ –¥—Ä—É–∂–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –Ω–∞–º –ø–∞–∫–µ—Ç–∞ dplyr. –ï—Å–ª–∏ –Ω–µ –ø–æ–ª—É—á–∏–ª–æ—Å—å, —Ä–∞—Å—à–∏—Ä—å—Ç–µ –ø–æ–∏—Å–∫–∏ –∏ –Ω–∞–π–¥–∏—Ç–µ —Å–ø–æ—Å–æ–± —Ä–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å —ç—Ç–æ—Ç –æ–±—ä–µ–∫—Ç, –∏—Å–ø–æ–ª—å–∑—É—è R –≤ —Ü–µ–ª–æ–º.–ï—Å–ª–∏ —É –≤–∞—Å –µ—Å—Ç—å –Ω–∞–±–æ—Ä—ã –¥–∞–Ω–Ω—ã—Ö —Å —Ä–∞–±–æ—Ç—ã, –∫–æ—Ç–æ—Ä—ã–µ –º–æ–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å, —Ç–æ —Å–º–µ–ª–µ–µ ‚Äî —Å–¥–µ–ª–∞–π—Ç–µ —ç—Ç–æ. –í –ø—Ä–æ—Ç–∏–≤–Ω–æ–º —Å–ª—É—á–∞–µ –æ–±—Ä–∞—Ç–∏—Ç–µ—Å—å –∫ –≤—Å—Ç—Ä–æ–µ–Ω–Ω—ã–º –Ω–∞–±–æ—Ä–∞–º –¥–∞–Ω–Ω—ã—Ö R –∏–ª–∏ –Ω–∞–±–æ—Ä–∞–º –¥–∞–Ω–Ω—ã—Ö —Å –æ—Ç–∫—Ä—ã—Ç—ã–º –∏—Å—Ö–æ–¥–Ω—ã–º –∫–æ–¥–æ–º –≤ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç–µ.–ü–æ –∏—Å—Ç–µ—á–µ–Ω–∏–∏ 25 –º–∏–Ω—É—Ç —Å–ø—Ä–æ—Å–∏—Ç–µ —Å–µ–±—è, –º–æ–∂–µ—Ç–µ –ª–∏ –≤—ã —Å–≤–æ–±–æ–¥–Ω–æ –ø—Ä–∏–º–µ–Ω—è—Ç—å —ç—Ç–æ—Ç –Ω–∞–≤—ã–∫. –ï—Å–ª–∏ –æ—Ç–≤–µ—Ç –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã–π, –∑–∞—á–µ—Ä–∫–Ω–∏—Ç–µ –ø–µ—Ä–≤—ã–π –ø—É–Ω–∫—Ç –≤ —Å–ø–∏—Å–∫–µ, –∞ –∑–∞–≤—Ç—Ä–∞ –ø–µ—Ä–µ—Ö–æ–¥–∏—Ç–µ –∫ —Å–ª–µ–¥—É—é—â–µ–º—É. –ï—Å–ª–∏ –≤—ã –∏—Å–ø—ã—Ç—ã–≤–∞–ª–∏ —Ç—Ä—É–¥–Ω–æ—Å—Ç–∏ –≤ –ø—Ä–æ—Ü–µ—Å—Å–µ, —Ç–æ –¥–∞–ª—å–Ω–µ–π—à–µ–µ –æ—Å–≤–æ–µ–Ω–∏–µ –Ω–∞–≤—ã–∫–∞ –º–æ–∂–Ω–æ –æ—Ç–ª–æ–∂–∏—Ç—å –Ω–∞ –∑–∞–≤—Ç—Ä–∞.–°–ª–µ–¥—É—é—â–∏–µ 25 –º–∏–Ω—É—Ç –ø–æ—Å–≤—è—Ç–∏–º –∏–∑—É—á–µ–Ω–∏—é SQL:–ü—Ä–æ–π–¥–∏—Ç–µ –±–µ—Å–ø–ª–∞—Ç–Ω—ã–π –∫—É—Ä—Å SQLZOO.–û–∫–æ–Ω—á–∏–≤ –∫—É—Ä—Å, –∏–º–ø–æ—Ä—Ç–∏—Ä—É–π—Ç–µ –¥–∞–Ω–Ω—ã–µ –≤ –æ–Ω–ª–∞–π–Ω —Å—Ä–µ–¥—É SQL –∏ –ø—Ä–æ–¥–æ–ª–∂–∞–π—Ç–µ —Ä–∞–±–æ—Ç—É –ø–æ –∞–≥—Ä–µ–≥–∏—Ä–æ–≤–∞–Ω–∏—é –∏ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏—é –≤–∞—à–∏—Ö —Ç–∞–±–ª–∏—Ü. –ù–∏ –≤ –∫–æ–µ–º —Å–ª—É—á–∞–µ –Ω–µ –∑–∞–≥—Ä—É–∂–∞–π—Ç–µ —Ç—É–¥–∞ –¥–∞–Ω–Ω—ã–µ, —Å–≤—è–∑–∞–Ω–Ω—ã–µ —Å —Ä–∞–±–æ—Ç–æ–π.–ö–∞–∫ —Ç–æ–ª—å–∫–æ –≤—ã –Ω–∞—É—á–∏–ª–∏—Å—å —Å–≤–æ–±–æ–¥–Ω–æ –∞–≥—Ä–µ–≥–∏—Ä–æ–≤–∞—Ç—å –∏ –æ–±—ä–µ–¥–∏–Ω—è—Ç—å —Ç–∞–±–ª–∏—Ü—ã, –ø—Ä–∏—Å—Ç—É–ø–∞–π—Ç–µ –∫ –æ—Å–≤–æ–µ–Ω–∏—é –æ–∫–æ–Ω–Ω—ã—Ö —Ñ—É–Ω–∫—Ü–∏–π.–ê –¥–æ–±–∏–≤—à–∏—Å—å —Å–≤–æ–±–æ–¥–Ω–æ–≥–æ –≤–ª–∞–¥–µ–Ω–∏—è –∏ —ç—Ç–∏–º –Ω–∞–≤—ã–∫–æ–º, –≤—ã –º–æ–∂–µ—Ç–µ –ø—Ä–µ–∫—Ä–∞—Ç–∏—Ç—å –∏–∑—É—á–µ–Ω–∏–µ SQL –∏ –∑–∞–º–µ–Ω–∏—Ç—å 25 –º–∏–Ω—É—Ç–Ω–æ–µ –∑–∞–Ω—è—Ç–∏–µ –ø–æ SQL –Ω–∞ R.–î–æ—Å—Ç–∏–≥–Ω—É–≤ —Å–≤–æ–±–æ–¥–Ω–æ–≥–æ –≤–ª–∞–¥–µ–Ω–∏—è R –∏ SQL, –ø–æ–¥—É–º–∞–π—Ç–µ, –∫–∞–∫ –≤—ã –º–æ–∂–µ—Ç–µ –ø—Ä–∏–º–µ–Ω–∏—Ç—å —ç—Ç–∏ –∑–Ω–∞–Ω–∏—è –≤ —Å–≤–æ–µ–π —Ä–∞–±–æ—Ç–µ. –ß—É–≤—Å—Ç–≤—É–µ—Ç–µ –º–æ—â—å —Ç–æ–π —Å–∏–ª—ã, –∫–æ—Ç–æ—Ä—É—é –≤—ã –æ–±—Ä–µ–ª–∏ –Ω–∞–¥ –¥–∞–Ω–Ω—ã–º–∏, –Ω–∞—É—á–∏–≤—à–∏—Å—å –ø–∏—Å–∞—Ç—å –∫–æ–¥?–†–∞–±–æ—Ç–∞ —Å–æ —Å–ø–∏—Å–∫–æ–º –∑–∞–∫–æ–Ω—á–µ–Ω–∞! –ß—Ç–æ –¥–∞–ª—å—à–µ?–ü—Ä–æ–¥–æ–ª–∂–∞–π—Ç–µ –æ–±—É—á–µ–Ω–∏–µ –∏ –æ–≤–ª–∞–¥–µ–≤–∞–π—Ç–µ –Ω–æ–≤—ã–º–∏ –Ω–∞–≤—ã–∫–∞–º–∏! –†–∞–∑–≤–∏–≤–∞–π—Ç–µ—Å—å! –ñ–∏–∑–Ω—å –Ω–µ –æ–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ—Ç—Å—è –ª–∏—à—å –≤—ã—Ç–∞–ª–∫–∏–≤–∞–Ω–∏–µ–º —Å–ø–∏—Å–∫–æ–≤ –¥–∞–Ω–Ω—ã—Ö –∏ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–µ–º –æ—Ç—á–µ—Ç–æ–≤!–û—Ç–∫—Ä–æ–π—Ç–µ –¥–ª—è —Å–µ–±—è –µ—â—ë –±–æ–ª–µ–µ —É–¥–∏–≤–∏—Ç–µ–ª—å–Ω—ã–µ –∑–Ω–∞–Ω–∏—è. –ù–∞–ø—Ä–∏–º–µ—Ä, –ø—Ä–æ—á–∏—Ç–∞–π—Ç–µ –∫–Ω–∏–≥—É ‚Äú–Ø–∑—ã–∫ R –≤ –∑–∞–¥–∞—á–∞—Ö –Ω–∞—É–∫–∏ –æ –¥–∞–Ω–Ω—ã—Ö‚Äù –ì–∞—Ä—Ä–µ—Ç–∞ –ì—Ä–æ–ª–µ–º—É–Ω–¥–∞ –∏ –•–∞–¥–ª–∏ –£–∏–∫–º–µ–Ω–∞, –ª–µ–≥–µ–Ω–¥—ã R. –£–∑–Ω–∞–π—Ç–µ, —á—Ç–æ —Ç–∞–∫–æ–µ Kaggle. –ü–æ–¥–ø–∏—à–∏—Ç–µ—Å—å –Ω–∞ R-bloggers –∏ —É—á–∏—Ç–µ—Å—å —É —Å–≤–æ–∏—Ö –∫–æ–ª–ª–µ–≥-–ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π R.–ò–∑—É—á–∞–π—Ç–µ R –µ–∂–µ–¥–Ω–µ–≤–Ω–æ –Ω–∞ –ø—Ä–æ—Ç—è–∂–µ–Ω–∏–∏ 6 –º–µ—Å—è—Ü–µ–≤. –ü–æ –º–µ—Ä–µ –æ—Å–≤–æ–µ–Ω–∏—è R –ø–µ—Ä–µ—Ö–æ–¥–∏—Ç–µ –∫ –∏–∑—É—á–µ–Ω–∏—é Python. –ó–Ω–∞–Ω–∏–µ –¥–≤—É—Ö —è–∑—ã–∫–æ–≤ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏—è —Å—Ç–∞–Ω–µ—Ç —Ö–æ—Ä–æ—à–µ–π —Å—Ç–∞—Ä—Ç–æ–≤–æ–π –ø–ª–æ—â–∞–¥–∫–æ–π –¥–ª—è –ø–æ–≥—Ä—É–∂–µ–Ω–∏—è –≤ –Ω–µ–≥–æ.–ó–∞–∫–ª—é—á–µ–Ω–∏–µ–ù–µ—Ç –ª—É—á—à–µ–π –Ω–∞–≥—Ä–∞–¥—ã –¥–ª—è –ø—Ä–µ–ø–æ–¥–∞–≤–∞—Ç–µ–ª—è, —á–µ–º –≤–∏–¥–µ—Ç—å, –∫–∞–∫ –µ–≥–æ —É—á–µ–Ω–∏–∫–∏ –ø—Ä–æ—à–ª–∏ –ø—É—Ç—å —Ä–∞–∑–≤–∏—Ç–∏—è –æ—Ç –∞–Ω–∞–ª–∏—Ç–∏–∫–æ–≤, —Ä–∞–±–æ—Ç–∞—é—â–∏—Ö —Å –¥–∞–Ω–Ω—ã–º–∏ —ç–ª–µ–∫—Ç—Ä–æ–Ω–Ω—ã—Ö —Ç–∞–±–ª–∏—Ü, –¥–æ –∞–Ω–∞–ª–∏—Ç–∏–∫–æ–≤ —Å –Ω–∞–≤—ã–∫–æ–º –Ω–∞–ø–∏—Å–∞–Ω–∏—è –∫–æ–¥–∞. –ö —Å–æ–∂–∞–ª–µ–Ω–∏—é, —è –Ω–µ –º–æ–≥—É –ª–∏—á–Ω–æ —Å–æ–ø—Ä–æ–≤–æ–∂–¥–∞—Ç—å –≤–∞—Å –≤ –ø—Ä–æ—Ü–µ—Å—Å–µ –≤–∞—à–µ–π –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω–æ–π —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏–∏! –ù–æ –Ω–∞–¥–µ—é—Å—å, —á—Ç–æ –º–∞—Ç–µ—Ä–∏–∞–ª —Å—Ç–∞—Ç—å–∏ –≤–¥–æ—Ö–Ω–æ–≤–∏—Ç –≤–∞—Å –Ω–∞ –ø–æ–≤—ã—à–µ–Ω–∏–µ —É—Ä–æ–≤–Ω—è —Å–≤–æ–∏—Ö –Ω–∞–≤—ã–∫–æ–≤, –∏ –≤—ã —Å—Ç–∞–Ω–µ—Ç–µ –±–æ–ª–µ–µ –º–æ—â–Ω—ã–º –∞–Ω–∞–ª–∏—Ç–∏–∫–æ–º –¥–∞–Ω–Ω—ã—Ö.–£ –≤–∞—Å –≤—Å—ë –ø–æ–ª—É—á–∏—Ç—Å—è!–ß–∏—Ç–∞–π—Ç–µ —Ç–∞–∫–∂–µ:7 —à–∞–≥–æ–≤ –¥–æ —É—Ä–æ–≤–Ω—è –ú–æ—Ü–∞—Ä—Ç–∞ –∫–æ–¥–∞5 –Ω–µ–¥–æ–æ—Ü–µ–Ω—ë–Ω–Ω—ã—Ö –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–π –≤ –ø–æ–º–æ—â—å –ø—Ä–æ–≥—Ä–∞–º–º–∏—Å—Ç—É–í—ã–±–∏—Ä–∞–µ–º —à—Ä–∏—Ñ—Ç–ß–∏—Ç–∞–π—Ç–µ –Ω–∞—Å –≤ Telegram, VK –∏ –Ø–Ω–¥–µ–∫—Å.–î–∑–µ–Ω–ü–µ—Ä–µ–≤–æ–¥ —Å—Ç–∞—Ç—å–∏ Justin: Learn to code for data: a pragmatist‚Äôs guideWritten byJenny VFollowLearn To CodeRSqlSpreadsheetsNuances Of ProgrammingMore from NOP::Nuances of ProgrammingFollow–û–±—Ä–∞–∑–æ–≤–∞—Ç–µ–ª—å–Ω—ã–µ —Å—Ç–∞—Ç—å–∏ –∏ –ø–µ—Ä–µ–≤–æ–¥—ã‚Ää‚Äî‚Ää–≤—Å—ë –¥–ª—è –ø—Ä–æ–≥—Ä–∞–º–º–∏—Å—Ç–∞Read more from NOP::Nuances of ProgrammingMore From Medium–ê–Ω–Ω–æ—Ç–∞—Ü–∏–∏ –¥–ª—è –ø–∞—Ä–∞–ª–ª–µ–ª–∏–∑–º–∞ –≤ Java: –Ω–∞—à –ø–æ–¥—Ö–æ–¥ –∫ —Ä–∞—Å—Ü–≤–µ—á–∏–≤–∞–Ω–∏—é –ø–æ—Ç–æ–∫–æ–≤–†—É–¥–æ–ª—å—Ñ –ö–æ—Ä—à—É–Ω in NOP::Nuances of Programming–¢–û–ü‚Ää‚Äî‚Ää5 —Å–æ–≤–µ—Ç–æ–≤, –∫–∞–∫ —É–ª—É—á—à–∏—Ç—å —Å–≤–æ–∏ UI –Ω–∞–≤—ã–∫–∏Teya Manasherova in NOP::Nuances of Programming–°–±–æ—Ä–∫–∞ –∏ –∑–∞–ø—É—Å–∫ –∑–∞–≥—Ä—É–∑—á–∏–∫–∞Iuliia Averianova in NOP::Nuances of ProgrammingFake-–æ–±—ä–µ–∫—Ç—ã –ø—Ä–∞–∫—Ç–∏—á–Ω–µ–µ mock-–æ–±—ä–µ–∫—Ç–æ–≤Iuliia Averianova in NOP::Nuances of ProgrammingUX –∏–ª–∏ UI‚Ää‚Äî‚Ää–Ω–∞ —á—Ç–æ —É–¥–µ–ª–∏—Ç—å –≤–Ω–∏–º–∞–Ω–∏–µ?Teya Manasherova in NOP::Nuances of Programming5 –ø–æ–¥–≤–æ–¥–Ω—ã—Ö –∫–∞–º–Ω–µ–π –Ω–µ—Ä–µ–ª—è—Ü–∏–æ–Ω–Ω—ã—Ö –±–∞–∑ –¥–∞–Ω–Ω—ã—Ö–ê–Ω–¥—Ä–µ–π –®–∞–≥–∏–Ω in NOP::Nuances of Programming5 –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ —Ç–æ–≥–æ, —á—Ç–æ –≤—ã —Ç—Ä–∞—Ç–∏—Ç–µ —Å–≤–æ–π –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª —Ä–∞–∑—Ä–∞–±–æ—Ç—á–∏–∫–∞ –≤–ø—É—Å—Ç—É—éElena Andenko in NOP::Nuances of Programming–ö–∞–∫ —Ä–∞–∑–≤–µ—Ä–Ω—É—Ç—å –ø–∞–∫–µ—Ç Cython –≤ PyPI–ê–Ω–¥—Ä–µ–π –®–∞–≥–∏–Ω in NOP::Nuances of ProgrammingAboutHelpLegalGet the Medium app"
Pre-Processing Data with R,https://medium.com/@tenos.id/pre-processing-data-with-r-b50593e9163e?source=tag_archive---------1-----------------------,"Text Mining,Text Preprocessing,R","Haloo sahabat Tenos, selamat bermalam minggu.Akhir-akhir ini sedang hype tentang text-mining loh. Kira-kira apasih text mining?Text mining adalah proses penemuan akan informasi atau trend baru yang sebelumnya tidak terungkap dengan memproses dan menganalisa data dalam jumlah besar (Adiwijaya, 2006).‚ÄúData dalam jumlah besar‚ÄùWah pasti bukan hanya jumlah yang besar, biasanya data nya pun bertipe ‚Äúunstructured data‚ÄùJadi sebelum text mining, kita perlu ‚Äúmembersihkan‚Äù unstructured data tersebut. Terkadang data yang diperoleh banyak mengandung ‚Äúnoise‚Äù, bahkan tidak memiliki struktur yang jelas, dan terkadang tidak mencerminkan makna yang sebenarnya.Nah proses tersebut dikenal dengan pre-processing data. Secara singkatnya mengubah unstructured data -> structured data untuk proses mining.Sebagai contoh di story medium Tenos yang berjudul ‚Äúscraping tweets dengan R‚Äù, pasti tahapan pertama yang dilakukan ketika sudah mendapatkan data #Pilkada2020 ialah melakukan Pre-Processing atau cleaning data.Sebagai contoh, mimin akan melakukan pre-processing data tweets #Pilkada2020 dengan R.Menyiapkan data tweets #Pilkada2020Menginstall package3. Text SubbingFungsi sub, kependekan dari substitute yaitu mencari pola tertentu pada suatu text dan menggantinya. Misalnya menhapus/mengganti substring dengan text kosongMaka seluruh cuitan yang mengandung pola ‚Äú\n‚Äù akan diganti dengan spasi4. Text ReplacementSerupa dengan text subbing, text replacement ini lebih spesifik. Dapat mengganti/menghapus html, url, emoji, tanggal, dan lain-lain.Sebagai contoh mimin akan menghapus html dan url pada data pilkada20205. Replacement -> Mention dan HastagFungsi replace ini memiliki banyak kegunaan selain menghapus html/url, kita dapat menghapus/mengganti hastag atau mention yang terdapat pada Pilkada2020 dengan fungsi replace_hash dan replace_tag.6. Slang WordSlang word biasa dikenal dengan ‚ÄúBAHASA GAUL‚Äù. Dalam menulis status di social media, acapkali ditemukan kata-kata gaul atau singkatan. Nah kata-kata seperti itu perlu kita ubah menjadi kata yang formal. Sehingga data yang kita miliki lebih tertata dan mudah untuk dimengerti.7. Text StrippingText stripping merupakan fungsi untuk menghapus pola atau symbol yang tidak relevan atau tidak bermakna. Seperti ‚Äú‚Ä¶..‚Äù atau ‚Äú,,,,,‚Äù pada text.StemmingStemming merupakan suatu proses untuk menemukan kata dasar dari sebuah kata. Dengan menghilangkan semua imbuhan (affixes) baik yang terdiri dari awalan (prefixes), akhiran(suffixes) dan confixes (kombinasi dari awalan dan akhiran) pada kata turunan. Stemming digunakan untuk mengganti bentuk dari suatu kata menjadi kata dasar. Sebagai contoh, saya ingin mengetahui kata dasar dari‚Äúmembagikan‚Äù ->bagi‚Äúmengembangkan‚Äù -> kembangBerbeda dengan text cleaning yang sudah tersedia banyak fungsi bawaan, sedangkan untuk melakukan stemming kita perlu membuat fungsi stemming terlebih dahuluTOKENIZINGTokenize merupakan proses pembagian kalimat atau text menjadi bagian-bagian tertentu. Misal kalimat‚Äúsaya memakan roti‚ÄùJika dilakukan tokinaizing maka akan terdapat 3 token yaitu,saya, memakan, rotiStopwordStop words merupakan common words atau kata umum yang biasanya muncul dalam jumlah yang banyak dan dianggap tidak memiliki makna. Sebagai contoh, the, and, of.Nah stopword yang tersedia pada R merupakan common word dalam Bahasa inggris.Sedangkan tweet #pilkada2020 merupakan tweet berbahasa Indonesia, jadi kita perlu data yg berisi stop word dalam Bahasa Indonesia sebagai contoh kata :yang, dan, tetapi, sedangkan, dllSetelah data sudah ‚Äúbersih‚Äù maka kita dapat melanjutkan ke tahap selanjutnya text mining. Entah melakukan Sentiment Analysis atau yang lainnya.Selamat Mencoba!Written byIndekstat ‚Äî TenosBig Data Analytic , IT Developer, and IT Consulting & ServicesFollowText MiningText PreprocessingRMore from Indekstat ‚Äî TenosFollowBig Data Analytic , IT Developer, and IT Consulting & ServicesMore From MediumFast transition between dplyr and data.tableNata Berishvili in Towards Data ScienceBBC News Text ClassificationCigdem Tuncer in Analytics VidhyaVanilla Neural Networks in RChris Mahoney in Towards Data ScienceTwitter Text Analysis in RKieran Tan Kah Wang in Towards Data ScienceUsing Keras Tokenizer Class for Text Preprocessing Steps‚Ää‚Äî‚Ää1st Presidential Debate Transcript 2020Kurt F. in An Idea (by Ingenious Piece)Natural Language Processing in Production: 27 Fast Text Pre-Processing MethodsBruce H. Cottman, Ph.D. in The StartupNLP- Text Preprocessing TechniquesPrassena Kannan in The StartupInterpret Regression Analysis Results using R: Biomedical DataSwayanshu Shanti Pragnya in Analytics VidhyaAboutHelpLegalGet the Medium app"
10 Best Data Science Books on R,https://medium.com/swlh/10-best-data-science-books-on-r-777dba4ec2fa?source=tag_archive---------0-----------------------,"Data Science,Data Analysis,R,Best Data Science Books,R Books","It is always complained that finding written sources in the R programming language is not as easy as in other current languages. Unfortunately, enough blog support and current question sources are not the address of the solution. However, these are not the only options for those who manage to work with books.Documentation literacy is perhaps the most difficult but necessary workload of software. Realizing this much earlier, I think, is the most important way to be permanent in the sector. However, not many people bother to do it either because they are used to it or because they think they have enough code knowledge ‚Äî that‚Äôs exactly what it is. It‚Äôs also a reason why you can‚Äôt get deep. In other words, as the reading level increases, it is not difficult to increase the quality.Of course there are other things that I would like to mention in this post. First of all, I will introduce some resources to you to better understand the R language. For this, I would like to share with you the resources and links that we will discuss in some detail. If you‚Äôre ready, let‚Äôs start:1-John Chambers / Software for Data AnalysisYou can find books by John Chambers and many other data scientists at Springer. These are a very well designed resource with good examples for you to gain real programming skill in R with statistics.Image by Springer (1)2-Bill Venables / An Introduction to RThe Bill Venables book is indeed a good introduction for R. The guide looks pretty straightforward, but is still ideal for beginnersImage by Amazon (2)3-Brian Ripley-Bill Venables / S ProgrammingThis book is dense, but contains a lot of high-level information that you won‚Äôt find easily anywhere else. This is basically a programming reference for those with experience in R and possibly another programming language. For example, information about loops (for, while, again) and their endings (next, search) is found on a page.Image by Springer (3)4- Venables-Ripley / Modern Applied Statistics with SThis is a print run to have in S + / R, but the book is not for novices in Statistics. You will not learn how and why to apply such models correctly, but how to fit generalized linear models into the language. For this purpose, there are a large number of custom monographs, and most of them use R for examples. Also, this book assumes some basic programming knowledge.Image by Springer (4)5-Pinheiroand Base / Mixed-Effects Models in S and S-PlusIt includes the excellent depth and scope of security analysis. This text is very well written and insightful. Can be used as a helpful resource.Image by Springer (5)6-Paul Murrell / R GraphicsThis book provided a thorough treatment of the different systems for creating graphics in R. It particularly emphasizes grid graphics, and how to use low level plotting functions to create, extend, and enhance plots. The book is divided into four major sections and there are ample examples of graphs and code. Further, each chapter starts with a brief preview and ends with a short summary paragraph.Image by Amazon (6)7- Garret Grolemund and Hadley Wickham / R for Data ScienceIf you want to sharpen your R skills, R for Data Science is the perfect book. It covers the basics for new R users, such as data cleaning, but also gets into more advanced topics as well. Data scientists can spend up to 80% of their time cleaning data, so this is a reference you will definitely want to keep close by.Image by Amazon (7)8-Hadley Wickham / Advanced RIt covers everything from the foundations, including data structures, object oriented programming, and debugging, to functional programming and performance code.Image by Amazon (8)9-Nina Zumel / Practical Data Science with R PaperbackThis is a great book that artfully bridges the gap of data science as a process and data science as a practice. Really well written. It introduces R, version control, databases, a bit of visualization and some techniques that everyone doing data science should have on their toolbox.Image by Amazon (9)10-Jared P. Lander / R For EveryoneThis book gives an introduction into R programming and graphics, but also to many statistical and Machine Learning methods. It even comprises chapters about Markdown, Shiny and building R packages. Pertaining to R programming it provides a very good overview about base R, but also tidyverse and the data.table package. The description of statistical methods is focused on the coding aspects.Image by Amazon (10)And the R-bloggers website has a longer list of books.And of course it has free links for R Books:20 Free Online Books to Learn R and Data Science - Python and R TipsIf you are interested in learning Data Science with R, but not interested in spending money on books, you are‚Ä¶cmdlinetips.comReferences(1)https://www.springer.com/gp/book/9780387759357(2)https://www.amazon.com/Introduction-R-William-N-Venables/dp/0954612086(3)https://www.springer.com/gp/book/9780387989662(4)https://www.springer.com/gp/book/9780387954578(5)https://www.springer.com/de/book/9780387989570(6)https://www.amazon.com/Graphics-Chapman-Hall-CRC/dp/1439831769(7)https://www.amazon.de/-/en/Hadley-Wickham/dp/1491910399(8)https://www.amazon.com/Advanced-Chapman-Hall-Hadley-Wickham/dp/1466586966(9)https://www.amazon.com/Practical-Data-Science-Nina-Zumel/dp/1617291560(10)https://www.amazon.de/R-Everyone-Addison-Wesley-Data-Analytics/dp/0321888030The StartupMedium's largest active publication, followed by +723K people. Follow to join our community.Follow147 1 Data ScienceData AnalysisRBest Data Science BooksR Books147¬†claps147¬†claps1 responseWritten byKurt F.FollowPhysicist Analyst & always learner ‚Äî writer because of his enthusiasm: Phython / R / Data Science / Computer Science / AI ‚Äî ML ‚Äî DLFollowThe StartupFollowMedium's largest active publication, followed by +723K people. Follow to join our community.FollowWritten byKurt F.FollowPhysicist Analyst & always learner ‚Äî writer because of his enthusiasm: Phython / R / Data Science / Computer Science / AI ‚Äî ML ‚Äî DLThe StartupFollowMedium's largest active publication, followed by +723K people. Follow to join our community.More From MediumWho‚Äôs home and who isn‚Äôt? The challenges of conducting face-to-face interviews in JordanLaura Silver in Pew Research Center: DecodedData Science Must-KnowBoadzie Daniel in Analytics VidhyaThemes Don‚Äôt Just Emerge ‚Äî Coding the Qualitative DataProjectUXAdopting Data Science Solutions for Business: Balancing Complexity, Accuracy and InterpretabilityThomas Gorin in BCG GAMMAPython‚Ää‚Äî‚ÄäRead multiple SQL database tables into csvMukesh SinghGetting Your Data Ready for AIO'Reilly Media in oreillymediaMick Jagger & Circular Buffers in F#Chris White in The StartupML: Student‚Äôs, Two-Sample & Paired Sample T-tests. Don‚Äôt Use It Blindly.Jeheonpark in The StartupLearn more.Medium is an open platform where 170 million readers come to find insightful and dynamic thinking."
5-Minute Guide to Calling Functions from R Scripts,https://towardsdatascience.com/5-minute-guide-to-calling-functions-from-r-scripts-41c4a09db1eb?source=tag_archive---------0-----------------------,"R,R Tutorial,Code,Google Trends,Editors Pick","Photo by Christina Morillo from PexelsYou‚Äôve likely heard the popular guideline that if you find yourself copying and pasting code more than 3 times, you should write it as a function. While you can write and store these functions at the top of your R Markdown files, this approach can get messy and is counterproductive if you end up copying and pasting the functions into multiple files. Often, the best way to stay organized is to write your functions in a script and to call them from any additional files where they‚Äôre needed.To demonstrate this process, I will use 3 functions to conduct a very simple change point analysis of Google searches containing the words ‚Äúsupreme court‚Äù over the past month.First, create your functionsIn an R script, we write three simple functions. The first plots the Google Trends data, the second performs a simple change point analysis using the bcp() function from the ‚Äúbcp‚Äù package, and the third plots the results of this analysis.google_graph = function(data, date, observation, graph_title) {    data %>%     ggplot() +    geom_line(aes(x = date, y = observation),               color = ""#09557f"",              alpha = 0.6,              size = 0.6) +    labs(x = ""Date (Start of Week)"",          y = ""Relative Proportion"",         title = graph_title) +    theme_minimal() +    scale_x_date(date_breaks = ""1 week"") +    theme(axis.text.x = element_text(angle = 45))  }bcp_analysis = function(observation, data) {    set.seed(100)    bcp = bcp(observation)    prob = bcp$posterior.prob  prob = as.data.frame(prob)     bcp_dataframe = cbind(data, prob) %>%     select(date, prob)  }bcp_plot = function(dataframe){    dataframe %>%     ggplot() +    geom_line(aes(x = date, y = prob),              color = ""#09557f"",              alpha = 0.6,              size = 0.6) +    labs(x = """",         y = ""Posterior Probability"",         title = ""Changepoint Probabilities"") +    theme_minimal() +    ylim(0, 1) +    scale_x_date(date_breaks = ""1 week"") +    theme(axis.text.x = element_text(angle = 45))  }I‚Äôve also checked this ‚ÄúSource on Save‚Äù box. If you check this box, then the file will be sourced automatically to the global environment when you save changes to your functions in the script.Connect to your functionsConnecting to functions stored in a script from an R Markdown file is very similar to connecting to functions stored in a package. Instead of using a library() statement, we use a source() statement and indicate the script‚Äôs path name. In this case, we use the following code:source(""./functions.R"")When we run this line of code, the functions contained within the script automatically appear in the Global Environment. The connection was successful!Use your functionsFirst, we‚Äôll just use the gtrends() function from the ‚ÄúgtrendsR‚Äù package to pull Google search volume for searches containing the words ‚Äúsupreme court‚Äù in the United States over the past month. A mutate step is also used to convert the ‚Äúdate‚Äù variable to a date format:data = gtrends(keyword = ""supreme court"", geo = ""US"",                time = ""today 1-m"")$interest_over_time %>%   mutate(date = as.Date(date))Now we have data to use in our functions! We use the google_graph() function the same way we would use any other function, allowing us to easily plot the data:google_graph(data, data$date, data$hits, ‚ÄòGoogle Searches for ‚ÄúSupreme Court‚Äù‚Äô)The plot looks good! Unsurprisingly, there is a massive surge in searches containing ‚Äúsupreme court‚Äù following the death of Supreme Court Justice Ruth Bader Ginsburg. Let‚Äôs use the bcp_analysis() and bcp_plot() functions to see if this spike represents a significant change point in this time-series object. Again, we use the functions the same way we would if we were using functions from a loaded package:bcp_dataframe = bcp_analysis(data$hits, data)bcp_plot(bcp_dataframe)Several days following the death of Ruth Bader Ginsburg have posterior probabilities of 1.00, indicating that searches containing the words ‚Äúsupreme court‚Äù likely changed in a statistically meaningful way around this time.It‚Äôs as simple as that! If we wanted to plot and analyze several different sources of data in separate Rmd files, it would be as easy as connecting those files to our ‚Äúfunctions‚Äù script with a source() statement and using our functions.Written byEmily A. HalfordI am currently a data analyst working in psychiatric epidemiology, and I am excited about the intersection of data science and mental health. Views are my own.Follow38 1 Sign up for The Daily PickBy Towards Data ScienceHands-on real-world examples, research,  tutorials, and cutting-edge techniques delivered Monday to Thursday. Make learning your daily ritual.¬†Take a lookGet this newsletterBy signing up, you will create a Medium account if you don‚Äôt already have one. Review our Privacy Policy for more information about our privacy practices.Check your inboxMedium sent you an email at  to complete your subscription.38¬†38¬†1 RR TutorialCodeGoogle TrendsEditors PickMore from Towards Data ScienceFollowA Medium publication sharing concepts, ideas, and codes.Read more from Towards Data ScienceMore From Medium5 YouTubers Data Scientists And ML Engineers Should Subscribe ToRichmond Alake in Towards Data Science7 Must-Haves in your Data Science CVElad Cohen in Towards Data Science21 amazing Youtube channels for you to learn AI, Machine Learning, and Data Science for freeJair Ribeiro in Towards Data ScienceThe Roadmap of Mathematics for Deep LearningTivadar Danka in Towards Data Science30 Examples to Master PandasSoner Yƒ±ldƒ±rƒ±m in Towards Data ScienceAn Ultimate Cheat Sheet for Data Visualization in PandasRashida Nasrin Sucky in Towards Data ScienceHow to Get Into Data Science Without a DegreeTerence S in Towards Data ScienceHow To Build Your Own Chatbot Using Deep LearningAmila Viraj in Towards Data ScienceAboutHelpLegalGet the Medium app"
Machine Learning to Kaggle Caravan Insurance Challenge on R,https://medium.com/swlh/machine-learning-to-kaggle-caravan-insurance-challenge-on-r-f52790bc7669?source=tag_archive---------1-----------------------,"Machine Learning,R,Data Science,Data Analysis,Logistic Regression","Photo by Scott Graham on UnsplashRecapping from the previous two posts, this post will utilise machine learning algorithms to predict customers who are mostly likely to purchase caravan policy based on 85 historic socio-demographic and product-ownership data attributes. In the previous post, we talked about using several feature selection methods like forward/backward stepwise selection and lasso regularisation to reduce the number of attributes we should fit into our ML algorithms. We will discuss which set of attributes as identified by different feature selection methods, will give us the best prediction results.Logistic Regression model development 1Since the target attribute (i.e. whether the customer purchases or not purchase caravan policy) is binomial discrete, we can use the simplest logistic regression as our first ML algorithm usingglm function from glmnet package. We first fit all the variables in to see how does the model performs using V86~ which means all attributes as the predictor variables other than V86 which is our target variable.Observations from this model:At 5% significance level, V47 (Contribution car policies), V55 (Contribution life policies), V59 (Contribution fire policies), V76 (Number of life insurances), V82 (Number of boat policies) are significantResidual deviance is lesser than null deviance which means this model is more useful compared to one without any predictor variablesNote that the *** signifies the level of significance of this predictor variable.We can use the Chisq test to check how well is the logistic regression model in comparison with a null model.The p-value is really small which means this model is significantly better than the null model.Since our task is to predict if a specific customer will buy caravan policies, we create a confusion matrix by using confusionMatrix function from the caret library to show how many predictions this model has made correctly and incorrectly in our training dataset.This model predicted that 9+7=16 customers will buy mobile home policies and 5465+341=5806 will not buy mobile home policies. Of these observations, 5465 out of (5465+9) which is approximately 99.84% (sensitivity) were correctly predicted by the model that these customers will not buy mobile home policies and they indeed not buy it. Whereas, only 7 out of (7+341) which is approximately 2.01% (specificity) were correctly predicted by the model that these customers will buy mobile home policies and they indeed bought it. Hence, this model has high sensitivity but very low specificity.Note that even though the accuracy of the model is high at 93.99%, it is expected as since our dataset is highly unbalanced with only 348 (6%) of the customers having bought mobile home policies. If we were to predict that all customers will not purchase mobile home policies, we will still get an overall percentage of 5474/5822 of correct predictions which is approximately 94.02% (i.e. No Information Rate). As a result, what we are aiming for in terms of the training error is a model with high specificity since we are trying to identify the customers that will buy mobile home policies.We can illustrate the sensitivity and specificity of the model by plotting a Receiver Operating Characteristic (ROC) curve by using the library pROC.What will be ideal in a ROC curve for a good model is for both the sensitivity and specificity to be close to the left top edge of the curve which means area under the curve is 1. However as seen in this plot, the line is far away from the top left edge and area under the curve is only 0.509 which is also obtainable as Balanced Accuracy in the previous Confusion Matrix and Statistics section.Since we know that our data is highly imbalanced, we can use sampling method like K-Fold Cross Validation approach to get a more accurate value for the accuracy of our model. We will use 10 Fold Cross Validation which involved splitting our training data into 10 folds, and we train 10 models each of them using 9 of the fold as training data and 10th fold as the validation data to get the accuracy of the model. We then obtain the mean of the accuracy of all the 10 models.With 10-Fold Cross Validation, we noted the accuracy of this model is 0.9375.Logistic Regression model development 2We repeat the above using the set of attributes identified by Forward stepwise selection, Backward stepwise selection and Lasso Regularisation. From there, it can be shown that Lasso Regularisation identified attributes performed better in terms of specificity. Now, we can try to fit another model using 7 significant attributes as identified by the logistic regression trained with Lasso regularisation identified attributes.Observations from this model:At 5% significance level, V18 (Lower level education), V47 (Contribution car policies), V59 (Contribution fire policies), V82 (Number of boat policies), V83 (Number of bicycle policies) are significant.Residual deviance of 2376.7 which is higher than the 2nd model.We create a confusion matrix to show how many predictions this model has made correctly and incorrectly in our training dataset.The sensitivity increased slightly from 99.8% to 99.9%. The specificity decreased slightly from 1.44% to 0.8621%.We can run the anova() function to compare this model and the 2nd model and see if they are statistically different.The anova test shows that this model is statistically different compared to the 2nd model since p-value is low (i.e. <0.05). Again, we use 10-Fold Cross Validation Sampling Method to check our accuracy of the model.The cross-validated accuracy is the highest thus far, and is slightly higher than the 2nd model at 0.9385.We can consider the variance inflation factors to see if there are any variables that we can consider to remove to simplify our model.Variance Inflation Factor is a measure of how much the variance of the estimated regression coefficient is ‚Äúinflated‚Äù by the existence of correlation among the predictor variables in the model.A VIF of 1 means that there is no correlation among the k-th predictor and the remaining predictor variables, and hence the variance of the k-th coefficient is not inflated at all. The general rule of thumb is that VIFs exceeding 4 warrant further investigation, while VIFs exceeding 10 are signs of serious multicollinearity requiring correction.We use the vif function from the car library to determine the VIF.As seen, all the 7 variables that were fitted in the 5th model have VIF of around 1 which means there is no correlation among the k-th predictor and the remaining predictor variables. Hence, we will stick to the same model parameters and keep it as the best model for our logistic regression.Linear Discriminant Analysis model development 1The 2nd type of model we can consider is Linear Discriminant Analysis (LDA) which is closely connected to Logistic Regression in which both produce linear decision boundaries that separate a class from another. The only difference is LDA will assume that the observations are drawn from Gaussian Distribution with a common covariance matrix in each class, and if this assumption is true, it will perform better than the Logisitc Regression.We first fit a LDA model with the full set of variables using lda function from the MASS library and show the confusion matrix to find out the specificity and sensitivity.We see that the sensitivity is 99.2% and specificity 7.8%. Comparing this full LDA model with the full Logistic Regression model, although the sensitivity of the LDA model is lower, it‚Äôs specificity is higher than 2.01% in the full Logisitic Regression model. If two or more variables are almost a linear combination to each other, their estimated coefficients will be close to 0 which makes it hard to interpret entirely their effects on the target variable. It is worthy to note that we should avoid variables that are highly correlated to each other in fitting LDA.Also note that there is a warning message of ‚Äúvariables are collinear‚Äù while fitting the LDA model with the full dataset which means some of our predictor variables are correlated to each other which was what was already explored in the EDA section.Let‚Äôs use 10-Fold Cross Validation Sampling method to get the accuracy of the this model.The accuracy of this model is 93.25%. If we were to compare this model to the models fitted in the logistic regression, this model is currently the best in terms of specificity as it is able to predict correctly 7.76% of the customers who indeed bought mobile home policies, which is the main aim for our prediction task, although the accuracy of the this model is lower than all the models in logistic regression.Linear Discriminant Analysis model development 2Next, we try to fit the 7 variables that were identified in Logistic Regression model development which performed the best in terms of model complexity and testing specificity.Sensitivity is 99.5% and specificity 4.02%. ROC (balanced accuracy) is 0.518. This model performs worse in terms of specificity and better in terms of sensitivity than the full LDA model.Note that we did not receive the warning message of ‚Äúvariables are collinear‚Äù in this model as the 7 variables fitted between them has non-significant correlation between them, which was explored in the Variable Inflation Factor (VIF).Let‚Äôs perform 10 Fold Cross Validation to get the mean accuracy of this model.Accuracy of this model is 0.9376 which is higher than the previous model at 0.9325, but still lower than all the models in logistic regression.Quadratic Discriminant Analysis model development 1The 3rd type of model we can consider is Quadratic Discriminant Analysis (QDA). In Linear Discriminant Analysis, we assume that the observations are drawn from Gaussian Distribution with a common covariance matrix in each class. However in QDA, this assumption does not hold as each class is allowed to have different covariance matrix.In both of our Logistic Regression and Linear Discriminant Analysis model development, we found out that the best model respectively is the one using the 7 variables that were deemed significant from Lasso regression, Forward Stepwise Selection and Backward Stepwise Selection. Hence, we first train our QDA model using this set of 7 variables.Sensitivity is 96.89% which is the lowest thus far out of all the models trained. Specificity is 9.20% which is the highest out of all models thus far.Let‚Äôs get the mean accuracy of this model from 10-Fold Cross Validation.Accuracy of model is 0.9189 which is the lowest thus far out of all the models that have been trained.Since the accuracy of QDA appears to be lower than both Logistic Regression and Linear Discriminant Analysis model, we will not train any more models with QDA.Chosen ModelComparing these 3 models, the best model we will choose for our prediction task will the 2nd model of Linear Discriminant Analysis containing the set of predictor variables (V18, V41, V47, V58, V59, V82, V83) that were identified as significant from the first 4 Logistic Regression Models. Firstly, even though the CV accuracy is lower compared to other models in Logistic Regression, it‚Äôs specificity is the highest. Since the task is to find the subset of customers who are likely to purchase caravan policies so that the rest who don‚Äôt will receive a mailing, our aim will be to maximise the identification of this group of customers who are likely to purchase mobile home policies so that the insurance company can save cost on the mailing who a smaller group of customers who are identified as not likely to purchase the mobile home policies, which means we will be looking more on specificity of the model. Although the specificity of the QDA model is the highest, the CV accuracy for the QDA model is very low. As a result, we chose the 2nd model in LDA as the best model for our prediction task as we want something in between for accuracy and specificity.Prediction using testing datasetWe are also provided with the testing dataset and we can use it to see how well will the LDA model we have identified as best perform. We are supposed to find a set of 800 customers in this testing dataset that are most likely to purchase mobile home policies based on the probabilities predicted by our model. After identifying this set of 800 customers, we then use the true testing data target values and see how many of them are identified correctly and incorrectly. Note that in real life problems, all we may have is a set of data which we have to use method like 70:30 split for us to get training and testing data in order to test our ML algorithms.The accuracy of our model using testing dataset is 79.7% in which it‚Äôs sensitivity was 81.74% and specificity 47.48%. Out of a total of 238 actual mobile home policy customers, our model correctly identified 113 of them.SummaryThe winner of this challenge came up with an algorithm that was able to identify 121 customers correctly, and ours which successfully identified 113, wasn‚Äôt that bad. The standard benchmark tests result in 94 using K-nearest neighbours, 102 using naive bayes and 105 using neural networks, which were all more complicated than our LDA model. Hope you guys enjoy this ML model building walkthrough and have a better sense now on how to approach a problem using exploratory data analysis, feature selection and machine learning!The StartupMedium's largest active publication, followed by +723K people. Follow to join our community.Follow52 Machine LearningRData ScienceData AnalysisLogistic Regression52¬†claps52¬†clapsWritten byKieran Tan Kah WangFollowData Analytics | Artificial Intelligence | Data Visualization | Perspective | https://www.linkedin.com/in/tankahwang/FollowThe StartupFollowMedium's largest active publication, followed by +723K people. Follow to join our community.FollowWritten byKieran Tan Kah WangFollowData Analytics | Artificial Intelligence | Data Visualization | Perspective | https://www.linkedin.com/in/tankahwang/The StartupFollowMedium's largest active publication, followed by +723K people. Follow to join our community.More From MediumTransferring large CSV files into a relational database using dingDONGTal shany in The StartupK-Nearest Neighbor(k-NN)Akshay Patel in Analytics VidhyaExploring the Bin Packing ProblemColton Saska in The StartupRobustness of Limited Training Data: Part 2Daniel Hogan in The DownLinQSearching for Pulsars with Machine LearningFrank Ceballos in Frank CeballosDealing with Categorical DataVictor Popov in machine_learning_eli5Rocket your Machine Learning practice with this bookJaime Zornoza in The StartupLinear RegressionMohammad RoufaLearn more.Medium is an open platform where 170 million readers come to find insightful and dynamic thinking."
A/B Hypothesis Testing Explained Using R,https://medium.com/@marco_17979/a-b-hypothesis-testing-explained-using-r-a44252f47149?source=tag_archive---------2-----------------------,"Conversion Optimization,Statistics,Math,R","PreludeIn the last article, I talked about the importance of using descriptive statistics for your data.If you didn‚Äôt read the last article, I highly suggest you do it.Over explaining 8 famous descriptive statistics concepts, some are taken for granted in this article and they won‚Äôt be explained again.Hopefully, you learned a lot just by looking at the practical examples.If you do it yourself using R, you‚Äôll learn much faster.Now, let‚Äôs dive into the second article.Ever heard of CRO?It stands for conversion rate optimization and it‚Äôs one of the branches of digital marketing nowadays.It‚Äôs based on the hypothesis that your website might perform better and take action on that.CRO consultants tweak buttons, product messaging, copy, design, and more to increase your conversion rate.If you‚Äôre a math geek and you‚Äôd like to get a sneak peek into the CRO world, this article is for you.If you‚Äôre a CRO consultant and you‚Äôd like to know how to use R for simple A/B testing concepts, this article is for you.If you‚Äôre just curious and you want to learn, this article is for you as well.We‚Äôre gonna explain the fundamental concepts of A/B testing using R:Sampling, sample size, and PopulationHypothesis Creation and Null HypothesisType I and Type II errorsP-Value and Significance LevelConfidence Level and PowerOne Sample T-test and Multiple T-testsMultiple T-tests problems and ANOVAHow Can A/B Hypothesis Testing Help you?Let‚Äôs say that you work for an E-com website and you want to learn how the last UX redesign impacted your user engagement or your purchase rate.You don‚Äôt know whether that‚Äôs positive or negative yet, but you have lots of data at hand.That‚Äôs where hypothesis testing comes into the picture.It‚Äôll help you evaluate whether the change is real and durable, or it‚Äôs just a random fluctuation.Hypothesis testing provides you with a framework you can repeatedly use to make informed decisions based on data.The better your test results, the higher is your confidence level that changing your website has been a good choice.We‚Äôll talk about that later on.Sampling, sample size, and PopulationBefore making tests, we need to know the fundamental concepts.In statistics, a sample is a portion of the original population.Why do we do this?Think about it.You want to know which is the average male height in the US.Would you measure all of us?You can, but it‚Äôs not efficient: it‚Äôd take you years.Instead, analysts take a sample of the original population, through a process called sampling.Keep in mind that there‚Äôs something called sampling error, and for a few reasons.Take a look at the chart below.Sampling Error ‚Äî Publication Age and Count of Books Published for Top 100 Novel Authors in FrenchDo you think that the sample we‚Äôve taken we‚Äôll help us make accurate predictions on the given population?Of course not.But what if we take a sample like this?Sample ‚Äî Publication Age and Count of Books Published for Top 100 Novel Authors in FrenchYou got it.This sample will help us make accurate predictions on the data, as it almost represents the population.We can‚Äôt talk of perfection in an imperfect world, but in statistics, everything is always a ‚Äúhypothesis‚Äù.How do you calculate the minimum sample size to take for the experiment?It all comes down to three factors:How large of a difference you want to detectConfidence levelPower and variabilityYou‚Äôre probably familiar just with the first one.We‚Äôll talk about the other two ones below. Keep reading!Hypothesis Creation and Null HypothesisNow that we found out what sample and population are, let‚Äôs talk about the hypothesis.I remember how boring it was trying to coming up with hypotheses for triangles at high school.In the real world, however, it‚Äôs all much more exciting.Let‚Äôs make an example.You think that changing your product image on your product page caused a lift in % of visitors adding the product to their carts.Using common sense, you‚Äôd use the ‚Äútrue‚Äù hypothesis:‚ÄúThe new product image causes a lift in conversion rate for the visitors-cart segment.‚ÄùIn statistics instead, to make things less confusing, we‚Äôll use the null hypothesis, which it‚Äôs the exact opposite:‚ÄúThe new product image doesn‚Äôt cause any effect on the conversion rate for the visitors-cart segment.‚ÄùThis helps us quite a bit, especially with errors.Think about why Type I and Type II errors are always false and come up with your own hypothesis (null).Write it down and we‚Äôll see if you got it right.Type I and Type II errorsWe‚Äôll now start using R, but let‚Äôs design an experiment first.We want to know whether‚Äúhistory and chemistry scholars are interested in volleyball at the same rates‚Äù.We invite 100 history majors and 100 chemistry majors to join a volleyball team.After one week, we check our subscribers.39% of chemistry majors subscribed, while just 34% of history majors subscribed.Since we‚Äôve taken samples of our populations, we want to know whether this result is accurate enough to predict the behavior of an entire population or it‚Äôs just a sampling error.We analyze our data again and we decide we should keep our null hypothesis since this is a sampling error.In other words:‚ÄúThe subscription rate for history majors is the same as for chemistry majors and any difference is due to sampling error.‚ÄùNow, a better analyst (you just started learning) works on this experiment and he states you‚Äôre wrong.You kept the null hypothesis but in reality, you should have rejected it.Well, your error is a false negative:A false negative happens when you kept the null hypothesis but in reality you should have rejected it. The hypothesis is false.This is called a Type II error.So now your result is changed:‚ÄúThe subscription rate for history and chemistry majors is different and we should reject the null hypothesis.‚ÄùNow let‚Äôs try with another experiment.We want to know whether people that took a Coursera certificate are more likely to get a pay raise or not.The population is around 10000 people, but we took a sample for efficiency.We call 200 people who have taken the certificates and 200 who did not.We then found that 25% of people who have taken the certificate got a pay raise, while just 18% of people who have not taken the certificate got a pay raise.We come up with our null hypothesis:‚ÄúThere‚Äôs no noticeable difference between people who have taken a Coursera certificate and people who did not in their pay raise.‚ÄùYou analyze the data and you found out that there‚Äôs a noticeable difference in the pay raise between people who have taken the certificate and people who have not.You formulate your results in plain words:‚ÄúThere‚Äôs a difference in the pay raise between people who have taken a Coursera certificate and people who have not.‚ÄùAs before, a senior analyst with some 20 years of experience comes and he states you‚Äôre wrong.Why?Because he found a Type I Error.The analyst:‚ÄúYou rejected the null hypothesis but in reality, you should not have done it. Now we‚Äôre in trouble.‚ÄùYou:‚ÄúLet‚Äôs go to that night club you really enjoy tonight‚Ä¶‚ÄùThe analyst:‚ÄúStop it!!‚ÄùAnd he starts crying.after having lunch, you analyze the data and you find that he‚Äôs right.A Type I Error is often called a false positive.As the analyst said, it means your null hypothesis should be kept but in reality, you‚Äôve rejected it.You then formulate the new results in plain words:‚ÄúAfter accurate analysis, there‚Äôs no noticeable difference in the pay raise between people who have taken a Coursera certificate and people who have not. ‚ÄùIn R, these results are found out using the function intersect().Let‚Äôs say that the outcomes of our last experiments are summarized in four vectors of numbers:Actual positiveActual negativeExperiemental positiveExperimental negativeThe first two ones are true positives and negatives.The last ones have been determined by running the experiment.In R:real_positive <- c(2, 5, 6, 7, 8, 10, 18, 21, 24, 25, 29, 30, 32, 33, 38, 39, 42, 44, 45, 47)real_negative <- c(1, 3, 4, 9, 11, 12, 13, 14, 15, 16, 17, 19, 20, 22, 23, 26, 27, 28, 31, 34, 35, 36, 37, 40, 41, 43, 46, 48, 49)experimental_positive <- c(2, 4, 5, 7, 8, 9, 10, 11, 13, 15, 16, 17, 18, 19, 20, 21, 22, 24, 26, 27, 28, 32, 35, 36, 38, 39, 40, 45, 46, 49)experimental_negative <- c(1, 3, 6, 12, 14, 23, 25, 29, 30, 31, 33, 34, 37, 41, 42, 43, 44, 47, 48)To detect a false positive or Type I Error, we‚Äôll intersect the real negative with the experimental positive.Why?Well, you rejected the null hypothesis with your experiment, so you‚Äôve got a positive.In reality, the result is negative.That‚Äôs why.In R:type_i_errors <- intersect(real_negative, experimental_positive)type_i_errorsTo detect a false negative or Type II Error, we‚Äôll intersect the real positive with the experimental negative.Why?Well, you kept the null hypothesis with your experiment, so you‚Äôve got a negative.In reality, the result is positive.That‚Äôs why.In R:type_ii_errors <- intersect(actual_positive, experimental_negative)type_ii_errorsThese lines of codes above will return a vector with elements in common between the first and second vectors taken as inputs.[4  9 11 13 15 16 17 19 20 22 26 27 28 35 36 40 46 49] #Type I[6 25 29 30 33 42 44 47] #Type IINow, let‚Äôs go on with the P-value and Confidence Level.P-Value and Significance LevelThe P-value is the probability of obtaining the difference you saw from a sample if there really isn‚Äôt a difference for all the population.P-values help determine how confident you can be in validating the null hypothesis.Let‚Äôs make an example.Do you remember the history and chemistry majors?We had got a 39% subscription rate to our volleyball team for chemistry majors, and 34% for history majors, with a difference of 5%.We run a test on the experiment and among other data, we find to have a p-value of 4%.This means that we‚Äôd see at least a 5% difference only 4 times out of 100 due to sampling error, given the assumption the null hypothesis is true (that‚Äôs where we start).The significance level is a threshold for your P-value.Conventionally, it‚Äôs been set at 5%, so you‚Äôd have a 5% probability of getting a false positive.When you‚Äôre running an A/B test, this is extremely important.However, sometimes it happens you‚Äôll accept a higher significance level to keep moving quickly.That‚Äôs fine if you don‚Äôt want to lose your shirt.Keep in mind that your P-value:does not tell you that B > Adoes not tell you the probability of making a mistake when you select B over AThese are common misconceptions and important to highlight.Confidence Level and PowerNow that you found the P-value, you can compute the confidence level by subtracting it to 100%P_value <- 4%Confidence_Level = 100% - P_valueprint(Confidence_Level) #96%The main difference between the P-value and the confidence level is in timing:P-value is obtained after you ran the test and indicates the probability of getting a false positiveThe confidence level is set before running the test and affects the confidence interval.Now, since we usually want to be able to reject the null hypothesis, we need to understand our ‚Äúpowerful‚Äù is our test.That‚Äôs why we need to introduce the concept of statistical power.Statical power is:‚Äúthe likelihood that a study will detect an effect, when there is an effect to be detected.‚ÄùAnd it‚Äôs determined by:size of the effect you want to detectsize of the sample usedOutcome #1:The bigger the effect the easier it is to detect.Outcome #2:The bigger the sample size the easier it is to detect.When you have an inaccurate sample size, you‚Äôre likely to get into an underpowered A/B test.That means you don‚Äôt have enough data to determine the result accurately.Your probability of getting a false negative type II error is higher than it should be.Actually, you can overpower an underpowered A/B test to make up for the difference, but not too much.Why?Because by doing that you can actually achieve the opposite outcome, getting a false positive type I error.So how do we regulate?Conventionally, you want to keep your statistical power around 80%, which means there‚Äôs a 20% probability of getting a type II error for your A/B tests.Now let‚Äôs finally do some testing!One Sample T-test and Multiple T-testsSuppose you run a blog and you estimate the average age of your readers to be 30.Yesterday you got 500 visitiros and the average age was 32.Are the visitors older than expected or it‚Äôs just due to sampling error?First, let‚Äôs set a null hypothesis:‚ÄúThe sample belongs to a population with the target mean.‚ÄùOr‚ÄúThe average age between the sample and the population are equal‚ÄùIn R, you can test this using the t.test() function.The t.test() function takes as inputs:Values of your samplethe argument mu, indicating the desired meanexpected_mean, indicating the value of your desired meanLet‚Äôs code this:load(""ages.Rda"")ages # [33 34 29 30 22 39 38 37 38 36 30 26 22 22]ages_mean <- mean(ages)ages_mean #32results <- t.test(ages, mu = 30)results# data:  ages# t = 0.59738, df = 13, p-value = 0.5605# alternative hypothesis: true mean is not equal to 30# 95 percent confidence interval:# 27.38359 34.61641# sample estimates:# mean of x :32The P-value is higher than the significance threshold usually accepted, so this means that we might get a false positive, hence rejecting the hypothesis when we should not.However, in the business world, we can accept a 5‚Äì6% P-value.In this case, we accept the alternative hypothesis, hence the true mean is 32, not 30.Now, let‚Äôs see how t.test() with multiple samples work.You want to compare two samples of your traffic:the average age of last week‚Äôs ordersthe average age of this week‚Äôs ordersYou calculate the means using R:last_week_mean <- mean(last_week)last_week_mean  # 25.44this_week_mean <- mean(this_week)this_week_mean # 29.02And you run the test to find out:results <- t.test(week_1,week_2)results # t = -3.5109, df = 94.554, p-value = 0.0006863# alternative hypothesis: true difference in means is not equal to 0# 95 percent confidence interval:# -5.594299 -1.552718# sample estimates:# mean of x mean of y # 25.44806  29.02157With a very low P-value and the right confidence interval, we can surely state that there‚Äôs a difference between the two sample means and the difference is not due to sampling error.Multiple T-tests and ANOVAWe‚Äôre actually a CRO agency and our client is in a hurry.He wants to speed up the process and he asks you to run multiple t.tests() between three different samples.Your client believes that the P-value stays always the same.However, you know your thing and you explain to him the exact opposite:‚ÄúRunning N t.tests() means you have to subtract to 1 the confidence level multiplied by N times, therefore highly increasing the chance of getting a false positive Type I error.‚ÄùIf your confidence level is 95% and you run 3 tests between 3 samples, then the probability of getting a false positive Type I error is:prob_error = 1 - (0.95*3)prob_error = 0.14 This error is unacceptable in statistics, and you explain it to the client.Now, if your client insists the only way to keep your error probability low is to use ANOVA or Analysis of Variance.In the case you‚Äôre comparing the means, ANOVA tests the null hypothesis that all of the datasets you are considering have the same mean.If you reject the null hypothesis using ANOVA, you're saying that at least one of your sample has a different mean, but it doesn't tell you which one.If you want to know which is the one, you‚Äôll need to perform uni-factorial analysis.In R, the ANOVA function is aov() and it takes as inputs the two vectors of samples and combine them into a new data frame, a table.Let‚Äôs say that you want to test the scores at a given game for each major in your college.Data frame for ANOVAWell, in R you‚Äôll use:results <- aov(score ~ group, data = df_scores)Note: Score ~ group indicates the relationship you want to analyze or how each major relates to the game score.To retrieve the P-value you need, you‚Äôll run the following piece of code:summary(results)In this case, the null hypothesis is that‚Äúall the majors score the same results at the video game‚ÄùIf you reject the null hypothesis, you can confidently state that a pair of datasets is significantly different.As we said though, you do know which ones.You talk with the client and now he finally understands.You can keep going with simple A/B testing, not multivariate A/B testing.FeedbackAwesome, this was the last piece of our article.How did I do?Did you understand the concepts or not?Let me know if you have any questions in the comment section below.For now, enjoy your day!MarcoWritten byMarco BasileCXL Certified Growth Marketer Hunting for a FT PositionFollowConversion OptimizationStatisticsMathRMore from Marco BasileFollowCXL Certified Growth Marketer Hunting for a FT PositionMore From MediumContent Strategist User GuidePhoebe Assenza in rhetoricaRethinking and Predicting the Future of TV AdvertisingAri Lewine in The StartupHarry Potter and The Psychology of KPop FansPrince & Matt in Pop Neuro MagazineHow to Choose Social Channels for Your Brand‚Ää‚Äî‚ÄäPt. 2Mindstream Interactive in Mindstream InteractiveThe 6 Steps of a Successful Product LaunchGeraint Clarke in The Startup5 Ways to Improve Your Newsletter Open RateBram Berkowitz in Better MarketingHow Will Being Forced to Use USB-C Charging Cables Affect Apple‚Äôs Brand?Andrei Tapalaga ‚úíÔ∏è in Better MarketingWhy a Jewellery Ad Sparked Major Outrage in IndiaNitish Menon in Better MarketingAboutHelpLegalGet the Medium app"
My first class of visualisation with R,https://medium.com/@zumaia/my-first-class-of-visualisation-with-r-762ee0ff030e?source=tag_archive---------3-----------------------,"R,Visual,Plots,Ggplot2,Graphics","‚ÄúYou won‚Äôt go to bed without knowing something else‚Äù is a saying that refers to the idea that every day we learn something new.This phrase highlights the nature of our lifelong learning, which is continuous and unstoppable, which increases day by day with small things: a new activity, information we didn‚Äôt know, a different way of looking at things.The saying implies that every day we must increase a little more our knowledge about things, that we must not waste time but use it to know new things.This phrase is generally used to express that we have learned something new. For example, someone tells us that the Atacama Desert in Chile is the driest in the world, and we respond to that, satisfied, ‚Äúyou won‚Äôt go to bed without knowing something else‚Äù. So it is used to indicate that we have learned something new or interesting.Photo by Simon Berger on UnsplashVariations of this saying are ‚Äúyou will not go to bed without knowing one more thing‚Äù, ‚Äúyou will not go to bed without knowing one more thing‚Äù, ‚Äúyou will never go to bed without knowing one more thing‚Äù, or ‚Äúyou will not go to bed without knowing one more thing‚Äù.Today was the first day I was given a master class on graphics and visualization! How the perspective of the data changes when you see it on g≈ïaficas instead of on a table.As an example they have shown us the ‚ÄúdatasauRus‚Äù library.The Datasaurus data packageThis package wraps the awesome Datasaurus Dozen dataset, which contains 13 sets of x-y data. Each sub-dataset has five statistics that are (almost) the same in each case. (These are the mean of x, mean of y, standard deviation of x, standard deviation of y, and Pearson correlation between x and y). However, scatter plots reveal that each sub-dataset looks very different. The dataset is intended to be used to teach students that it is important to plot their own datasets, rather than relying only on statistics.The Datasaurus was created by Alberto Cairo in this great blog post.Datasaurus shows us why visualization is important, not just summary statistics.He‚Äôs been subsequently made even more famous in the paper Same Stats, Different Graphs: Generating Datasets with Varied Appearance and Identical Statistics through Simulated Annealing by Justin Matejka and George Fitzmaurice.In the paper, Justin and George simulate a variety of datasets that the same summary statistics to the Datasaurus but have very different distributions.This package looks to make these datasets available for use as an advanced Anscombe‚Äôs Quartet, available in R as anscombe.Load packagespackages <- c(""datasauRus"",""ggplot2"",""gganimate"")newpack  = packages[!(packages %in% installed.packages()[,""Package""])]if(length(newpack)) install.packages(newpack)a=lapply(packages, library, character.only=TRUE)UsageTo see that statistics are (almost) the same for each sub-dataset, you can use dplyr.The Datasaurus data packageSteph Locke This package wraps the awesome Datasaurus Dozen dataset, which contains 13 sets of x-y data. Each‚Ä¶cran.r-project.orgif(requireNamespace(""dplyr"")){  suppressPackageStartupMessages(library(dplyr))  datasaurus_dozen %>%     group_by(dataset) %>%     summarize(      mean_x    = mean(x),      mean_y    = mean(y),      std_dev_x = sd(x),      std_dev_y = sd(y),      corr_x_y  = cor(x, y)    )}To see that each sub-dataset looks very different, you can draw scatter plots.if(requireNamespace(""ggplot2"")){  library(ggplot2)  ggplot(datasaurus_dozen, aes(x=x, y=y, colour=dataset))+    geom_point()+    theme_void()+    theme(legend.position = ""none"")+    facet_wrap(~dataset, ncol=3)}Let¬¥s make animate!#library(datasauRus) #library(ggplot2) #library(gganimate)p <- ggplot(datasaurus_dozen, aes(x=x,y=y)) +geom_point() +theme_minimal() +transition_states(dataset,3,1) + ease_aes() anim_save(""myfilename.gif"",p)I hope you like it.No matter what books or blogs or courses or videos one learns from, when it comes to implementation everything can look like ‚ÄúOutside the Curriculum‚Äù.The best way to learn is by doing! The best way to learn is by teaching what you have learned!Never give up!See you on Linkedin!Oscar Rojo Mart√≠n - Studing Data Science at Universidad de Deusto - San Sebasti√°n, Basque Country, Spain | LinkedInView Oscar Rojo Mart√≠n's profile on LinkedIn, the world's largest professional community.www.linkedin.comReferences:https://cran.r-project.org/web/packages/datasauRus/vignettes/Datasaurus.htmlWritten byOscar RojoCurrently studing a Master in Data Science. Passionate about learning new skills. Former branch risk analyst. https://www.linkedin.com/in/oscar-rojo-martin/FollowPublic domain.RVisualPlotsGgplot2GraphicsMore from Oscar RojoFollowCurrently studing a Master in Data Science. Passionate about learning new skills. Former branch risk analyst. https://www.linkedin.com/in/oscar-rojo-martin/More From MediumBuild a Simple DL Model Using TensorFlowChan Naseeb in Analytics VidhyaBuild a Trump vs Biden Prediction Model With R From ScratchMatt C in The StartupData Science‚Äôs Evolution, and MineCaroline Clark in The StartupUnderstanding Linear and Polynomial Regression in Few StepsSnigdha Sen in The StartupAccounting for Uncertainty in Predictive OptimizationJo Saakvitne in BCG GAMMAPredictive modeling of  Diabetes DatabaseMurali Ambekar in The Startup3 Steps to Advanced Alerting on Airflow with DatabandJosh Benamram in DatabandCan we judge a movie by its cover?Anelia ValtchanovaAboutHelpLegalGet the Medium app"
