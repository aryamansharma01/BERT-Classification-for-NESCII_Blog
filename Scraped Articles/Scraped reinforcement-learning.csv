title,articleUrls,keywords,text
Actor-Critic With TensorFlow 2.x [Part 1 of 2],https://towardsdatascience.com/actor-critic-with-tensorflow-2-x-part-1-of-2-d1e26a54ce97?source=tag_archive---------0-----------------------,"Machine Learning,Data Science,Reinforcement Learning,Artificial Intelligence,TensorFlow","Photo by David Veksler on UnsplashIn this series of articles, we will try to understand the actor-critic method and will implement it in 3 ways i.e naive AC, A2C without multiple workers, and A2C with multiple workers.This is the first part of the series, we will be implementing Naive Actor-Critic using TensorFlow 2.2. Let us first understand what the actor-critic method is and how it works? Knowing the Reinforce Policy gradient method will be beneficial, you can find it here.Overview:If you have read about Reinforce Policy gradient method than you know that its update rule isUpdate Rule for ReinforceIn the Actor-Critic method, we subtract the baseline from the discounted reward. And the common baseline use for these methods is the state value function. So our update rule for actor-critic will look like the following.Actor-Critic update ruleIn Actor-Critic Methods, we have two neural networks namely Actor and a critic. The actor is used for action selection and Critic is used to calculate state value. If you look at the update equation then you can notice that state value is being used as a baseline. Having a baseline helps to determine if an action taken was bad/good or it was the state that was bad/good. You can find very good resources for theory purposes in the reference section.Naive Actor-Critic:In this implementation, we will be updating our neural networks on each timestamp. This implementation differs from A2C where we update our network after every n timestamp. We will implement A2C in the next part of this series.Neural Networks:The neural network can be implemented basically in two ways.One Network for both Actor and Critic functionalities i.e one network with two output layers one for state value and another one for action probabilities.Separate networks, one for actor and another for a critic.We will be using Separate networks for Actor and Critic in this article because I find this one to learn quickly.Code:Actor and Critic Networks:Critic network output one value per state and Actor’s network outputs the probability of every single action in that state.Here, 4 neurons in the actor’s network are the number of actions.Note that Actor has a softmax function in the out layer which outputs action probabilities for each action.Note: number of neurons in hidden layers are very important for the agent learning and vary from environment to environment.Agent class’s init method:Here, we initialize optimizers for our networks. Please note that the learning rate is also important and can vary from the environment and method used.Action Selection:This method makes use of the TensorFlow probabilities library.Firstly, Actor gives out probabilities than probabilities are turned into a distribution using the TensorFlow probabilities library, and then an action is sampled from the distribution.Learn function and losses:We will be making use of the Gradient Tape technique for our custom training.Actor loss is negative of Log probability of action taken multiplied by temporal difference used in q learning.For critic loss, we took a naive way by just taking the square of the temporal difference. You can use the mean square error function from tf2 if you want but then u need to do some modification to temporal difference calculation. We will be using MSE in the next part of this series, so don’t worry.You can find more about the custom training loop at TensorFlow official website.Note: Make sure that you call networks inside with statement (context manager) and only use tensors for the network predictions, Otherwise you will get an error regarding no gradient provided.Trining loop:The agent takes action in environment and then bot networks are updates.For the Lunar lander environment, this implementation performs well.Note: what I noticed while implementing these methods is that the Learning rate and neurons in hidden layers hugely affect the learning.You can find the full code for this article here. Stay tuned for upcoming articles where we will be implementing A2C with and without multiple workers.The Second Part of this series can be accessed here.So, this concludes this article. Thank you for reading, hope you enjoy and was able to understand what I wanted to explain. Hope you read my upcoming articles. Hari Om…🙏References:Reinforcement Learning, Second EditionThe significantly expanded and updated new edition of a widely used text on reinforcement learning, one of the most…mitpress.mit.eduIntuitive RL: Intro to Advantage-Actor-Critic (A2C)Reinforcement learning (RL) practitioners have produced a number of excellent tutorials. Most, however, describe RL in…hackernoon.comAn intro to Advantage Actor Critic methods: let's play Sonic the Hedgehog!by Thomas Simonini An intro to Advantage Actor Critic methods: let's play Sonic the Hedgehog! Since the beginning of…www.freecodecamp.orgMachine Learning with PhilHowdy! At Neuralnet.ai we cover artificial intelligence tutorials in a variety of topics, ranging from reinforcement…www.youtube.comWritten byAbhishek Suran(Cyber Security + Machine Learning)Follow49 Sign up for The Daily PickBy Towards Data ScienceHands-on real-world examples, research,  tutorials, and cutting-edge techniques delivered Monday to Thursday. Make learning your daily ritual. Take a lookGet this newsletterBy signing up, you will create a Medium account if you don’t already have one. Review our Privacy Policy for more information about our privacy practices.Check your inboxMedium sent you an email at  to complete your subscription.49 49 Machine LearningData ScienceReinforcement LearningArtificial IntelligenceTensorFlowMore from Towards Data ScienceFollowA Medium publication sharing concepts, ideas, and codes.Read more from Towards Data ScienceMore From Medium5 YouTubers Data Scientists And ML Engineers Should Subscribe ToRichmond Alake in Towards Data Science7 Must-Haves in your Data Science CVElad Cohen in Towards Data Science21 amazing Youtube channels for you to learn AI, Machine Learning, and Data Science for freeJair Ribeiro in Towards Data ScienceThe Roadmap of Mathematics for Deep LearningTivadar Danka in Towards Data Science30 Examples to Master PandasSoner Yıldırım in Towards Data ScienceAn Ultimate Cheat Sheet for Data Visualization in PandasRashida Nasrin Sucky in Towards Data ScienceHow to Get Into Data Science Without a DegreeTerence S in Towards Data ScienceHow To Build Your Own Chatbot Using Deep LearningAmila Viraj in Towards Data ScienceAboutHelpLegalGet the Medium app"
Reinforcement Learning on Ball Balancer Game from Unity,https://medium.com/@aniruddha.choudhury94/reinforcement-learning-on-ball-balancer-game-from-unity-aaacb118e48a?source=tag_archive---------1-----------------------,"Reinforcement Learning,Augmented Reality,Artificial Intelligence,Machine Learning,Python","Designing and deploying cutting-edge AI solutions for manufacturing environments is a complicated process.Let’s see how we can build an RL environment for 3D Ball balancer.ReinForcement Learning:RL is a more complex and challenging method to be realized, but basically, it deals with learning via interaction and feedback, or in other words learning to solve a task by trial and error, or in other-other words acting in an environment and receiving rewards for it.ReinForcement LearningAgent — the learner and the decision maker.Environment — where the agent learns and decides what actions to perform.Action — a set of actions which the agent can perform.State — the state of the agent in the environment.Reward — for each action selected by the agent the environment provides a reward. Usually a scalar value.Policy — the decision-making function (control strategy) of the agent, which represents a mapping from situations to actions.Value function — mapping from states to real numbers, where the value of a state represents the long-term reward achieved starting from that state, and executing a particular policy.Function approximator — refers to the problem of inducing a function from training examples. Standard approximators include decision trees, neural networks, and nearest-neighbor methodsMarkov decision process (MDP) — A probabilistic model of a sequential decision problem, where states can be perceived exactly, and the current state and action selected determine a probability distribution on future states. Essentially, the outcome of applying an action to a state depends only on the current action and state (and not on preceding actions or states).Dynamic programming (DP) — is a class of solution methods for solving sequential decision problems with a compositional cost structure. Richard Bellman was one of the principal founders of this approach.Monte Carlo methods — A class of methods for learning of value functions, which estimates the value of a state by running many trials starting at that state, then averages the total rewards received on those trials.Temporal Difference (TD) algorithms — A class of learning methods, based on the idea of comparing temporally successive predictions. Possibly the single most fundamental idea in all of reinforcement learning.Model — The agent’s view of the environment, which maps state-action pairs to probability distributions over states. Note that not every reinforcement learning agent uses a model of its environmentSTEPS:1> Download Unityhttps://store.unity.com/#plans-individual2> Open Visual Studiogit clone -- branch release_3 https://github.com/Unity-Technologies/ml-agents.git3> Environment SetupMac OS X SetupCreate a folder where the virtual environments will reside $ mkdir ~/python-envsTo create a new environment named sample-env execute $ python3 -m venv ~/python-envs/sample-envTo activate the environment execute $ source ~/python-envs/sample-env/bin/activateUpgrade to the latest pip version using $ pip3 install --upgrade pipUpgrade to the latest setuptools version using $ pip3 install --upgrade setuptoolsTo deactivate the environment execute $ deactivate (you can reactivate the environment using the same activate command listed above)Ubuntu SetupInstall the python3-venv package using $ sudo apt-get install python3-venvFollow the steps in the Mac OS X installation.Windows SetupCreate a folder where the virtual environments will reside md python-envsTo create a new environment named sample-env execute python -m venv python-envs\sample-envTo activate the environment execute python-envs\sample-env\Scripts\activateUpgrade to the latest pip version using pip install --upgrade pipTo deactivate the environment execute deactivate (you can reactivate the environment using the same activate command listed above)Once you build the environment run the following command:pip install mlagentsmlagents-learn5> Install ML-Agent package in UnityClick the Package Manger.And the in Advance section Search Ml-agent and select the version from dropdown 1.0.25> Open UnityOnce you open Unity Create a new project and paste the github below link folder there in AssetsDrag the Folder in Assets. Below is the Screenshothttps://github.com/aniruddhachoudhury/AR-RL-/tree/master/ML-Agents6> Open the 3D Ball Agents in UnityClick the 3DBall agentIt will redirect toAn agent is an autonomous actor that observes and interacts with an environment. In the context of Unity, an environment is a scene containing one or more Agent objects, and, of course, the other entities that an agent interacts with.Note: In Unity, the base object of everything in a scene is the GameObject. The GameObject is essentially a container for everything else, including behaviors, graphics, physics, etc. To see the components that make up a GameObject, select the GameObject in the Scene window, and open the Inspector window. The Inspector shows every component on a GameObject.The first thing you may notice after opening the 3D Balance Ball scene is that it contains not one, but several agent cubes. Each agent cube in the scene is an independent agent, but they all share the same Behavior. 3D Balance Ball does this to speed up training since all twelve agents contribute to training in parallel.7> Training ML-Agent Reinforcement Learning in Virtual EnvironmentAgentThe Agent is the actor that observes and takes actions in the environment. In the 3D Balance Ball environment, the Agent components are placed on the twelve “Agent” GameObjects. The base Agent object has a few properties that affect its behavior:Behavior Parameters — Every Agent must have a Behavior. The Behavior determines how an Agent makes decisions.Max Step — Defines how many simulation steps can occur before the Agent’s episode ends. In 3D Balance Ball, an Agent restarts after 5000 steps.Behavior Parameters : Vector Observation SpaceBefore making a decision, an agent collects its observation about its state in the world. The vector observation is a vector of floating point numbers which contain relevant information for the agent to make decisions.The Behavior Parameters of the 3D Balance Ball example uses a Space Size of 8. This means that the feature vector containing the Agent's observations contains eight elements: the x and z components of the agent cube's rotation and the x, y, and z components of the ball's relative position and velocity.Behavior Parameters : Vector Action SpaceAn Agent is given instructions in the form of a float array of actions. ML-Agents Toolkit classifies actions into two types: continuous and discrete. The 3D Balance Ball example is programmed to use continuous action space which is a a vector of numbers that can vary continuously. More specifically, it uses a Space Size of 2 to control the amount of x and z rotations to apply to itself to keep the ball balanced on its head.Training a new model with Reinforcement LearningWhile we provide pre-trained .nn files for the agents in this environment, any environment you make yourself will require training agents from scratch to generate a new model file. In this section we will demonstrate how to use the reinforcement learning algorithms that are part of the ML-Agents Python package to accomplish this. We have provided a convenient command mlagents-learn which accepts arguments used to configure both training and inference phases.Training the environmentOpen a command or terminal window.Navigate to the folder where you cloned the ml-agents repository. then you should be able to run mlagents-learn from any directory.Run mlagents-learn config/ppo/3DBall.yaml --run-id=first3DBallRun.config/ppo/3DBall.yaml is the path to a default training configuration file that we provide. The config/ppo folder includes training configuration files for all our example environments, including 3DBall.run-id is a unique name for this training session.If you want to run again please run the below command which will restart the training.mlagents-learn config/ppo/3DBall.yaml --force --run-id=first3DBallRunWhen the message “Start training by pressing the Play button in the Unity Editor” is displayed on the screen, you can press the Play button in Unity to start training in the Editor.If mlagents-learn runs correctly and starts training, you should see something like this:Note how the Mean Reward value printed to the screen increases as training progresses. This is a positive sign that training is succeeding.8> Observing Training ProgressOnce you start training using mlagents-learn in the way described in the previous section, the ml-agents directory will contain a results directory. In order to observe the training process in more detail, you can use TensorBoard. From the command line run:tensorboard --logdir=resultsFor Windows:Instructions For Unity ML-Agents Setup:Install Python2. git clone — branch release_3 git@github.com:Unity-Technologies/ml-agents.git3. Create A New 3D Unity Project4. Go To Package Manager and install the ml-agents5. Copy ml-agents\Project\Assets\ML-Agents to Your Assets Folder6. Virtual Environment Setup7. Activate your virtual environment .\[VirtualEnvName]\Scripts\Activate.ps1 or .bat or just activate8. Type pip install mlagents9. Type mlagents-learn .\config\ppo\3DBall.yaml — run-id=first3DBallRun10. Go back to Unity and hit play and training should beginNext Check out the Train RL Model of 3D Ball Balancer.That’s it for today. Source code can be found on Github. I am happy to hear any questions or feedback. Connect with me at linkdin.Written byAniruddha ChoudhuryMachine Learning|AI|Deep learning|NLP|Senior Datascience Publicis Sapient Connect- http://www.linkedin.com/in/aniruddha-choudhury-5a34b511bFollow2 2 2 Reinforcement LearningAugmented RealityArtificial IntelligenceMachine LearningPythonMore from Aniruddha ChoudhuryFollowMachine Learning|AI|Deep learning|NLP|Senior Datascience Publicis Sapient Connect- http://www.linkedin.com/in/aniruddha-choudhury-5a34b511bMore From MediumGo: What is the Unsafe Package?Vincent Blanchon in A Journey With GoStarting With Google ScriptJesus Najera in OfficeSuite AutomationLessons I learned from music that helped me learn to codeMark Sauer-UtleyScroll Views in XCode 11Luis Mesquita in Mac O’ClockHow to Paint in FlutterSuragchProgressive Web Apps —  One Codebase, Multiple DevicesFerenc Almasi in The StartupImage Processing: Let’s Edit Our Photos with Python!Cindy TehRDKit: Simple File Input and OutputCam KirkAboutHelpLegalGet the Medium app"
Writing Fast Deep Q Learning Pipelines on Commodity Hardware,https://medium.com/analytics-vidhya/writing-fast-deep-q-learning-pipelines-on-commodity-hardware-a3c59cdda429?source=tag_archive---------2-----------------------,"Deep Learning,Reinforcement Learning,Dqn,Ddpg,Q Learning","The state of the art in Deep Reinforcement Learning has been ramping up in scale over time, and its becoming ever more difficult to reproduce the state of art on commodity hardware.Previous works have shown that with enough optimization, patience, and time, we can get pretty close. To this end, I began to study how to write efficient training pipelines with the goal of zero-down time: the GPU must never stall waiting for data (both on the training and inference ends), and pipeline must be able to take on the full throughput the system is capable of.How NOT to Write PipelinesMost DeepRL baselines are written in a synchronous form.There are benefits to this:1) Simplicity: the baseline is just that; a baseline. They typically try to keep the code as simple as possible to get the salient points across. Writing fast code often means making code less readable and more error-prone.2) Benchmarks: Keeping things sequential makes it easy to have good apples-to-apples comparisons on how many [samples | episodes | training-steps | <insert-metric-here> ] algorithms take compared to one another. When these things are simply left to run as fast as they can, disparities can simply be due to how fast one stage can process data compared to the other.But the problem is performance: inference, learning and environment rollouts are all blocking each other because there are data dependencies between them, but prior art has shown these dependencies can be weakened enough to allow us to run them all as separate processes just trying to execute as fast as they can.Isolating the Learner Process: APEXOne exemplar way of isolating the training process is Horgan et al’s APEX DQN.The replay memory is instantiated asynchronously. It provides an API to add to and sample from it from other processes.The Inference & Environment steps still run sequentially in their own process (the “Actor Process”). They queue data into the replay processThe Training process is instantiated asynchronously too. It runs in an infinite loop simply getting minibatches from the Replay as fast as it can and training on them.The Training (or “Learner”) process pipes new network parameters back to the actor process periodicallyThis allows us to treat the training the same way we do for deep learning, and the same pipelining tricks that are standard there apply here too:To ensure the GPU is never stalled waiting for data, we use parallelize data loading in the Trainer process: minibatches from the Replay are asynchronously copied to the GPU while the training step runs so they are always ready for use. We use the same trick for getting parameters off the GPU without slowing training down.[Future Note]: We can also use tricks like virtualization to expand the capacity of the replay network, and pipeline it aggressively so we don’t lose throughput, but that’s a story for another day.While this is a huge faff, it leads to a significant speedup in wall-clock time:Score over Time (Hours): Synchronous Baseline (Orange) vs Asynchronous (Blue) on LunarLander Environment.The speedup comes from both the increase in training speed, and how quickly we gather data. With aggressive pre-fetching and tuning batch-sizes, we can max-out GPU utilization as is for training, but there are still improvements to be made for inference.SampleFactory and SeedRL: Isolating InferenceThe main flaw with APEX is it still needs an instance of the network for every environment, and those still run synchronously. This causes 2 main issues:Memory Waste: it requires multiple instances of the same network, which leaves less room in RAM for the replay buffer.Compute Waste: since it can’t vectorize inference across environments even though in principle the computation it does should allow for it.Latency: If running inference on GPU, it incurs further latency hits for copying observations synchronously to GPU and on all the redundant kernel launches.To get around this, prior art batches the environments synchronously (ie: waits for all environments to return observations then concatenate them into a batch) with so called vectorized environments.These are widely used in on-policy deep RL and get past problems (1) and (2) since they then only require a single network instance to operate on the batch of observations.But these do nothing to alleviate problem 3, and infact worsen it: while they are canonically called vectorized environments, the environments themselves typically aren’t really vectorized at all. They run asynchronously then stop and synchronize when they return observations. This means some complete their steps at different times, and just stall waiting for others to catch up.Two new works (SeedRL by Espeholt et al and Petrenko et al’s Sample Factory) concurrently address this issue with the same general idea:Split the “Actor worker” into 2: the “Policy workers” which are responsible for model inference, and the “Rollout Workers” which are responsible for running the environment simulation:These all run asynchronously, simply processing data as fast as they get it.The Rollout workers send observations into a shared queue to the policy workers.The Policy workers grabs all queued observations and processes them as a batch, then passes the action predictions back to the rollout workersThis allows the environments that finished later to queue their data in parallel while the policy workers are busy with the earlier environments.Further, this allows us to hide the data loading latency by parallelizing it as we do for the trainer run it as another asynchronous pipeline stage.We can then simply tune the number of environments we can run simultaneously and the maximum batch size the model can take with much better scalability, and max out inference throughput as well (… at least until we exceed the capacity of other pipeline stages).Code available here!Analytics VidhyaAnalytics Vidhya is a community of Analytics and Data…Follow70 1 Sign up for Data Science Blogathon: Win Lucrative Prizes!By Analytics VidhyaLaunching the Second Data Science Blogathon – An Unmissable Chance to Write and Win Prizesprizes worth INR 30,000+! Take a lookGet this newsletterBy signing up, you will create a Medium account if you don’t already have one. Review our Privacy Policy for more information about our privacy practices.Check your inboxMedium sent you an email at  to complete your subscription.Deep LearningReinforcement LearningDqnDdpgQ Learning70 claps70 claps1 responseWritten byGershom Agim | llucidFollowEngineer @ Automotive Industry. Focus on Sigproc, Control & MLFollowAnalytics VidhyaFollowAnalytics Vidhya is a community of Analytics and Data Science professionals. We are building the next-gen data science ecosystem https://www.analyticsvidhya.comFollowWritten byGershom Agim | llucidFollowEngineer @ Automotive Industry. Focus on Sigproc, Control & MLAnalytics VidhyaFollowAnalytics Vidhya is a community of Analytics and Data Science professionals. We are building the next-gen data science ecosystem https://www.analyticsvidhya.comMore From MediumProfile Builder | Machine learning & fashion in 36 itemsKallirroi Dogani in The ASOS Tech BlogImage Classification using Logistic Regression on the American Sign Language MNISTGryan GalarioFeature Selection: Beyond feature importance?Dor Amir in Fiverr EngineeringCenterNet and Its VariantsNing Guanghan in Analytics VidhyaDocument Translation Using Attention, EAST, and TesseractSwarup Barua in The StartupDistill-BERT: Using BERT for Smarter Text GenerationRohit Pillai in The StartupLinear Regression With Normal Equation Complete Derivation (Matrices)Pratik Shukla in The StartupUsing Deep Learning to Create a Stock Trading BotVictor Sim in Analytics VidhyaLearn more.Medium is an open platform where 170 million readers come to find insightful and dynamic thinking."
Dynamic Programming to Artificial Intelligence: Q-Learning,https://medium.com/swlh/dynamic-programming-to-artificial-intelligence-q-learning-51a189fc0441?source=tag_archive---------3-----------------------,"Machine Learning,Artificial Intelligence,Reinforcement Learning,Q Learning","A failure is not always a mistake, it may simply be the best one can do under the circumstances. The real mistake is to stop trying. — B. F. SkinnerReinforcement learning models are beating human players in games around the world. Huge international companies are investing millions in reinforcement learning. Reinforcement learning in today’s world is so powerful because it requires neither data nor labels. It could be a technique that leads to general artificial intelligence.Supervised and Unsupervised LearningAs a summary, in supervised learning, a model learns to map input to outputs using predefined and labeled data. An unsupervised learning approach teaches a model to cluster and group data using predefined data.Reinforcement LearningHowever, in reinforcement learning, the model receives no data set and guidance, using a trial and error approach.Reinforcement learning is an area of machine learning defined by how some model (called agent in reinforcement learning) behaves in an environment to maximize a given reward. The most similar real-world example is of a wild animal trying to find food in its ecosystem. In this example, the animal is the agent, the ecosystem is the environment, and the food is the reward.Reinforcement learning is frequently used in the domain of game playing, where there is no immediate way to label how “good” an action was, since we would need to consider all future outcomes.Markov Decision ProcessesThe Markov Decision Process is the most fundamental concept of reinforcement learning. There are a few components in an MDP that interact with each other:Agent — the modelEnvironment — the overall situationState — the situation at a specific timeAction — how the agent actsReward — feedback from the environmentMDP NotationAn agent receives information about its current state from the environment, makes an action, and receives a reward. The process repeats. Source: Sutton, R. S. and Barto, A. G. Introduction to Reinforcement LearningTo repeat what was previously discussed in more mathematically formal terms, some notation must be defined.t represents the current time stepS is the set of all possible states, with S_t being the state at time tA is the set of all possible actions, with A_t being the action performed at time tR is the set of all possible rewards, with R_t being the reward received after performing A_(t-1)T is the last time step (the last step happens when a certain condition is reached or t is higher than a value)The process can be written as:The agent receives a state S_tThe agent performs an action A_t based on S_tThe agent receives a reward R_(t+1)The environments transitions into a new state S_(t+1)The cycle repeats for t+1Expected Discounted Return (Making Long-Term Decisions)We discussed that in order for an agent to play a game well, it would need to take future rewards into consideration. This can be described as:G(t) = R_(t+1) + R_(t+2) +… + R_(T), where G(t) is the sum of the rewards the agent expects after time t.However, if T is infinite, in order to to make G(t) converge to a single number, we define the discount rate γ to be a number smaller than 1, and define:G(t) = R_(t+1) + γR_(t+2) +γ²R_(t+2)+…This can also be written as:G(t) = R_(t+1) + γG(t+1)Value and Quality (Q-Learning is Quality-Learning)A policy describes how an agent will act given any state it finds itself in. An agent is said to follow a policy. Value and Quality functions describe how “good” it is for an agent to be in a state, or a state and perform an action.Specifically, the value function v_p(s) is equal to the expected discounted return while starting in state s and following a policy p. The quality function q_p(s, a) is equal to the best expected discounted return possible while starting in state s, performing action a, and then following policy p.v_p(s) = (G(t) | S_t=s)q_p(s, a) = (G(t) | S_t=s, A_t = a)A policy is better or equal to another policy if it has a greater or equal discounted expected return for every state. The optimal value and quality functions v* and q* use the best possible policy.Bellman Equation for Q*The Bellman Equation another extremely important concept that turns q-learning into dynamic programming combined with a gradient descent-like idea.It states that when following the best policy, the q value of a state and action (q_p(s, a)) is the same as the reward received for performing a during s plus the maximum expected discounted reward after performing a during s multiplied by the discount rate.q*(s_t, a_t) = R_(t+1) + γq*(s_(t+1), a_(t+1))The quality of the best action is equal to the reward plus the quality of best action on the next time step times the discount rate.Once we find q*, we can find the best policy by using q-learning to find the best policy.Q-LearningQ-learning is a technique which attempts to maximize the expected reward over all time steps by finding the best q function. In other words, the objective of q-learning is the same as the objective of dynamic programming, but with the discount rate.In q-learning, a table with all possible state-action pairs is created, and the algorithm iteratively updates all the values of the table using the bellman equation until the optimal q-values are found.We define a learning rate, a number between 0 and 1 describing how much of the old q-value we overwrite and the new one we keep each iteration.The process can be described like with the pseudocode:Q = np.zeros((state_size, action_size))for i in range(max_t):  action = np.argmax(Q[current_state,:])  new_state, reward = step(action)  Q[state, action] = Q[state, action] * (1-learning_rate) + \  (reward + gamma * np.argmax(Q[new_state,:])) * learning_rate  state = new_state  if(game_over(state)):    breakExploration and ExploitationIn the beginning, we do not know anything about our environment, so we want to prioritize exploring and gathering information, even it it means we do not get as much reward as possible.Later, we want to increase our high score and prioritize finding ways to getting more rewards by exploiting the q-table.To do this, we can create the variable epsilon, described by hyperparameters to describe when to explore, and when to exploit. Specifically, when a random number generated is higher than epsilon, we exploit, otherwise, we explore.The new code is as follows:Q = np.zeros((state_size, action_size))epsilon = 1for _ in range(batches):  for i in range(max_t):    if(epsilon > random.uniform(0, 1)):      action = np.argmax(Q[state,:])    else:      action = np.random.rand(possible_actions(state))    new_state, reward = time_step(action)    Q[state, action] = Q[state, action] * (1-learning_rate) + \(reward + gamma * np.argmax(Q[new_state,:])) * learning_rate    state = new_state  epsilon *= epsilon_decay_rate  if(game_over(state)):      breakSummaryReinforcement learning focuses on a situation where an agent receives no data set, and learns from the actions and rewards it receives from the environment.The Markov Decision Process is a control process that models decision making of an agent placed in an environment.The Bellman Equation describes a characteristic that the best policy has that turns the problem into modified dynamic programming.The agent prioritizes exploring in the beginning, but eventually transitions to exploitingThe StartupMedium's largest active publication, followed by +723K people. Follow to join our community.Follow51 Machine LearningArtificial IntelligenceReinforcement LearningQ Learning51 claps51 clapsWritten byMars XiangFollowI talk about math and other topics: youtube.com/channel/UCLeCoh8O6YPQ96HP1ttYyeAFollowThe StartupFollowMedium's largest active publication, followed by +723K people. Follow to join our community.FollowWritten byMars XiangFollowI talk about math and other topics: youtube.com/channel/UCLeCoh8O6YPQ96HP1ttYyeAThe StartupFollowMedium's largest active publication, followed by +723K people. Follow to join our community.More From MediumUse C# and ML.NET Machine Learning To Predict Taxi Fares In New YorkMark Farragher in The Machine Learning AdvantageMRR vs MAP vs NDCG: Rank-Aware Evaluation Metrics And When To Use ThemMoussa Taifi, Ph.D. in The StartupComputer Vision: Lane Finding Through Image ProcessingArchit Rastogi in The StartupMachine Learning with Tree-Based ModelsBlack_Raven (James Ng) in AI In Plain EnglishPerf Machine Learning on Rasp PiGant Laborde in freeCodeCamp.orgMulti-Variate Econometric Model ConfigurationSarit Maitra in The StartupArtificial Bee Colony (ABC) Algorithm — DemystifiedRafiq Awad in The StartupThese Frameworks Have Helped LinkedIn Build Machine Learning at ScaleJesus Rodriguez in The StartupLearn more.Medium is an open platform where 170 million readers come to find insightful and dynamic thinking."
"First they steal our jobs, now they steal our joy.",https://medium.com/@sallyrobotics.blog/first-they-steal-our-jobs-now-they-steal-our-joy-ac44354b7098?source=tag_archive---------4-----------------------,"Reinforcement Learning,Deep Learning,Machine Learning,Q Learning,Deep Q Learning","— By Siddansh Bohra, Navigation Researcher @ Sally Robotics.Photo by Franck V. on UnsplashIntroductionIn 2012, the Arcade Learning environment — a suite of 57 Atari 2600 games (dubbed Atari57) — was proposed as a benchmark set of tasks: these canonical Atari games pose a broad range of challenges for an agent to master. Achieving superhuman performance on these Atari games in 2015 was Deepmind’s rise to fame and was also an important milestone in the development of RL³. Recently Deepmind’s AlphaGo beat the greatest ‘human’ player of all time at Go which is a googol time more complex than chess. Deepmind then developed Starcraft2 which also beat the world champions.“Intelligent behavior arises from the actions⁶ of an individual seeking to maximize its reward⁴ signals in a complex and changing world”-Micheal LittonLet us try to understand one of the simpler papers by Deepmind in which they describe how they achieved superhuman performance in most Atari games.AtariHow they did it?Pre-processing the dataData pre-processing is applied to the images to scale down/up the images, to augment the image to create a larger dataset, to have a uniform aspect ratio for all images.The raw Atari frames, which are 210 × 160-pixel images with a 128-color palette are reduced to the input dimensionality. The raw frames are first converted to a grey-scale from their RGB representation and down-sampled to a 110×84 image. The final input representation is obtained by cropping an 84 × 84 regions of the image that roughly captures the playing area. Pre-processing is applied to the last 4 frames of history and stacked to produce the input to the Q-function.Making the ModelThe input to the neural network consists is an 84 × 84 × 4 image. The first hidden layer convolves 16 8 × 8 filters with stride 4 with the input image and applies a rectifier nonlinearity. The second hidden layer convolves 32 4 × 4 filters with stride 2, again followed by a rectifier nonlinearity. The final hidden layer is fully connected and consists of 256 rectifier units. The output layer is a fully connected linear layer with a single output for each valid action. The number of valid actions varied between 4 and 18 on the games.Fig1: Network architecture. The input to the neural network consists of 84 *84 *4 images produced by the pre-processing map.How does it work though?The goal of the agent⁷ is to select actions that maximize cumulative future reward⁴. The deep convolutional neural network approximates the optimal action-value function at each timestep(t), achievable by a behavior policy⁸, after observing the current state⁵(s) of the agent and taking an action(a). All positive rewards are fixed at 1 and all negative rewards at −1, leaving 0 rewards unchanged.An approximate value function Q(s,a;θi) is parameterized using the deep convolutional neural network shown in Fig. 1, in which θi are weights of the Q-network at iteration i. To perform experience replay⁹, the agent’s experiences et = (st,at,rt,st+1) are stored at each time-step t in a data set Dt = {e1,…,et}. During learning Q-learning updates are applied on mini-batches of experience (s,a,r,s’) ~ U(D), drawn uniformly at random from the pool of stored samples. The Q-learning update at iteration i uses the following loss function1:θi^- are the target network parameters updated with the Q-network¹² parameters θi.The optimization algorithm used is the RMSProp algorithm with mini-batches of size 32. The behavior policy during training was Ɛ-greedy¹¹ with Ɛ annealed linearly from 1 to 0.1 over the first million frames and fixed at 0.1 thereafter. The network is trained for a total of 10 million frames and used a replay memory of one million most recent frames.Checking if it worksIn supervised learning, it is easy to track the performance of a model during training by evaluating it on the test and validation sets. In reinforcement learning, however, accurately evaluating the progress of an agent during training can be challenging. Since the evaluation metric is the total reward the agent collects in an episode or game averaged over a number of games, it is periodically computed during training. The average total reward metric tends to be very noisy because small changes to the weights of policy can lead to large changes in the distribution of states the policy visits. The leftmost two plots in fig.2 show how the average total reward evolves during training on the games Seaquest and Breakout. Both averaged reward plots are noisy, giving the impression that the learning algorithm is not making steady progress.A more stable, metric is the policy’s estimated action-value¹⁰ function Q, which provides an estimate of how much discounted reward the agent can obtain by following its policy from any given state. A fixed set of states is collected by running a random policy before training starts and tracking the average of the maximum(the max for each state is taken over possible actions) predicted Q for these states. The two rightmost plots in fig.2 show that average predicted Q increases much more smoothly than the average total reward obtained by the agent and plotting the same metrics on the other five games produces similarly smooth curves.This suggests that, despite lacking any theoretical convergence guarantees, this method can train large neural networks using a reinforcement learning signal and stochastic gradient descent in a stable manner. This approach robustly learns successful policies over a variety of games based solely on sensory inputs with only very minimal prior knowledge.Figure2: Training curves tracking the agent’s average score and average predicted action-value. a, each point is the average score achieved per episode after the agent is run with e-greedy policy (e = 0.05) for 520 k frames on Space Invaders. b, Average score achieved per episode for Seaquest. c, Average predicted action-value on a held-out set of states on Space Invaders. Each point on the curve is the average of the action-value Q computed over the held-out set of states. d, Average predicted action-value on Seaquest. See Supplementary Discussion for details.FlexingThis DQN method outperformed the best existing reinforcement learning methods on 43 of the games without incorporating any of the additional prior knowledge about Atari 2600 games used by other approaches. The DQN agent performed at a level that was comparable to that of a professional human game tester across the set of 49 games, achieving more than 75% of the human score on more than half of the games (29 games).Figure3: The performance of DQN is normalized with respect to a professional human game tester (that is, 100% level) and random play (that is, 0% level). The normalized performance of DQN, expressed as a percentage, is calculated as 100 * (DQN score — random play score)/(human score — random play score).Other interesting facts about this paperDuring the training of this DQN agent on the breakout, it was observed that it did not make any intelligent moves in the first 10 mins of playing and lost all its lives very fast. At the 120-minute mark, it achieved human-level performance in most games and continued improvements to achieve superhuman level performance after about 2 hours of training.After presenting their initial results with the algorithm, Google almost immediately acquired the company for several hundred million dollars, hence the name Google DeepMind. Google bought London-based artificial intelligence company DeepMind for more than $500 million.The AI developed an interesting strategy to beat the game breakout. The AI started tunneling so that it could hit the bricks from behind.AI playing BreakoutAs you observe from fig.3, the DQN failed to perform well on a few games. Agent57 changes this and is the most general agent in Atari57(the suite of 57 Atari games). Agent57 obtains above human-level performance on the very hardest games in the benchmark set, as well as the easiest ones.Agent57 vs other agents on the toughest Atari games.ConclusionThis paper introduced a new deep learning model for reinforcement learning and demonstrated its ability to master difficult control policies for Atari 2600 computer games, using only raw pixels as input. Players train for 1000s of hours to become professional players but modern reinforcement learning algorithms can beat the best players with just a few hours of training. This paper laid the foundation for Deepmind to go on and beat several world champions at their games.Important terms1. Loss function: It is a method of evaluating how well specific algorithm models the given data. If predictions deviate too much from actual results, loss function would result in a very large number.2. Deep neural net: A deep neural network (DNN) is an artificial neural network (ANN) with multiple layers between the input and output layers. The DNN finds the correct mathematical manipulation to turn the input into the output, whether it be a linear relationship or a non-linear relationship.3. Reinforcement Learning: Reinforcement learning (RL) is an area of machine learning concerned with how software agents ought to take actions in an environment in order to maximize the cumulative reward.4. Reward: A numerical value received by the Agent from the Environment as a direct response to the Agent’s actions. The reward is discounted by a discount function ( γ ) overtime to motivate the agent to take action.The essence of Reinforcement Learning5. State: Every scenario the Agent encounters in the Environment is formally called a state.6. Action: Actions are the Agent’s methods which allow it to interact and change its environment, and thus transfer between states. The decision of which action to choose is made by the policy.7. Agent: Agent is the model that we try to design.8. Policy: The policy, denoted as π (or sometimes π(a|s)), is a mapping from some state s to the probabilities of selecting each possible action given that state.9. Experience replay: Experience replay enables reinforcement learning agents to memorize and reuse past experiences, just as humans replay memories for the situation at hand.10. Action value: this provides an estimate of how much-discounted reward the agent can obtain by following its policy from any given state.11. Ɛ-Greedy approach: In this approach, the agent explores the actions with a probability of Ɛ and exploits the greedy choice with a probability of 1- Ɛ.12. Q-Learning: In its most simplified form, it uses a table to store all Q-Values of all possible state-action pairs possible. It updates this table using the Bellman equation, while action selection is usually made with an ε-greedy policy.SourcesCode: https://sites.google.com/a/ deepmind.com/dqnPaper: https://www.cs.toronto.edu/~vmnih/docs/dqn.pdfhttps://web.stanford.edu/class/psych209/Readings/MnihEtAlHassibis15NatureControlDeepRL.pdfVideo :2-minute papers channel: https://www.youtube.com/channel/UCbfYPyITQ-7l4upoX8nvctgAgent57: https://deepmind.com/blog/article/Agent57-Outperforming-the-human-Atari-benchmarkAI playing BreakoutWritten bySally RoboticsSally Robotics is an ‘Autonomous Vehicles’​ research group by robotics researchers at the Centre for Robotics & Intelligent Systems (CRIS), BITS Pilani.Follow497 497 497 Reinforcement LearningDeep LearningMachine LearningQ LearningDeep Q LearningMore from Sally RoboticsFollowSally Robotics is an ‘Autonomous Vehicles’​ research group by robotics researchers at the Centre for Robotics & Intelligent Systems (CRIS), BITS Pilani.More From MediumWhat GPT-3 Means for Non-Technical ProfessionalsMichael in The StartupArtificial Intelligence and ConsciousnessSubhash KakAR, AI, and Emotional LaborCortney HardingArtificial Intelligence Isn’t as Autonomous Nor Intelligent as You Might ThinkShadow of the ValleyAI Knowledge Map: how to classify AI technologiesFrancesco CoreaThe U.S. Leads in Artificial Intelligence, but for How Long?MIT Technology Review in MIT Technology ReviewGoogle’s Self-Training AI Turns Coders Into Machine-Learning MastersMIT Technology Review in MIT Technology ReviewRobotic Process Automation in ProcurementGaurav Gurkhe in Into Advanced ProcurementAboutHelpLegalGet the Medium app"
"<strong class=""bt"">Scrutinizing Multi-armed Bandits</strong>",https://medium.com/@markosbgoliveira/scrutinizing-multi-armed-bandits-179bbc6a2a47?source=tag_archive---------5-----------------------,"Multi Armed Bandit,Reinforcement Learning,Exploration Exploitation,Ucb","IntroductionConsider the following game. An artificial race car agent is faced with a discrete racetrack such as the one in Figure 1 several times, always beginning at the starting line. At each trial t, it selects a car setup and tries to reach the finish line as fast as possible in a maximum number of moves M. There are K setups to be chosen at every trial. Setups dictate the dynamics of the agent in the racetrack. Some of them are better, helping the agent reach the finish line quickly, while others are not, leading the agent mostly to off-track positions. To help the agent to choose between setups in further trials, a prize for the selected setup is given to the agent in terms of how fast it reached the finish line in the current trial. However, the same setup k∈ K can return distinct prizes in different trials of the game, due to the stochastic nature of the environment. The game’s objective is to minimize the sum of the moves an agent makes to reach the finish line T times.Figure 1. A (successful) path built by a race car agent on a simple racetrack during a trial, where: yellow squares are allowed positions, grey squares are not allowed positions, salmon squares are starting positions, and blue squares are finish positions. The end of the trial occurs either when the agent steps on the finish line or passes through it.The details of the problem will be presented further, but with the given information, think for a second which learning approach would you use to solve this problem.Before suggesting a solution, let’s see some other real-world problems that share the main features of the racetrack problem above.A couple enters a casino with several slot-machines with different expected payoffs. They have a limited number of plays. After each play, they observe the payoff of the machine. To maximize their gains, they focus on the machines whose average return was higher.A doctor treats a patient describing innovative medicines. In future appointments, he (or she) observers the effectiveness of the treatments and start to recommend ones that delivered better results.A mobile app chooses an advertisement category and template to be shown to new users of its services. As user’s data are being gathered and click rates are being observed, the app display ads and templates that maximize the probability of an ad-clicking event.All of the examples share one excel feature: choosing the wrong action hurts: the couple loses money, the doctor loses patients (patients may lose health), and the mobile application loses revenue. Thus, it’s essential to avoid errors when learning how to act in these problems. Notice that it’s not possible to build a database and run a predictor or a classifier from the obtained data because it is assumed that prior data do not exist! Naturally, some prior knowledge can be put into the action-selection mechanism. However, for the problem we’re going to cover, we suppose that there aren’t previous data available, i.e., the agent is embedded in uncharted territory.Another key feature of these problems is the necessity of the agent to explore. The agent doesn’t know which actions are favorable in terms of its objective, even if all actions are tried a few times. That’s because these problems are stochastic in the sense that the same action may return (very) different reward values (the same medical treatment may have different side effects). One action may lead to good rewards just due to randomness and not because it’s inherently suitable. Naturally, probability bounds with confidence levels exist and can be exploited (actually, some MAB agents utilize these bounds to lead its choices). In this sense, to find a good action with a certain level of confidence, the agent needs to explore the available actions by actually trying them.On the contrary, because the agent regrets wrong action selections, if one action is estimated to be very good, the agent should exploit it, keeping exploration to a minimum. Harshly decreasing exploration is not a good strategy, though. Imagine being a doctor having to choose a single treatment to be applied permanently from now on, given the observed results on a disease. If the requirement used to make this decision is soft, the doctor may prematurely declare a suboptimal treatment as the winner, discarding the best one. Hence, the lack of exploration may lead to suboptimal choices and low future rewards. This dilemma is known as the exploration-exploitation tradeoff, and in simple words, there’s no clear answer on how to balance these strategies in a novel problem. Just keep in mind that a good learning algorithm should consider both exploration and exploitation.Multi-armed BanditsA simple and yet effective solving methodology for the problems we’ve briefly visited is called Multi-armed Bandits (MAB) (naturally, that’s not the unique computational approach to deal with such problems). Multi-armed Bandits (MAB) is a solution method to solve the problem of discovering how to act in a context that’s repeatedly shown to an artificial or real-world agent. The idea is to “intelligently” learn how to choose actions sampled from a fixed set of actions to maximize a performance value that we receive after any attempt. It is the most straightforward framework of the reinforcement learning paradigm. In this post, I will explain the MAB approach in a simple problem formulation.By the way, I intend to make a series in which I will cover several reinforcement learning (RL) algorithms in the same context problem you will see here, so you can point out the differences between the learning approaches, understand their main components, assumptions, limitations, and benefits. The content presented here (as well as the problem example we will tackle) is based on the 2nd edition of the book Reinforcement Learning — An Introduction, written by Richard Sutton and Andrew Barto.Briefly, a MAB problem can be formulated as follows. At each episode or trial t=1,2,.., T, where T is a fixed number of episodes, an action a is chosen among the K available actions, that’s a = At. A reward, denoted by Rt, which is a function of the action taken, is observed. The objective of the algorithm is to maximize the total reward over the T available steps. It’s assumed that rewards are stochastic IID rewards: rewards are independently sampled from fixed and unknown reward distributions for each action across episodes.We’re going to cover action-value MABs, which basically have two main components:The estimation method. For each action a and at any trial t, the algorithm will maintain a quantitative estimate Qt(a) of the quality of a (the “value” of a). The strategy the bandit uses to build this value based on the reward observations gathered whenever a is selected, defines its estimation method.The action-selection method. Based on current estimates Qt(a), the action-selection method decides which action should be selected at trial t. Thus, actions are selected according to their current values; that’s why these methods are classified as action-value methods.Following this framework, a pseudo-code of a MAB algorithm is shown in Algorithm 1. As the agent observes the action-reward pairs across the trials, it builds its knowledge about the quality of each action by changing their estimates. That’s why Qt(a) is a function of t. The unknown expected reward of each action is denoted by q*(a), which is the real value of a; q*(a) is unknown, because all aspects of the reward distributions are initially unknown, and fixed, because rewards are assumed to be IIDs. Naturally, we would like to have Qt(a) closer to q*(a).Different MABs strategies define distinct methods for estimation and action-selection. Naturally, both components are essential to a MAB solution. If the estimations are very wrong, the action-selection method will produce poor choices; that’s why blindly following the action with maximum Qt(a) (greedy action-selection) is a bad strategy. For example, if the initial estimates are zero and the rewards are bounded to be positive, the action a=At taken at the first trial will be the best one according to Qt, where t = 1. Selecting this action in all further trials (t = 2,…,T) is not wise because this first estimate is very informative. Not only that, the observed reward Rt (where t=1) could be a very unusual response of a, when compared to its expected reward. All this to say that exploration is ubiquitous in MAB solutions.Let’s now look at the racetrack problem in detail and try to see why a MAB learning algorithm can be useful to this problem.The racetrack problemThe full description of the racetrack problem is given below.A racecar starts at rest in a random position in the starting line of a racetrack built in a discrete space of positions. The car needs to cross the finish line as fast as possible by moving out discretely across the racetrack. An episode or trial t, where t=1,2,…T, refers to the attempt of the car to reach the finish restricted to a maximum number of moves M. An episode starts with the car positioned in a random position in the starting line with both velocities zero. It ends either when the car crosses the finish line or when the maximum number of moves reaches the maximum. If the car hits the boundary in an attempt to step out of the track, it goes back to the starting line. At the end of an episode, a numerical value that measures the performance of the car in the episode is given. This measure is called reward, and it’s defined as Rt=1000/n_moves, where n_moves is the number of moves the agent made in the current episode. counts all moves in the episode, and it doesn’t restart, even when the car goes back to the starting line due to a boundary hit. If the agent did not cross the finish line in the maximum number of moves, it receives a reward equals zero. The reward is then proportional to how fast the agent crosses the finish line.Trial t is an attempt for the agent to reach the finish line. To achieve that, it will discretely move in the racetrack following the rules below (I have omitted the dependence on to simplify the notation):Before any move m, the agent knows its current discrete position (px, py) and velocity (vx, vy) with respect to a reference axis in the grid.It applies on its own a horizontal acceleration Δvx and a vertical acceleration Δvy, whose magnitudes are taken from R={-1, 0, +1} . The acceleration vector (Δvx, Δvy) is called the response vector. At each move, nine (|R|²) possible different responses are possible.It updates its velocity vector to (v’x, v’y), where v’x = vx + Δvx and v’y = vy + Δvy.It deterministically moves to a new position (p’x, p’y), where p’x = px + v’x and p’y = py + v’y.The above process will repeatedly occur at until the end of the episode is triggered. Also, by convention, a positive vertical velocity makes the car go upward while a positive horizontal velocity makes the car go to the right.Regarding moves, two additional problem constraints (rules) exist:Both agent’s velocities values cannot exceed a magnitude of 5, and the vertical velocity cannot be negative (the car cannot go downwards). If that’s the case, the velocity will be maintained at the boundary values.The car cannot stop (having both velocity components zero) in the middle of the track. If a response makes the agent stop, a random velocity component is settled to 1 uniformly at random. That biases the agent to go upward and to the right.At the beginning of this text, I’ve stated that the setup dictates the agent’s dynamics. How does the setup, which is the action chosen by the agent before each trial (not move), fit in this framework? The setup will control how the response vector is generated at each move. More formally, a car setup s is a tuple (f, g), where f is a discrete probability distribution on the set R of possible magnitudes for the vertical acceleration and g is a discrete probability distribution on R for the horizontal acceleration. An example of a setup s is shown below.Figure 2. An example of a setup. The distribution on the left generates vertical velocity changes, while the one on the right generates horizontal velocity changes.The agent’s dynamics during trial t is then controlled by the setup st chosen at the beginning of the episode, because st dictates the probability distributions of the accelerations, which ultimately build the agent’s responses before all moves in trial t. In the MAB formulation, we say that is the action or the arm chosen by the agent at trial t. The game will reward the agent according to the selected setup st, returning a reward value Rt at the end of the episode. No reward is observed for the setups not taken. At the next trial (t+1), the agent can select the same setup again or change among the K available ones.Notice the stochasticity of the problem now. Because the response vectors (Δvx, Δvy) are samples of probabilities distributions, it’s not possible to say with certainty the position of the agent after its first move, let alone the number of moves it will take to complete the racetrack. However, it’s possible to infer something about the expectations of these quantities, i.e., the general trend of the agent’s movement. For example, the setup in Figure 2 is expected to be reasonable for racetracks that goes up and to the left. In this sense, the agent will try to select the setup with the maximum expected reward E[Rt|st]. Setups with higher expected rewards are better because they make the racecar to cross the finish line in less discrete moves in general.In theory, an infinite number of setups exist because continuous probability distributions over the magnitudes in can be defined. To limit the scope of the possible actions, a fixed discrete set D of distributions was established, which is represented in the diagram below.Figure 3: The set D of discrete distributionsEach row of the diagram represents a discrete probability distribution. For example, the distribution #6 is the uniform distribution shown in the left plot of Figure 2, while the right plot corresponds to distribution #9. The set was built in a way that if all distributions were chosen an equal number of times, each acceleration value would be picked with equal probability (the number of cells of each color is the same). In this framework, |D|=10 and there are 100 possible setups (10²). In addition, a setup st can be represented as an ordered pair (ny, nx), where ny denotes the number of the vertical distribution according to the diagram in Figure 3, and nx denotes the number of the horizontal distribution. So, the setup of Figure 2 is (6, 9). Now..Which distribution pair intuitively is the best one in the racetrack in Figure 1 in terms of the reward we have created (crossing the finish line as fast as possible)?A reasonable choice is the setup (1, 4). Distribution #1 is the one that accelerates the car upward most of the time, while distribution #4 is the one that maintains the car at rest in the horizontal direction.The naive approachTo check if setup (1, 4) is indeed better, an agent was tested against the racetrack in Figure 1 100 trials (T=100) for each of the 100 possible setups, according to our problem formulation. The maximum number of moves M was set to 1000. The results for the setups with the highest and lowest ranks concerning the average reward are presented below.Figure 4: Results from the naive approach, in which each setup is tested an equal number of times.Setup (1, 4) was the best one with ease (as we’ve imagined). A reward of 138.70 means that, on average, it was necessary 1000/138.7=7.2 moves for the racecar to cross the finish line. DC is the dominant changeof the setup: it’s the magnitude value from R most likely to be selected for each probability distribution. Notice that setups that mostly go upward and maintains their horizontal velocity are the best ones. On the contrary, the ones that extensively try to brake the car (that car cannot go downwards) and drive the car to a horizontal direction with high probability are the ones with worst results. Also note that bad setups come with small standard deviation, which means that they maintain their bad performances across the episodes and cannot do too much.The experiment we’ve run for identifying the best setup can be beneficial when we have enough computational time to try each possible action a vast number of times, and no negative economic impact exist for choosing bad actions. As we increase the number of trials and, consequently, the reward observations, the average reward will converge to the expected reward for each setup. Having reasonable estimates for the expected rewards for each setup means that we have found how to act in the racetrack, i.e., we found the action (or the setup) that leads to better rewards on average.However, this approach is very naive and can be unfeasible in several real-world problems, such as the ones I’ve mentioned. In these cases, we do not have the luxury of indefinitely trying all possible choices as much as we want and then select the best one. Instead, it’s essential to maximize the agent’s performance as we build knowledge about how to act in the limited trials at our disposal. Consider the advertising problem, for example. Theoretically, we could try each template an equal number of times and then select the one with the highest average reward to be fixed on the app. However, advertising is costly, and we will be losing revenue by selecting very bad performing layouts several times. Look at the average reward of the worst setup in the results above; it just doesn’t worth investing in it.MAB assumptionsTo understand why a MAB solution can be useful to solve the racetrack problem we’ve stated, let’s scrutinize the assumptions of this model as we relate them to the elements of the problem itself.If you have a problem you believe that MAB approaches are likely to be effective on, you should check if all of the conditions below are satisfied before modeling a solution.1. A single state scenario.The agent must face the same scenario (state) in every episode. It selects a single action and receives the reward; when the episode ends, the same state is presented again. In our problem, the car is in the starting line of the same racetrack, it selects a setup and observes the reward. In the next episode, it faces the same racetrack again. Thus, it’s considered to face always the same state.2. A limited number of trials is available.If making exhaustive computational tests are feasible when approaching a problem, testing each action a huge number of times is enough. In practice, this naive approach is often unfeasible. In the racetrack problem, we’ve limited the number of episodes to simulate a restriction in the computational resources.3. A fixed and small set of discrete actions must be available to the agent at every episode.The action set must be discrete and small enough so that every action can be tried at least once. In our problem, we have 100 different setups that can be taken at any episode.4. The actions must significantly impact the observed rewards.The actions must be strong enough to overcome the stochastic nature of the environment. To test whether the actions influence the reward values, we can check if there are differences in the distributions of the rewards received when different actions are selected. Look below the distribution of rewards received by the best setup (highest average reward), the setup with a median average reward (rank 50), and the worst setup (lowest average reward) in the experiment we’ve run previously.Figure 5. Reward distributions of a good, an intermediate, and a lousy setup choice.As we can see, reward distributions are very different. A hypothesis test for the differences between the means would probably confirm that.5. A numerical reward value for the action selected must be available after each episode.A reward signal must be computed after any episode. In our problem, we compute reward values by counting the number of moves the car made in the episode. This information is put into a function that makes sense to the agent in terms of its objective.6. The rewards are assumed to come from stochastic unknown distributions for each action.If the rewards were deterministic, trying each action once would be enough. In our problem, stochasticity comes from the fact that each action (setup choice) defines probability distributions, which changes the path of the car in the racetrack and, ultimately, the observed rewards.7. Different actions must produce different expected rewards.If all different actions produce similar reward values, either the actions are too weak and cannot control the agent in the environment or all actions are equally good, and there’s nothing to search for. The histogram comparison plotted in Figure 5 shows that different distributions indeed produce different rewards in our problem.8. The (inevitably) errors that will occur through the learning by interaction process are affordable.That is especially important for real-world problems where actions involve economic rewards. When we learn from interaction, we must expect mistakes. To gain the benefit of learning satisfactory behaviors, we must afford the regret of making bad decisions due to not knowing which are these behaviors first and foremost.We must make mistakes in order to learn how not to make mistakes.9. The performance of the agent during the learning process is significant.The idea is not just to find the best action to be taken at the given state but to find it in a way that the cumulative reward received during the learning process (across episodes) is maximized.Modeling a MAB solutionLet’s dive into the development of an action-value MAB solution. Specifically, we need to define the estimation method and the action-selection method.A natural and effective way of estimating the value of an arbitrary action a at trial t is by averaging the rewards received at all prior episodes where a was selected. This method is known as the sample-average estimation method. In this case, Qt(a) is estimated as follows:At first sight, all previously observed rewards must be stored so that Qt(a) can be computed. However, an incremental update formula can be derived (see the book for details). The update rule for the estimate of an action a after its nth selection is presented below.Pay attention to the structure of the update:This rule says that the agent is changing its estimates toward the correct target reward values that it’s observing during learning. That’s a very common update rule that plays a critical role in all kinds of learning paradigms, such as in supervised learning. For example, in the learning process of a neural network, it has its parameters changed according to a very similar update rule to the one presented above. In that case, the step size is known as the learning rate, and the target is the correct label for the current input. In addition, instead of estimates, the network learns the right parameters (that frame the input and output relationship of the network) as the data are scanned.With the estimate values at trial t, a naive approach of selecting the action At is to select a with the highest current value. That’s known as the greedy action-selection method:However, as we’ve briefly talked about, it’s not hard to see that such an approach is very poor because the algorithm doesn’t employ any kind of exploration. Remember that exploration is crucial because there is always uncertainty about the accuracy of the current action-value estimates. To solve this issue, we can slightly change the greedy selection approach to another one that mostly selects the action with the highest expected reward, but that also explores with a small probability ε: that’s the ε-greedy action-selection method.This approach is still very simple, but if combined with the sample-average estimation method, it produces a very effective MAB algorithm for stationary MAB problems (with IID rewards and fixed reward distributions). The drawback is that the exploration rate ε is a parameter to be set. The optimal value for ε depends on the problem. A good start tough is to use values from 0.01 (meaning that we explore in 1% of the trials) and 0.10.Below are the comparison results of an experiment contemplating three MAB agents facing the racetrack of Figure 1 with M=1000 and T=10000 (so that the number of trials is the same as in the experiment with the naive approach tested earlier). All MAB agents estimate values according to the sample-average method and select actions according to the ε-greedy action-selection mechanism. Different exploration rates are considered: ε = 0.1, ε = 0.01 and ε = 0.001.Figure 6. Results from a comparison experiment with three e-greedy MAB agents with different exploration rates.The results indicate that all ε-greedy approaches outperformed, in terms of average reward, the naive strategy by far. That’s because these algorithms focused on the selection of actions with higher average reward, instead of naively looking at them (and neglecting the past reward information from the history of observations).According to most results, the best setup on the racetrack of Figure 1 is setup (1, 4), which matches the intuition. Notice, however, that the ε-greedy with ε = 0.001 returned a suboptimal action (setup (2, 7)) with a higher cumulative reward than the naive approach. Nevertheless, in its turn, the naive approach could correctly return setup (1, 4) as the best setup. Now, which agent performed better (and it’s actually preferred)?MAB algorithms are built to reduce the cumulative reward: that’s the unique objective of the algorithm. However, if enough trials are provided to the agent (if T is high enough) by optimizing this objective, one could optimize a second one: finding the best action. This can be done by looking at the last estimates ( Qt(a) where t=T) and declaring the best action a* as:You certainly could use MAB algorithms to find which actions deliver most payoffs in this way, but remember that’s not the inherent purpose of the algorithm. It’s an indirect effect of trying to reduce the cumulative reward. For problems where there’s a negative economic impact of choosing a poor action, such as in medical trials, the average reward is a better metric of ranking algorithms (and parameters). In this sense, the ε-greedy was better, even though it could not rank the estimates according to the (correct) rank of expected rewards, due to a poor choice of the exploration rate.Coming back to the results in Figure 6, the ε-greedy agents appear to be significantly sensitive about the value of ε; an intermediate value of the exploration rate seems to be advantageous. Also, notice that reducing too much the exploration rate profoundly harmed the performance. Let’s look closely at the inner behavior of these MAB across the trials agents by looking at the 1000-trial moving average of observed rewards.Figure 7. The 1000-trial moving average of observed rewards for three e-greedy MAB agents.An increasing moving average means that some learning has occurred: the agent could increase its performance from trial-and-error experience. Results in Figure 7 shows that the two most exploitative agents quickly found a good setup (indeed, the best one), exploiting it as much as possible (approx.) after trial 1000. The difference in the height of both curves after stagnation (~trial 1000) is due to the higher exploration rate of the ε = 0.1 agent. This configuration forces the agent to explore approximately 10% of the trials during the whole search, averaging down the payoffs.In general, the advantage of a higher exploration rate is that it finds more quickly the best action. The disadvantage is that it continues to explore until the end of the search massively. It turns out that investing in exploration at the very ending of the search is not beneficial because the agent will not be able to exploit such action further on in an attempt to increase its average reward. Thus, low exploration rates are suggested in trials near T. Even though the agent with ε = 0.001 has achieved that, it could not reach a stationary level in the T available trials of the experiment. If T continues to increase tough, eventually, this agent will reach a higher stationary level than the others and further, achieve a higher average reward. This indicates that the best agent depends on problem specifications, such as the number of trials T.One could choose ε as being a monotonically decreasing function of t. This makes the agent increase its effectiveness by increasing exploration at the beginning of the search, where the payoff is higher. In general, exponential decay curves are used as such functions.In addition to T, the size of the actions set K also has an impact on agents’ behavior and performance. In the problem we’re looking at, |K|=100, a considerably large set. The agent with ε = 0.01 explores actions 1% of the time. This means that it’s expected the selection of approximately 9 to 10 different actions in the first 1000 trials. If no good action is selected, especially at the beginning of the search, this agent will return poor results. Hence, the sequence of actions taken by the algorithm impacts all of its further choices and, consequently, its results. This explains why the ε = 0.01 agent was the best one according to the last results. It was lucky to have the (1, 4) setup among the ones that were visited. During the experiment, this agent selected exactly ten actions in the first 1000 trials (setup (1, 4) was the sixth one) and 69 actions (out of 100) in total. Thus, the obtained results for this agent do not express its expected behavior being overly optimistic (actually, all results are not trustworthy).A more robust study that considers different independent runs for each agent is performed further so that the effects in the results of the initially selected actions are minimized. However, the unreliability of the last results does not invalidate our early discussion about them, as they’re supported by theory. For example, the ε = 0.01 agent achieved a higher stationary level than the agent with ε = 0.1 the because both of them could find the best setup in the first trials. On the contrary, if only ε = 0.1 agent could find this setup in the first trials (which is the most likely scenario), we would expect it to reach a higher stationary level than the other, as the second one would keep exploiting a poor action choice.Improving MABsAn exciting improvement in the selection mechanism with respect to the ε-greedy method is to consider the number of times Na the action a was called prior to t. That makes sense because as more data on a is observed, more reliable is its estimate (see Hoeffding’s inequality for details). That’s the idea behind the Upper-confidence bound algorithm (or UCB). It still estimates the action-values using average past rewards; however, the action-selection mechanism selects the action with the maximum possible average value according to a confidence level, which considers Na. In particular, the action-selection mechanism of UCB is given by:The first term is the current estimate of the average reward, while the second is the uncertainty of this estimate. Both terms are balanced by a confidence level c > 0 that needs to be set a priori. Together, both terms build an upper bound on the expected reward of a. The algorithm always chooses the action with maximum bound, choosing actions deterministically, as opposed to ε-greedy. Thus, the exploration does not come from intrinsic randomness inherent in the algorithm, but from the estimates and uncertainties, which are themselves random variables.As c is increased, the algorithm favors exploration, because more credit is given to the uncertainty term rather than the current estimate. If the confidence level is minimal, the bound decreases because Qt(a) pretty much approximates the real expectations. Summarizing, UCB prioritizes actions with high current rewards and poor estimates. That’s a simple but yet very effective strategy to handle problems that fit the assumptions we’ve covered. In general, it performs better than ε-greedy, because it leverages exploration in regions where uncertainty is known to be high.Three UCB MAB agents (with c = 0.1, c = 1.0, and c = 10) with the sample-average estimation method were tested against the ε-greedy MABs from the previous experiment. Each agent was tested ten times, with T = 10000 and M = 1000, following the same formulation as before. Multiple tests per approach were performed so that the sensitivity of the approaches with respect to the initially selected actions is decreased. The results averaged over the ten independent runs appear in Figures 8 and 9.Figure 8. Results (averaged over ten runs) from a comparison experiment with e-greedy MABs and UCB MABs.Figure 9. The 1000-trial moving average of 10-average rewards evaluated at each trial of ε-greedy MABs and UCB MABs.Figure 8 shows the average reward of all considering approaches. The average reward values encompass all multiple trials (10000) all multiple runs (10). The “% (1, 4)” column presents the number of times setup (1, 4) was identified by the agent as the best action. By looking at the results, UCB approaches clearly outperformed the ε-greedy ones. One important aspect of UCB agents is their low sensitivity according to the confidence level parameter. That’s a critical feature for problems in which is not possible to validate and test several agents, which is typical of MAB problems. Also, Figure 9 shows that, practically, during the entire search, the UCB agents delivered higher rewards, which makes them first suitable for any T specification.As the ε-greedy approaches could outperform the naive method by using the past-reward information as guidance, the UCB approaches could outperform the e-greedy ones by also considering the number of times each action was called, evolving into a more robust solution.Environmental factors in MAB problemsConsider the following famous framework existent in reinforcement learning (RL) and artificial intelligence in general.Figure 10. Learning by interaction approach (source).Five components can be found here: two main entities that interact with each other, the agent and the environment, and three signals that frame this interaction, the state, the reward, and the action signals. At each trial t, the agent interacts with the environment. The agent is the component that we, as system developers or interpreters, design.The agent learns through experience through trial-and-error following some built-in guidelines. It’s assumed that the agent can significantly modify its environment through its actions. The agent tries to modify the environment in a way that it’s relevant to it: in terms of rewards and (sometimes) states. In the first case, the agent must be able to change the rewards it receives by taking different actions. In more complex problems tough, the agent can also influence its state on the environment. The state is often described by some useful information of the environment at current trial t, often denoted by st (do not confuse with a setup in the racetrack problem, which was an action in the formulation).The racetrack problem (as a MAB problem) was simple in the way that only a single state exists. No setup choice could change the agent’s state because the episode ends immediately after the environment’s response to the selected action. In the next episode, the same state was presented again to the agent, thus st = s, for t = 1, 2, …, T. The generalization of this single state characteristic will the problem toward the standard reinforcement learning problem.Another common characteristic of this learning by interaction approach is that the agent does not know the rewards for actions not taken; it only knows the partial information related to the action just selected. From this partial information, the agent must build knowledge about how to act in its environment in a way that is ‘good’ for it. However, the agent by itself does not know what is good, bad, or anything about its performance. To learn what is good, we, as system developers, must build to the agent a metric that quantifies its performance in an episode, which is the reward signal.In the racetrack problem formulation, it was stated that we would want a racecar that finishes the track as fast as possible; thus, we designed a reward signal that was proportional to that. The reward signal was built to fit what we wanted. None other condition was modeled. For example, the performance of the car was not affected by boundary hits because we did not explicitly state this condition in the reward signal. If that would be important to us, we could, for example, define that each boundary hit would correspond to 100 moves or more, depending on the negative impact of a hit for us compared to a move.In some problems, choosing the reward signal is easily translated to what we want; in others, it’s not. Designing it is especially hard when multiple independent situations impact the performance, and a tradeoff must be postulated; for example, balancing the car’s slowness and boundary hits. In both cases, the reward signal is often chosen intuitively. Just keep in mind that the system will behave very differently depending on how do we choose the reward signal, as it would behave differently as we choose a different set of actions.While we have some control over choosing the reward signal, we often don’t have it on selecting the action set. In a real-world problem, this set is often dictated and represents our available resources to work within the environment. Often, if the actions belong to a continuous set (such as real numbers), a uniform discretization is performed to simplify the problem. That’s the approach we took when we defined the set D of discrete probability distributions.More importantly, the available actions should have a significant impact on the reward value comparing to other environmental factors that we cannot control, and that also affects the reward received. In real-world problems, in general, there are infinitely many things that may happen, and that could impact the reward. Thus, the observed reward at each episode is often considered as being sampled from a fixed and unknown probability distribution, which is a function of the action the agent takes and the state it is. Even in simple problems, an agent may not be able to control itself in a way that impacts the rewards it receives.Consider the racetrack problem with the additional adversity that after the evaluation of the response vector (Δvx, Δvy), there’s a probability p of such response failing, which makes a random response to be applied to the car. If p is high enough, the action of choosing a setup does not have an impact on the rewards because the problem becomes almost entirely stochastic. In this case, we may say that the agent cannot control itself in the environment. That’s an extreme example, but the take away is that we must be sure that the agent immersed in a stochastic environment can change its observations (including the reward values) by choosing its actions. In this sense, as the impact of our action decreases and the impact of other environmental phenomena increases in the reward value, the problem becomes more complex, and more data (episodes) are needed to solve the problem at a reasonable level of cumulative reward.The cardinality of the actions set (|K|) also affects the solution. As we increase the set of actions, we must proportionally increase the number of episodes to test them. There’s no free lunch! Finding the best setup (or action) at each state in a set with 100 options is easy compared to finding the best one in a set with a million options.The framework illustrated in Figure 10 misses one important component of the RL approach, its objective. What is the purpose of an agent embedded in an environment that it knows anything (or very little) about? I like to say that the purpose is to intelligently learn to behave. This objective statement has two parts that I want to decouple and explain in detail.“Learn to behave”: the agent must learn how to behave, which is: to properly select actions given states. By “learning,” I mean that the agent’s original behavior is changed positively (towards higher rewards) as it experiences with the environment.“Intelligently”: the learning process must be intelligent in a way that it should seek for increasing rewards without suffering too much on the process: making too many mistakes or receiving low rewards several times. For example, searching for the best action at random without any guidance can still return a good (or even the best) action if we look at the final estimates. However, it’s not an “intelligent” way of doing it. The agent would probably be hurt too much in the learning process. On the contrary, MAB algorithms learn how to behave and (sometimes) find the best action by explicitly trying to maximize the reward received in the whole process of learning. It’s not just about learning how to behave or finding the best setup as in the problem we’ve tackled; it’s about finding the best behavior intelligently.Final thoughtsMulti-armed bandits is a solution methodology that fit several real-world problems, ranging from medical trials to ad recommendation. The simplest MAB formulation was covered in this article. In this context, the UCB algorithm is a very good candidate as a solution to these problems. Naturally, more complex bandit formulations that outperform the ones covered here exist. However, most of them rely on strong assumptions about the agent-environment relationship, which makes it challenging to apply them to novel problems. That’s in accordance with the Pareto’s Principle: 80% of results come from 20% of the effort. Increasing the complexity of an approach will possibly not increase too much its efficiency. That’s why an intuitive and simple algorithm such as UCB is very useful in practice.It is also worth mentioning that the algorithms we’ve covered are still useful for problems that do not fit precisely the assumptions we’ve stated. For example, MAB algorithms can be easily extended to problems in which the reward distributions for each arm are dynamic across episodes. In this case, the estimation values are given by weighted-averages, whose weights exponentially increase as to how recent the reward sample was observed. Another useful extension is to specify the action-selection mechanism according to side information, called context, that’s given at the beginning of each episode. For example, a doctor can use patient information such as age, pre-existing conditions, and more as a clue for the action-selection mechanism of a MAB approach. In the race game we’ve presented, one may use the starting line position as context, as different setups may perform differently depending on the initial position. That’s said, MAB is a very broad and compelling computation methodology to solve real-world problems with different principles and requirements.Written byMarkos Flavio Bock Gau de OliveiraSon of God, data scientist and ML researcher.Follow6 6 6 Multi Armed BanditReinforcement LearningExploration ExploitationUcbMore from Markos Flavio Bock Gau de OliveiraFollowSon of God, data scientist and ML researcher.More From MediumUsing Machine Learning To Identify Smartphone Users By The Way They WalkSteven Wessels in DVT Software EngineeringExamining Regional Differences by Generating City NamesIsabella BrodyIs stereoscopic 3D vision what Deep Learning needs to generalize modeling of the realityAlan Tan in Data Driven InvestorCreating Genetic Algorithms With Python:Victor Sim in Analytics VidhyaFruit Classification With K-Nearest NeighborsZain Ul Ebad in The StartupRecent Advancements in NLP (1/2)Moiz Saifee in The StartupA quick introduction to Language Models in Natural Language ProcessingDevyanshu ShuklaMachine Learning (ML) Algorithms For Beginners with Code Examples in PythonTowards AI Team in Towards AIAboutHelpLegalGet the Medium app"
Cracking Blackjack — Part 5,https://towardsdatascience.com/cracking-blackjack-part-5-70bd2f726133?source=tag_archive---------0-----------------------,"Cracking Blackjack,Reinforcement Learning,Artificial Intelligence,Data Science,Machine Learning","Hi!If you haven’t done so already, please read Parts 1–4 before continuing. The rest of this article will assume you have read and understood the previous articles.Image from UnsplashOutline for this ArticleLearn how the run_mc() function facilitates the algorithm.Dive deep into Step 6 of the First-Visit MC algorithm, which is where the Q-table and Prob-table are updated after each episode.10,000 ft Overview of First-Visit MC from Part 4Initialize the Blackjack environment from Part 2.Define the Q-table, Prob-table, alpha (α), epsilon (ε), ε-decay, ε-min, and gamma (γ) explained in Part 3.Define how many episodes you would like your agent to learn from. More episodes usually yield a more profitable policy.Play an episode. Record all of the (state → action → reward) tuples in the episode.After the episode, apply ε-decay/ε-min to ε.Then, update the Q-table and the Prob-table using (state → action → reward) tuples from Step 4 and the associated formulas.Repeat Steps 4–6 for the number of episodes defined in Step 3.After all episodes, the resulting Q-table and Prob-table represent the optimized policy for Blackjack that the AI agent just learned.How to Facilitate Steps 4–6In Part 4, we learned how Step 4 is implemented in the play_game() function. Before diving into Steps 5 & 6 in the same level of detail, I would like introduce the run_mc() function, which allows Steps 4–6 to work together.Skim the code below. I will be explaining it in detail below. View the code in its entirety here.Implementing Key Data Structures and Valuesrun_mc() is where the Q-table and Prob-table are explicitly defined.The Q-table stores the relative value (or Q-value) of each action for each possible state. Again, a state is comprised of the player’s hand value and the dealer’s up-card.To represent this in Python, Q is a 2-D list where each index of the outer list corresponds to a unique state, and each inner list is a 2-element list of Q-values for hitting and standing at the state that corresponds to the index.The numpy.zeros() function facilitates this by allowing us to specify the shape of the Numpy array we want to use to represent the Q-table. We also want all Q-values to start at 0 to not bias our agent when it starts to experiment and explore options. numpy.zeros() facilitates this by filling the Numpy array we specify with zeros.As per our Blackjack environment in Part 2, env.observation_space[0].n is 18 for the possible player hand values (3–20), and env.observation_space[1].n is 10 for the possible dealer up-card values (2–11). env.action_space.n is 2 because the only possible actions are hit and stand.Therefore, the number of possible states is env.observation_space[0].n * env.observation_space[1].n, which is 180. This means that the outer list of Q has 180 values. Each of these 180 values in Q is an inner list of zeros of size env.action_space.n (or 2).Q = np.zeros([env.observation_space[0].n * env.observation_space[1].n, env.action_space.n], dtype=np.float16)We define the Prob-table (or prob) in the same exact way, except each value starts at 0.5 (or 50%) instead of 0 to ensure each action has an equal chance of getting selected at the start of the learning process.prob = np.zeros([env.observation_space[0].n * env.observation_space[1].n, env.action_space.n], dtype=np.float16) + 0.5run_mc() is also where the important levers discussed in Part 3 (epsilon, alpha, and gamma) are explicitly defined. The reasoning behind the values assigned to each will be explained in the next article.alpha = 0.001epsilon = 1    decay = 0.9999    epsilon_min = 0.9gamma = 0.8Steps 4–6 Working TogetherThese lines of code encapsulate all of Steps 4–6.for _ in range(num_episodes):    episode = play_game(env, Q, prob)            epsilon = max(epsilon * decay, epsilon_min)            Q = update_Q(env, episode, Q, alpha, gamma)    prob = update_prob(env, episode, Q, prob, epsilon)First, we simulate an episode using the play_game() function as discussed in Part 4. This is Step 4.Then, we apply the decay factor to epsilon, or just use epsilon_min if epsilon has already gone below epsilon_min. This is Step 5.Finally, we update our Q-table and Prob-table using update_Q() and update_prob(). episode and epsilon are crucial arguments for these functions, and they change from episode to episode. This is Step 6.Deep Dive into Step 6Before I continue, congrats on reading this far! You are about to learn the final, and most important, part of this Monte Carlo process. You have come a long way! Take a moment to reflect on that!update_Q()The reason this is called the “First-Visit” MC algorithm is because we have a first-visit approach to valuating rewards.“First-Visit” means that we want to start tracking the rewards of a state-action pair from its first occurrence in the episode, and then use the cumulative rewards of the episode to update our Q-values for that state-action pair in our Q-table.The other option is an “Every Visit” approach. In this approach, we use the immediate rewards of a state-action pair every time it occurs in an episode to update Q-values.It doesn’t make sense to use the every-visit approach for our problem given the format of Blackjack and how we have set up our environment.The rewards for state-action pairs in our episodes are all 0 except for the final state-action pair. This is because Blackjack players (and our agent) may have to make more than one decision (meaning more than one state-action pair) before the results of the Blackjack round (or episode) is known.Therefore, our agent will not improve much if we force it to update the Q-values in the Q-table using immediate rewards of $0 after each state-action pair. Instead, we will use the rewards for the final state-action pair and discount them to approximate the value of a prior state-action pair within the same episode.We will use the following formula to find discounted rewards for state-action pairs in each episode:Image Made by AuthorAn example of calculating discounted rewards was shown in Part 3:Image Made by AuthorA visual example:Image Made by AuthorPutting it together: For each (state → action → reward) tuple in episode, we will use the discounted rewards for the state-action pair to update the corresponding Q-value in the Q-table.Here is the formula for the change in Q-value for each state-action pair encountered in our episode:Image Made by Authorupdate_Q() essentially calculates ⍙Q for each state-action pair seen in our episode, and adds ⍙Q to the current Q-value. The implementation is below.Below is the calculation of discounted rewards for each state-action pair in our episode. step is defined earlier, and is essentially the i subscript seen in our formulae that keeps track of which state-action pair we are analyzing.total_reward = 0gamma_exp = 0for curr_step in range(step, len(episode)):    curr_reward = episode[curr_step][2]    total_reward += (gamma ** gamma_exp) * curr_reward    gamma_exp += 1Below is the calculation of ⍙Q and using that to update the corresponding Q-value in our Q-table.# Update the Q-valueQ_state_index = get_Q_state_index(state)curr_Q_value = Q[Q_state_index][action]Q[Q_state_index][action] = curr_Q_value + alpha * (total_reward - curr_Q_value)At the end of the learning process, we can simply look at our Q-table to see which action has the greater relative value for any given state.update_prob()Please take some more time to digest update_Q(). If you fully understand update_Q(), then update_prob() will be a breeze!To refresh your memory, update_prob() tweaks the probability distribution of taking a hit or stand action for each state encountered in our episode. update_prob() is called after update_Q() finishes because Q-values influence how the probabilities get updated, and we want the most updated Q-values.I’ll start with the code this time, since the formulae for update_prob() are much simpler.In update_prob(), we use the Q-table to find out which action (hit or stand) has the greater Q-value for the state we are updating. This is best_action.First, we tweak the probability of picking best_action in the given state. The formula for ⍙Prob is as follows (where prob[Q_state_index][best_action] is the current probability for picking the best_action):⍙Prob = prob[Q_state_index][best_action] + 1 - epsilonIt is possible for ⍙Prob to exceed 1 (or 100%), so we update the probability in the prob-table like this:prob[Q_state_index][best_action] = min(1, ⍙Prob)Finally, we will update the probability of the other_action based on the new probability of best_action:prob[Q_state_index][other_action] = 1 - prob[Q_state_index][best_action]It is important to update these probabilities because this will impact the explore vs exploit dynamic in our learning process (read more in Part 3).Smaller ⍙Prob values at the start of the learning process ensures that it will take a while before best_action becomes the agent’s dominant choice. Larger ⍙Prob values towards the end will eventually cause the probability of best_action to be 100%, and the agent can start to always take what it thinks is the optimal decision.What’s NextCongrats! You now know all the science behind “cracking” Blackjack! Next, we will dive into the art of experimenting with this algorithm to find the best possible Blackjack returns!Thank you for reading!I would really appreciate feedback of any kind! I want to become a more consistent, effective content creator.Did you learn something? Was I hard to follow?Feel free to leave a comment below or email me at adithyasolai7@gmail.com!GitHub Repo for this project.Written byadithyasolaiJunior CS Student @ UMD College Park | SWE Intern @ Amazon & OcientFollow55 Sign up for The Daily PickBy Towards Data ScienceHands-on real-world examples, research,  tutorials, and cutting-edge techniques delivered Monday to Thursday. Make learning your daily ritual. Take a lookGet this newsletterBy signing up, you will create a Medium account if you don’t already have one. Review our Privacy Policy for more information about our privacy practices.Check your inboxMedium sent you an email at  to complete your subscription.55 55 Cracking BlackjackReinforcement LearningArtificial IntelligenceData ScienceMachine LearningMore from Towards Data ScienceFollowA Medium publication sharing concepts, ideas, and codes.Read more from Towards Data ScienceMore From Medium5 YouTubers Data Scientists And ML Engineers Should Subscribe ToRichmond Alake in Towards Data Science7 Must-Haves in your Data Science CVElad Cohen in Towards Data Science21 amazing Youtube channels for you to learn AI, Machine Learning, and Data Science for freeJair Ribeiro in Towards Data ScienceThe Roadmap of Mathematics for Deep LearningTivadar Danka in Towards Data Science30 Examples to Master PandasSoner Yıldırım in Towards Data ScienceAn Ultimate Cheat Sheet for Data Visualization in PandasRashida Nasrin Sucky in Towards Data ScienceHow to Get Into Data Science Without a DegreeTerence S in Towards Data ScienceHow To Build Your Own Chatbot Using Deep LearningAmila Viraj in Towards Data ScienceAboutHelpLegalGet the Medium app"
Deep Q-Network (DQN)-II,https://towardsdatascience.com/deep-q-network-dqn-ii-b6bf911b6b2c?source=tag_archive---------0-----------------------,"Artificial Intelligence,Reinforcement Learning,Deep Learning,Deep R L Explained,Towards Data Science","This is the second post devoted to Deep Q-Network (DQN), in the “Deep Reinforcement Learning Explained” series, in which we will analyse some challenges that appear when we apply Deep Learning to Reinforcement Learning. We will also present in detail the code that solves the OpenAI Gym Pong game using the DQN network introduced in the previous post.Challenges in Deep Reinforcement LearningUnfortunately, reinforcement learning is more unstable when neural networks are used to represent the action-values, despite applying the wrappers introduced in the previous section. Training such a network requires a lot of data, but even then, it is not guaranteed to converge on the optimal value function. In fact, there are situations where the network weights can oscillate or diverge, due to the high correlation between actions and states.In order to solve this, in this section we will introduce two techniques used by the Deep Q-Network:Experience ReplayTarget NetworkThere are many more tips and tricks that researchers have discovered to make DQN training more stable and efficient, and we will cover the best of them in future posts in this series.Experience ReplayWe are trying to approximate a complex, nonlinear function, Q(s, a), with a Neural Network. To do this, we must calculate targets using the Bellman equation and then consider that we have a supervised learning problem at hand. However, one of the fundamental requirements for SGD optimization is that the training data is independent and identically distributed and when the Agent interacts with the Environment, the sequence of experience tuples can be highly correlated. The naive Q-learning algorithm that learns from each of these experiences tuples in sequential order runs the risk of getting swayed by the effects of this correlation.We can prevent action values from oscillating or diverging catastrophically using a large buffer of our past experience and sample training data from it, instead of using our latest experience. This technique is called replay buffer or experience buffer. The replay buffer contains a collection of experience tuples (S, A, R, S′). The tuples are gradually added to the buffer as we are interacting with the Environment. The simplest implementation is a buffer of fixed size, with new data added to the end of the buffer so that it pushes the oldest experience out of it.The act of sampling a small batch of tuples from the replay buffer in order to learn is known as experience replay. In addition to breaking harmful correlations, experience replay allows us to learn more from individual tuples multiple times, recall rare occurrences, and in general make better use of our experience.As a summary, the basic idea behind experience replay is to storing past experiences and then using a random subset of these experiences to update the Q-network, rather than using just the single most recent experience. In order to store the Agent’s experiences, we used a data structure called a deque in Python’s built-in collections library. It’s basically a list that you can set a maximum size on so that if you try to append to the list and it is already full, it will remove the first item in the list and add the new item to the end of the list. The experiences themselves are tuples of [observation, action, reward, done flag, next state] to keep the transitions obtained from the environment.Experience = collections.namedtuple(‘Experience’,            field_names=[‘state’, ‘action’, ‘reward’,            ‘done’, ‘new_state’])class ExperienceReplay:  def __init__(self, capacity):      self.buffer = collections.deque(maxlen=capacity)  def __len__(self):      return len(self.buffer)  def append(self, experience):      self.buffer.append(experience)    def sample(self, batch_size):      indices = np.random.choice(len(self.buffer), batch_size,                replace=False)      states, actions, rewards, dones, next_states =              zip([self.buffer[idx] for idx in indices])      return np.array(states), np.array(actions),                      np.array(rewards,dtype=np.float32),              np.array(dones, dtype=np.uint8),                 np.array(next_states)Each time the Agent does a step in the Environment, it pushes the transition into the buffer, keeping only a fixed number of steps (in our case, 10k transitions). For training, we randomly sample the batch of transitions from the replay buffer, which allows us to break the correlation between subsequent steps in the environment.Most of the experience replay buffer code is quite straightforward: it basically exploits the capability of the deque library. In the sample() method, we create a list of random indices and then repack the sampled entries into NumPy arrays for more convenient loss calculation.Target NetworkRemember that in Q-Learning, we update a guess with a guess, and this can potentially lead to harmful correlations. The Bellman equation provides us with the value of Q(s, a) via Q(s’, a’) . However, both the states s and s’ have only one step between them. This makes them very similar, and it’s very hard for a Neural Network to distinguish between them.When we perform an update of our Neural Networks’ parameters to make Q(s, a) closer to the desired result, we can indirectly alter the value produced for Q(s’, a’) and other states nearby. This can make our training very unstable.To make training more stable, there is a trick, called target network, by which we keep a copy of our neural network and use it for the Q(s’, a’) value in the Bellman equation.That is, the predicted Q values of this second Q-network called the target network, are used to backpropagate through and train the main Q-network. It is important to highlight that the target network’s parameters are not trained, but they are periodically synchronized with the parameters of the main Q-network. The idea is that using the target network’s Q values to train the main Q-network will improve the stability of the training.Later, when we present the code of the training loop, we will enter in more detail how to code the initialization and use of this target network.Deep Q-Learning AlgorithmThere are two main phases that are interleaved in the Deep Q-Learning Algorithm. One is where we sample the environment by performing actions and store away the observed experienced tuples in a replay memory. The other is where we select the small batch of tuples from this memory, randomly, and learn from that batch using a gradient descent (SGD) update step.These two phases are not directly dependent on each other and we could perform multiple sampling steps then one learning step, or even multiple learning steps with different random batches. In practice, you won’t be able to run the learning step immediately. You will need to wait till you have enough tuples of experiences in D.The rest of the algorithm is designed to support these steps. We can summarize the previous explanations with this pseudocode for the basic DQN algorithm that will guide our implementation of the algorithm:In the beginning, we need to create the main network and the target networks, and initialize an empty replay memory D. Note that memory is finite, so we may want to use something like a circular queue that retains the d most recent experience tuples. We also need to initialize the Agent, one of the main components, which interacts with the Environment.Note that we do not clear out the memory after each episode, this enables us to recall and build batches of experiences from across episodes.Coding the Training LoopHyperparameters and execution timeBefore going into the code, mention that DeepMind’s Nature paper contained a table with all the details about hyperparameters used to train its model on all 49 Atari games used for evaluation. DeepMind kept all those parameters the same for all games, but trained individual models for every game. The team’s intention was to show that the method is robust enough to solve lots of games with varying complexity, action space, reward structure, and other details using one single model architecture and hyperparameters.However, our goal in this post is to solve just the Pong game, a quite simple and straightforward game in comparison to other games in the Atari test set, so the hyperparameters in the paper are are not the most suitable for a didactic post like this one. For this reason, we decided to use more personalized parameter values for our Pong Environment that converges to mean score of 19.0 in a reasonable wall time, depending on the GPU type that colab assigns to our execution (about a couple of hours at most). Remember that we can know the type of GPU that has been assigned to our runtime environment with the command !nvidia-smi.Let’s start introducing the code in more detail. The entire code of this post can be found on GitHub (and can be run as a Colab google notebook using this link). We skip the import details of the packages, it is quite straightforward, and we focus on the explanation of the hyperparameters:DEFAULT_ENV_NAME = “PongNoFrameskip-v4” MEAN_REWARD_BOUND = 19.0 gamma = 0.99                    orbatch_size = 32                 replay_size = 10000             learning_rate = 1e-4            sync_target_frames = 1000        replay_start_size = 10000      eps_start=1.0eps_decay=.999985eps_min=0.02These DEFAULT_ENV_NAME identify the Environment to train on and MEAN_REWARD_BOUNDthe reward boundary to stop training. We will consider that the game has converged when our agent reaches an average of 19 games won (out of 21) in the last 100 games. The remaining parameters indicate:gammais the discount factorbatch_size, the minibatch sizelearning_rateis the learning ratereplay_sizethe replay buffer size (maximum number of experiences stored in replay memory)sync_target_framesindicates how frequently we sync model weights from the main DQN network to the target DQN network (how many frames in between syncing)replay_start_size the count of frames (experiences) to add to replay buffer before starting training.Finally, the hyperparameters related to the epsilon decay schedule are the same as the previous post:eps_start=1.0eps_decay=.999985eps_min=0.02AgentOne of the main components we need is an Agent, which interacts with the Environment, and saves the result of the interaction into the experience replay buffer. The Agent class that we will design already save directly the result of the interacts with the Environment into the experience replay buffer, performing these three steps of the sample phase indicated in the portion of the previous pseudocode:First of all, during the Agent’s initialization, we need to store references to the Environment and experience replay buffer D indicated as an argument in the creation of the Agent’s object as exp_buffer:class Agent:     def __init__(self, env, exp_buffer):        self.env = env        self.exp_buffer = exp_buffer        self._reset()def _reset(self):        self.state = env.reset()        self.total_reward = 0.0In order to perform Agent’s steps in the Environment and store its results in the experience replay memory we suggest the following code:def play_step(self, net, epsilon=0.0, device=”cpu”):    done_reward = None    if np.random.random() < epsilon:       action = env.action_space.sample()    else:       state_a = np.array([self.state], copy=False)       state_v = torch.tensor(state_a).to(device)       q_vals_v = net(state_v)       _, act_v = torch.max(q_vals_v, dim=1)       action = int(act_v.item())The method play_step uses an ϵ-greedy(Q) policy to select actions at every time step. In other words, with the probability epsilon (passed as an argument), we take the random action; otherwise, we use the past model to obtain the Q-values for all possible actions and choose the best.After obtaining the action the method performs the step in the Environment to get the next observation: next_state, reward and is_done:    new_state, reward, is_done, _ = self.env.step(action)    self.total_reward += rewardFinally, the method stores the observation in the experience replay buffer, and then handle the end-of-episode situation:    exp = Experience(self.state,action,reward,is_done,new_state)    self.exp_buffer.append(exp)    self.state = new_state    if is_done:       done_reward = self.total_reward       self._reset()    return done_rewardThe result of the function is the total accumulated reward if we have reached the end of the episode with this step, or None if not.Main LoopIn the initialization part, we create our environment with all required wrappers applied, the main DQN neural network that we are going to train, and our target network with the same architecture. We also create the experience replay buffer of the required size and pass it to the agent. The last things we do before the training loop are to create an optimizer, a buffer for full episode rewards, a counter of frames and a variable to track the best mean reward reached (because every time the mean reward beats the record, we will save the model in a file):env = make_env(DEFAULT_ENV_NAME)net = DQN(env.observation_space.shape,          env.action_space.n).to(device)target_net = DQN(env.observation_space.shape,          env.action_space.n).to(device)buffer = ExperienceReplay(replay_size)agent = Agent(env, buffer)epsilon = eps_startoptimizer = optim.Adam(net.parameters(), lr=learning_rate)total_rewards = []frame_idx = 0best_mean_reward = NoneAt the beginning of the training loop, we count the number of iterations completed and update epsilon as we introduced in the previous post. Next, the Agent makes a single step in the Environment (using as arguments the current neural network and value for epsilon). Remember that this function returns a non-None result only if this step is the final step in the episode. In this case, we report the progress in the console (count of episodes played, mean reward for the last 100 episodes and the current value of epsilon):while True:  frame_idx += 1  epsilon = max(epsilon*eps_decay, eps_min)  reward = agent.play_step(net, epsilon, device=device)  if reward is not None:     total_rewards.append(reward)     mean_reward = np.mean(total_rewards[-100:])     print(“%d: %d games, mean reward %.3f, (epsilon %.2f)” %           (frame_idx, len(total_rewards), mean_reward, epsilon))After, every time our mean reward for the last 100 episodes reaches a maximum, we report this in the console and save the current model parameters in a file. Also, if this mean rewards exceed the specified MEAN_REWARD_BOUND ( 19.0 in our case) then we stop training. The third if, helps us to ensure our experience replay buffer is large enough for training:if best_mean_reward is None or         best_mean_reward < mean_reward:             torch.save(net.state_dict(),                        DEFAULT_ENV_NAME + “-best.dat”)             best_mean_reward = mean_reward             if best_mean_reward is not None:             print(“Best mean reward updated %.3f” %                   (best_mean_reward))if mean_reward > MEAN_REWARD_BOUND:             print(“Solved in %d frames!” % frame_idx)             breakif len(buffer) < replay_start_size:             continueLearn phaseNow we will start to describe the part of the code, from the main loop, that refers to the phase where the network learn (a portion of the previous pseudocode):The whole code that we wrote for implementing this part is as follows:batch = buffer.sample(batch_size) states, actions, rewards, dones, next_states = batchstates_v = torch.tensor(states).to(device)next_states_v = torch.tensor(next_states).to(device)actions_v = torch.tensor(actions).to(device)rewards_v = torch.tensor(rewards).to(device)done_mask = torch.ByteTensor(dones).to(device)state_action_values = net(states_v).gather(1,                           actions_v.unsqueeze(-1)).squeeze(-1)next_state_values = target_net(next_states_v).max(1)[0]next_state_values[done_mask] = 0.0next_state_values = next_state_values.detach()expected_state_action_values=next_state_values * gamma + rewards_vloss_t = nn.MSELoss()(state_action_values,                      expected_state_action_values)optimizer.zero_grad()loss_t.backward()optimizer.step()if frame_idx % sync_target_frames == 0:   target_net.load_state_dict(net.state_dict())We are going to dissect it to facilitate its description since it is probably the most complex part to understand.The first thing to do is to sample a random mini-batch of transactions from the replay memory:batch = buffer.sample(batch_size) states, actions, rewards, dones, next_states = batchNext, the code wraps individual NumPy arrays with batch data in PyTorch tensors and copies them to GPU ( we are assuming that the CUDA device is specified in arguments):states_v = torch.tensor(states).to(device)next_states_v = torch.tensor(next_states).to(device)actions_v = torch.tensor(actions).to(device)rewards_v = torch.tensor(rewards).to(device)done_mask = torch.ByteTensor(dones).to(device)This code inspired by the code of Maxim Lapan. It is written in a form to maximally exploit the capabilities of the GPU by processing (in parallel) all batch samples with vector operations. But explained step by step it can be understood without problems.Then, we pass observations to the first model and extract the specific Q-values for the taken actions using the gather() tensor operation. The first argument to this function call is a dimension index that we want to perform gathering on. In this case, it is equal to 1, because it corresponds to actions dimension:state_action_values = net(states_v).gather(1,                           actions_v.unsqueeze(-1)).squeeze(-1)The second argument is a tensor of indices of elements to be chosen. Here it is a bit more complex to explain the code. Let’s try it!. Maxim Lapan suggest to use the functions unsqueeze() and squeeze(). Because the index should have the same number of dimensions as the data we are processing (2D in our case) it apply a unsqueeze()to the action_v (that is a 1D) to compute the index argument for the gather functions. Finally, to remove the extra dimensions we have created, we will use the squeeze()function. Let’s try to illustrate what a gather does in summary on a simple example case with a batch of four entries and four actions:Note that the result of gather() applied to tensors is a differentiable operation that will keep all gradients with respect to the final loss value.Now that we have calculated the state-action values for every transition in the replay buffer, we need to calculate target “y” for every transition in the replay buffer too. Both vectors are the ones we will use in the loss function. To do this, remember that we must use the target network.In the following code, we apply the target network to our next state observations and calculate the maximum Q-value along the same action dimension, 1:next_state_values = target_net(next_states_v).max(1)[0]Function max() returns both maximum values and indices of those values (so it calculates both max and argmax). Because in this case, we are interested only in values, we take the first entry of the result.Remember that if the transition in the batch is from the last step in the episode, then our value of the action doesn’t have a discounted reward of the next state, as there is no next state from which to gather the reward:next_state_values[done_mask] = 0.0Although we cannot go into detail, it is important to highlight that the calculation of the next state value by the target neural network shouldn’t affect gradients. To achieve this, we use thedetach() function of the PyTorch tensor, which makes a copy of it without connection to the parent’s operation, to prevent gradients from flowing into the target network’s graph:next_state_values = next_state_values.detach()Now, we can calculate the Bellman approximation value for the vector of targets (“y”), that is the vector of the expected state-action value for every transition in the replay buffer:expected_state_action_values=next_state_values * gamma + rewards_vWe have all the information required to calculate the mean squared error loss:loss_t = nn.MSELoss()(state_action_values,                      expected_state_action_values)The next piece of the training loop updates the main neural network using the SGD algorithm by minimizing the loss:optimizer.zero_grad()loss_t.backward()optimizer.step()Finally, the last line of the code syncs parameters from our main DQN network to the target DQN network every sync_target_frames:if frame_idx % sync_target_frames == 0:   target_net.load_state_dict(net.state_dict())And so far the code for the main loop!What is next?This is the second of three posts devoted to present the basics of Deep Q-Network (DQN), in which we present in detail the algorithm. In the next post, we will talk about the performance of the algorithm and also show how we can use it.Deep Reinforcement Learning Explained - Jordi TORRES.AIContent of this seriesWritten byJordi TORRES.AIProfessor at UPC Barcelona Tech & Barcelona Supercomputing Center. Research focuses on Supercomputing & Artificial Intelligence https://torres.ai @JordiTorresAIFollow99 1 Sign up for The Daily PickBy Towards Data ScienceHands-on real-world examples, research,  tutorials, and cutting-edge techniques delivered Monday to Thursday. Make learning your daily ritual. Take a lookGet this newsletterBy signing up, you will create a Medium account if you don’t already have one. Review our Privacy Policy for more information about our privacy practices.Check your inboxMedium sent you an email at  to complete your subscription.99 99 1 Artificial IntelligenceReinforcement LearningDeep LearningDeep R L ExplainedTowards Data ScienceMore from Towards Data ScienceFollowA Medium publication sharing concepts, ideas, and codes.Read more from Towards Data ScienceMore From Medium5 YouTubers Data Scientists And ML Engineers Should Subscribe ToRichmond Alake in Towards Data Science7 Must-Haves in your Data Science CVElad Cohen in Towards Data Science21 amazing Youtube channels for you to learn AI, Machine Learning, and Data Science for freeJair Ribeiro in Towards Data Science30 Examples to Master PandasSoner Yıldırım in Towards Data ScienceThe Roadmap of Mathematics for Deep LearningTivadar Danka in Towards Data ScienceAn Ultimate Cheat Sheet for Data Visualization in PandasRashida Nasrin Sucky in Towards Data ScienceHow to Get Into Data Science Without a DegreeTerence S in Towards Data Science4 Types of Projects You Must Have in Your Data Science PortfolioSara A. Metwalli in Towards Data ScienceAboutHelpLegalGet the Medium app"
Deep Q-Network (DQN)-I,https://towardsdatascience.com/deep-q-network-dqn-i-bce08bdf2af?source=tag_archive---------1-----------------------,"Artificial Intelligence,Reinforcement Learning,Deep Learning,Deep R L Explained,Towards Data Science","In the previous post, we have presented solution methods that represent the action-values in a small table. We referred to this table as a Q-table. In the next three posts of the “Deep Reinforcement Learning Explained” series, we will introduce the reader to the idea of using neural networks to expand the size of the problems that we can solve with reinforcement learning presenting the Deep Q-Network (DQN), that represents the optimal action-value function as a neural network, instead of a table. In this post, we will do an overview of DQN as well as introduce the OpenAI Gym framework of Pong. In the next two posts, we will present the algorithm and its implementation.Atari 2600 gamesThe Q-learning method that we have just covered in previous posts solves the issue by iterating over the full set of states. However often we realize that we have too many states to track. An example is Atari games, that can have a large variety of different screens, and in this case, the problem cannot be solved with a Q-table.The Atari 2600 game console was very popular in the 1980s, and many arcade-style games were available for it. The Atari console is archaic by today’s gaming standards, but its games still are challenging for computers and is a very popular benchmark within RL research (using an emulator)ATARI 2600 (source: Wikipedia)In 2015 DeepMind leveraged the so-called Deep Q-Network (DQN) or Deep Q-Learning algorithm that learned to play many Atari video games better than humans. The research paper that introduces it, applied to 49 different games, was published in Nature (Human-Level Control Through Deep Reinforcement Learning, doi:10.1038/nature14236, Mnih, and others) and can be found here.The Atari 2600 game environment can be reproduced through the Arcade Learning Environment in the OpenAI Gym framework. The framework has multiple versions of each game but for the purpose of this post, the Pong-v0 Environment will be used.We will study this algorithm because it really allows us to learn tips and tricks that will be very useful in future posts in this series. DeepMind’s Nature paper contained a table with all the details about hyperparameters used to train its model on all 49 Atari games used for evaluation. However, our goal here is much more modest: we want to solve just the Pong game.As we have done in some previous posts, the code presented in this post has been inspired by the code of Maxim Lapan who has written an excellent practical book on the subject.The entire code of this post can be found on GitHub and can be run as a Colab google notebook using this link.Our previous examples for FrozenLake, or CartPole, were not demanding from a computation requirements perspective, as observations were small. However, from now on, that’s not the case. The version of code shared in this post converges to a mean score of 19.0 in 2 hours (using a NVIDIA K80). So don’t get nervous during the execution of the training loop. ;-)PongPong is a table tennis-themed arcade video game featuring simple two-dimensional graphics, manufactured by Atari and originally released in 1972. In Pong, one player scores if the ball passes by the other player. An episode is over when one of the players reaches 21 points. In the OpenAI Gym framework version of Pong, the Agent is displayed on the right and the enemy on the left:In Pong, the two paddles move the ball back and forth. The score is kept by the numbers at the top of the screen. (source: torres.ai)There are three actions an Agent (player) can take within the Pong Environment: remaining stationary, vertical translation up, and vertical translation down. However, if we use the method action_space.n we can realize that the Environment has 6 actions:import gymimport gym.spacesDEFAULT_ENV_NAME = “PongNoFrameskip-v4”test_env = gym.make(DEFAULT_ENV_NAME)print(test_env.action_space.n)6Even though OpenAI Gym Pong Environment has six actions:print(test_env.unwrapped.get_action_meanings())[‘NOOP’, ‘FIRE’, ‘RIGHT’, ‘LEFT’, ‘RIGHTFIRE’, ‘LEFTFIRE’]three of the six being redundant (FIRE is equal to NOOP, LEFT is equal to LEFTFIRE and RIGHT is equal to RIGHTFIRE).DQN OverviewAt the heart of the Agent of this new approach, we found a deep neural network instead of a Q-table as we saw in the previous post. It should be noted that the Agent was only given raw pixel data, what a human player would see on screen, without access to the underlying game state, position of the ball, paddles, etc.As a reinforcement signal, it is fed back the change in game score at each time step. At the beginning, when the neural network is initialized with random values, it’s really bad, but overtime it begins to associate situations and sequences in the game with appropriate actions and learns to actually play the game well (that, without a doubt, the reader will be able to verify for himself with the code that will be presented in this series).Input spaceAtari games are displayed at a resolution of 210 by 60 pixels, with 128 possible colors for each pixel:print(test_env.observation_space.shape)(210, 160, 3)This is still technically a discrete state space but very large to process as it is and we can optimize it. To reduce this complexity, it is performed some minimal processing: convert the frames to grayscale, and scale them down to a square 84 by 84 pixel block. Now let’s think carefully if with this fixed image we can determine the dynamics of the game. There is certainly ambiguity in the observation, right? For example, we cannot know in which direction the ball is going). This obviously violates the Markov property.The solution is maintaining several observations from the past and using them as a state. In the case of Atari games, the authors of the paper suggested to stack 4 subsequent frames together and use them as the observation at every state. For this reason, the preprocessing stacks four frames together resulting in a final state space size of 84 by 84 by 4:Input state-space transformation (source: torres.ai)OutputUnlike until now we presented a traditional reinforcement learning setup where only one Q-value is produced at a time, the Deep Q-network is designed to produce in a single forward pass a Q-value for every possible action available in the Environment:(source: torres.ai)This approach of having all Q-values calculated with one pass through the network avoids having to run the network individually for every action and helps to increase speed significantly. Now, we can simply use this vector to take an action by choosing the one with the maximum value.Neural Network ArchitectureThe original DQN Agent used the same neural network architecture, for the all 49 games, that takes as an input an 84x84x4 image.The screen images are first processed by three convolutional layers. This allows the system to exploit spatial relationships, and can sploit spatial rule space. Also, since four frames are stacked and provided as input, these convolutional layers also extract some temporal properties across those frames. Using PyTorch, we can code the convolutional part of the model as:nn.Conv2d(input_shape, 32, kernel_size=8, stride=4),        nn.ReLU(),        nn.Conv2d(32, 64, kernel_size=4, stride=2),        nn.ReLU(),        nn.Conv2d(64, 64, kernel_size=3, stride=1),        nn.ReLU()where input_shape is the observation_space.shape of the Environment.The convolutional layers are followed by one fully-connected hidden layer with ReLU activation and one fully-connected linear output layer that produced the vector of action values:nn.Linear(conv_out_size, 512),         nn.ReLU(),         nn.Linear(512, n_actions)where conv_out_size is the number of values in the output from the convolution layer produced with the input of the given shape. This value is needed to pass to the first fully connected layer constructor and can be hard-coded due it is a function of the input shape (for 84x84 input, the output from the convolution layer will have 3136). However, in order to code a generic model (for all the games) that can accept different input shape, we will use a simple function, _get_conv_out that accepts the input shape and applies the convolution layer to a fake tensor of such a shape:def get_conv_out(self, shape):         o = self.conv(torch.zeros(1, *shape))         return int(np.prod(o.size()))conv_out_size = get_conv_out(input_shape)Another issue to solve is the requirement of feeding convolution output to the fully connected layer. But PyTorch doesn’t have a “flatter” layer and we need to reshape the batch of 3D tensors into a batch of 1D vectors. In our code, we suggest solving this problem in the forward() function, where we can reshape our batch of 3D tensors into a batch of 1D vectors using the view() function of the tensors.The view() function “reshape” a tensor with the same data and number of elements as input, but with the specified shape. The interesting thing of this function is that lets one single dimension be a -1 in which case it’s inferred from the remaining dimensions and the number of elements in the input (the method will do the math in order to fill that dimension). For example, if we have a tensor of shape (2, 3, 4, 6), which is a 4D tensor of 144 elements, we can reshape it into a 2D tensor with 2 rows and 72 columns using view(2,72). The same result could be obtained by view(2,-1), due [144/ (3*4*6) = 2].In our code, actually, the tensor has a batch size in the first dimension and we flatten a 4D tensor (the first dimension is batch size and the second is the color channel, which is our stack of subsequent frames; the third and fourth are image dimensions.)from the convolutional part to 2D tensor as an input to our fully connected layers to obtain Q-values for every batch input.The complete code for class DQN that we just described is written below:import torchimport torch.nn as nnimport numpy as npclass DQN(nn.Module):    def __init__(self, input_shape, n_actions):        super(DQN, self).__init__()self.conv = nn.Sequential(        nn.Conv2d(input_shape[0], 32, kernel_size=8, stride=4),        nn.ReLU(),        nn.Conv2d(32, 64, kernel_size=4, stride=2),        nn.ReLU(),        nn.Conv2d(64, 64, kernel_size=3, stride=1),        nn.ReLU()    )conv_out_size = self._get_conv_out(input_shape)self.fc = nn.Sequential(         nn.Linear(conv_out_size, 512),         nn.ReLU(),         nn.Linear(512, n_actions)    )def _get_conv_out(self, shape):         o = self.conv(torch.zeros(1, *shape))         return int(np.prod(o.size()))def forward(self, x):         conv_out = self.conv(x).view(x.size()[0], -1)         return self.fc(conv_out)We can use the print function to see a summary of the network architecture:DQN(  (conv): Sequential(    (0): Conv2d(4, 32, kernel_size=(8, 8), stride=(4, 4))    (1): ReLU()    (2): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2))    (3): ReLU()    (4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))    (5): ReLU()  )  (fc): Sequential(    (0): Linear(in_features=3136, out_features=512, bias=True)    (1): ReLU()    (2): Linear(in_features=512, out_features=6, bias=True)  ))OpenAI Gym WrappersIn DeepMind’s paper, several transformations (as the already introduced the conversion of the frames to grayscale, and scale them down to a square 84 by 84 pixel block) is applied to the Atari platform interaction in order to improve the speed and convergence of the method. In our example, that uses OpenAI Gym simulator, transformations are implemented as OpenAI Gym wrappers.The full list is quite lengthy and there are several implementations of the same wrappers in various sources. I used the version of Lapan’s Book that is based in the OpenAI Baselines repository. Let’s introduce the code for each one of them.For instance, some games as Pong require a user to press the FIRE button to start the game. The following code corresponds to the wrapper FireResetEnvthat presses the FIRE button in environments that require that for the game to start:class FireResetEnv(gym.Wrapper):   def __init__(self, env=None):       super(FireResetEnv, self).__init__(env)       assert env.unwrapped.get_action_meanings()[1] == ‘FIRE’       assert len(env.unwrapped.get_action_meanings()) >= 3def step(self, action):       return self.env.step(action)def reset(self):       self.env.reset()       obs, _, done, _ = self.env.step(1)       if done:          self.env.reset()       obs, _, done, _ = self.env.step(2)       if done:          self.env.reset()       return obsIn addition to pressing FIRE, this wrapper checks for several corner cases that are present in some games.The next wrapper that we will require is MaxAndSkipEnv that codes a couple of important transformations for Pong:class MaxAndSkipEnv(gym.Wrapper):    def __init__(self, env=None, skip=4):        super(MaxAndSkipEnv, self).__init__(env)        self._obs_buffer = collections.deque(maxlen=2)        self._skip = skipdef step(self, action):        total_reward = 0.0        done = None        for _ in range(self._skip):           obs, reward, done, info = self.env.step(action)           self._obs_buffer.append(obs)           total_reward += reward           if done:               break        max_frame = np.max(np.stack(self._obs_buffer), axis=0)        return max_frame, total_reward, done, infodef reset(self):       self._obs_buffer.clear()       obs = self.env.reset()       self._obs_buffer.append(obs)       return obsOn one hand, it allows us to speed up significantly the training by applying max to N observations (four by default) and returns this as an observation for the step. This is because on intermediate frames, the chosen action is simply repeated and we can make an action decision every N steps as processing every frame with a Neural Network is quite a demanding operation, but the difference between consequent frames is usually minor.On the other hand, it takes the maximum of every pixel in the last two frames and using it as an observation. Some Atari games have a flickering effect (when the game draws different portions of the screen on even and odd frames, a normal practice among Atari 2600 developers to increase the complexity of the game’s sprites), which is due to the platform’s limitation. For the human eye, such quick changes are not visible, but they can confuse a Neural Network.Remember that we already mentioned that before feeding the frames to the neural network every frame is scaled down from 210x160, with three color frames (RGB color channels), to a single-color 84 x84 image using a colorimetric grayscale conversion. Different approaches are possible. One of them is cropping non-relevant parts of the image and then scaling down as is done in the following code:class ProcessFrame84(gym.ObservationWrapper):     def __init__(self, env=None):         super(ProcessFrame84, self).__init__(env)         self.observation_space = gym.spaces.Box(low=0, high=255,                                shape=(84, 84, 1), dtype=np.uint8)def observation(self, obs):         return ProcessFrame84.process(obs)@staticmethod     def process(frame)         if frame.size == 210 * 160 * 3:             img = np.reshape(frame, [210, 160,  3])                                     .astype(np.float32)         elif frame.size == 250 * 160 * 3:             img = np.reshape(frame, [250, 160, 3])                                                   .astype(np.float32)         else:             assert False, “Unknown resolution.”                    img = img[:, :, 0] * 0.299 + img[:, :, 1] * 0.587 +                                           img[:, :, 2] * 0.114             resized_screen = cv2.resize(img, (84, 110),                                          interpolation=cv2.INTER_AREA)             x_t = resized_screen[18:102, :]             x_t = np.reshape(x_t, [84, 84, 1])             return x_t.astype(np.uint8)As we already discussed as a quick solution to the lack of game dynamics in a single game frame, the class BufferWrapper stacks several (usually four) subsequent frames together:class BufferWrapper(gym.ObservationWrapper):    def __init__(self, env, n_steps, dtype=np.float32):        super(BufferWrapper, self).__init__(env)        self.dtype = dtype        old_space = env.observation_space        self.observation_space =                 gym.spaces.Box(old_space.low.repeat(n_steps,                  axis=0),old_space.high.repeat(n_steps, axis=0),                      dtype=dtype)    def reset(self):        self.buffer = np.zeros_like(self.observation_space.low,        dtype=self.dtype)        return self.observation(self.env.reset())def observation(self, observation):        self.buffer[:-1] = self.buffer[1:]        self.buffer[-1] = observation        return self.bufferThe input shape of the tensor has a color channel as the last dimension, but PyTorch’s convolution layers assume the color channel to be the first dimension. This simple wrapper changes the shape of the observation from HWC (height, width, channel) to the CHW (channel, height, width) format required by PyTorch:class ImageToPyTorch(gym.ObservationWrapper):    def __init__(self, env):        super(ImageToPyTorch, self).__init__(env)        old_shape = self.observation_space.shape        self.observation_space = gym.spaces.Box(low=0.0, high=1.0,                                            shape=(old_shape[-1],                                 old_shape[0], old_shape[1]),                                dtype=np.float32)def observation(self, observation):      return np.moveaxis(observation, 2, 0)The screen obtained from the emulator is encoded as a tensor of bytes with values from 0 to 255, which is not the best representation for an NN. So, we need to convert the image into floats and rescale the values to the range [0.0…1.0]. This is done by the ScaledFloatFrame wrapper:class ScaledFloatFrame(gym.ObservationWrapper):     def observation(self, obs):         return np.array(obs).astype(np.float32) / 255.0Finally, it will be helpful for the following simple function make_env that creates an environment by its name and applies all the required wrappers to it:def make_env(env_name):    env = gym.make(env_name)    env = MaxAndSkipEnv(env)    env = FireResetEnv(env)    env = ProcessFrame84(env)    env = ImageToPyTorch(env)     env = BufferWrapper(env, 4)    return ScaledFloatFrame(env)What is next?This is the first of three posts devoted to Deep Q-Network (DQN), in which we provide an overview of DQN as well as an introduction of the OpenAI Gym framework of Pong. In the next two posts (Post 16, Post 17), we will present the algorithm and its implementation, where we will cover several tricks for DQNs to improve their training stability and convergence.Deep Reinforcement Learning Explained by BSC & UPCContent of this seriestorres.aiWritten byJordi TORRES.AIProfessor at UPC Barcelona Tech & Barcelona Supercomputing Center. Research focuses on Supercomputing & Artificial Intelligence https://torres.ai @JordiTorresAIFollow5 Sign up for The Daily PickBy Towards Data ScienceHands-on real-world examples, research,  tutorials, and cutting-edge techniques delivered Monday to Thursday. Make learning your daily ritual. Take a lookGet this newsletterBy signing up, you will create a Medium account if you don’t already have one. Review our Privacy Policy for more information about our privacy practices.Check your inboxMedium sent you an email at  to complete your subscription.5 5 Artificial IntelligenceReinforcement LearningDeep LearningDeep R L ExplainedTowards Data ScienceMore from Towards Data ScienceFollowA Medium publication sharing concepts, ideas, and codes.Read more from Towards Data ScienceMore From Medium5 YouTubers Data Scientists And ML Engineers Should Subscribe ToRichmond Alake in Towards Data Science7 Must-Haves in your Data Science CVElad Cohen in Towards Data Science21 amazing Youtube channels for you to learn AI, Machine Learning, and Data Science for freeJair Ribeiro in Towards Data Science30 Examples to Master PandasSoner Yıldırım in Towards Data ScienceThe Roadmap of Mathematics for Deep LearningTivadar Danka in Towards Data ScienceAn Ultimate Cheat Sheet for Data Visualization in PandasRashida Nasrin Sucky in Towards Data ScienceHow to Get Into Data Science Without a DegreeTerence S in Towards Data Science4 Types of Projects You Must Have in Your Data Science PortfolioSara A. Metwalli in Towards Data ScienceAboutHelpLegalGet the Medium app"
Maze solver using Naive Reinforcement Learning,https://towardsdatascience.com/maze-rl-d035f9ccdc63?source=tag_archive---------2-----------------------,"Machine Learning,Games,Reinforcement Learning,Programming,Artificial Intelligence","This is a short maze solver game I wrote from scratch in python (in under 260 lines) using numpy and opencv. Code link included at the end.The arrows show the learned policy improving with training. Given an agent starts from anywhere, it should be able to follow the arrows from its location, which should guide it to the nearest destination block.I wrote this to understand the fundamentals of Q-Learning and apply the theoretical concepts directly in code from scratch. Follow along if you wanna get your hands dirty with reinforcement learning!Game Objective -Find the optimal movement policy which takes an agent from any starting (shown in black-gray shades on the left) to the closest destination (blue-ish) box while avoiding danger zone (red) and wall (green) boxes.A “policy” can be thought of as the set of “smart-movement” rules which the agent learns to navigate its environment. In this case, they’re visualized as arrows (shown on left). This is done through Q-Learning.Significance -You might ask if making game-playing AIs like these are relevant at all in practical applications and that’s fair. Actually these are toy-problems designed in such a way that, their solutions are broadly applicable.For example, the current example of maze solving can further be extended for autonomous navigation in an occupancy grid to get to the nearest EV charging station.The Q-Learning Algorithm and the Q-Table approach -Q-Learning is centered around the Bellman Equation and finding the q-value for each action at the current state. Finding an optimal policy involves recursively solving this equation multiple times.The Bellman Equation. This can be recursively solved to obtain the “Q-values” or “quality values” of different actions given the agent’s current state.Only the main parts of the Bellman Equation relevant to this implementation will be explained in this article. For a more in-depth primer on the Bellman equation, check reference [1].What is the Q-value?Imagine you are an unfortunate soul stuck in a simple 2D world like the following -Yes, that’s you. You are sad. The orange arrows dictate the displacements you can make in this 2D world.Well, you look sad. You should be. Who wants to be in a 2D world anyway?Well… lets put a smile on that face, shall we? 🎃Given that the only movements you can make are the orange arrows shown in the image on the left (and a no-op operation), you gotta find your way to the nearest exit portal.Given these conditions, at any given stage, you’ll have to make a decision on one of these actions. To do that, your brain does an internal “ranking” of the actions taking many things into consideration. This might include things like -Where is the nearest exit?Are there any danger zones?Where dem walls at boi?Why is it getting hot in here? (We’ll get to this by discussing adding a small -ve reward for every time the agent does nothing)Now you being an advanced human, process these implicitly and assign a quality -value or a “Q-value” to each of the actions (up, down, left, right, no-op) you can take at that point.But how can you make a computer do it?Simple, you somehow assign a numeric q-value to each action at each situation you might encounter. However, this is the naive approach; and as stated in the title, we shall stick to this here. For more advanced stuff, there are tons of other articles where you should be looking.Pretty much like how we humans form perceptions of “good” and “bad” actions based on real-life experiences, the agent has to be trained in a similar way.Now, this brings us to the following question -What is the Q-table?Simply put, this is the memory of experiences per-say you’ll be updating and querying every time you have to make a decision and perform an action in the environment.An accurate visual representation of your relationship with the Q-table is shown on the left.Now, to build the Q-table, you need to collect information about the world. It needs to know of danger zones, walls it could bump in to, and pretty much anything to help you not die soon (much like life itself).To do this, let’s assume you can die a thousand deaths. Yes, sacrifice is necessary for science.Armed with this, you will start at random locations and kind-of begin randomly roaming around until you start forming a perception of the world around you. This perception is shaped by what you encounter while roaming around.You wanna avoid pain. In this sense, actions in situations which lead to -ve rewards. Therefore, you ‘take note of them’ in the Q-table whenever you encounter them.For example, you may hit a wall — that’s bad, cuz you’re bleeding. Now you’ll remember in that situation, whatever action you took which caused you to bleed, shouldn’t be repeated.Sometimes, you’ll even encounter danger zones raging with fire 🔥🧨 which will end your life as soon as you step on them. This is worse than bleeding, which will be quantified by assigning a more -ve reward value for such experiences.Now for the better things in life.Similarly, you’ll also keep track of all the good things (when you receive a +ve reward) which happen during your time in the maze. Well, in this case, there’s only one good thing which can happen - E S C A P E.This just sounds like another way of dying, but hey let’s pretend its more fun cuz it sounds different than death.To do all of this, you’ll basically build a table storing the q-values of performing each and every action in every possible scenario in the environment (do remember that this is naive for a reason).A higher q-value for a given action in a given state means that action will be more likely to be taken by you (the agent).Shown below are two different states with example q-values for each action that can be performed by you (the agent) at those states.In each state, the agent is located in the boxed region in the checkerboard world. For each state, shown to the right are different actions (up, left, right, down, no-op respectively from top to bottom) the agent can take along with their q-values derived from the Q-Table.The q-values then act as a guide towards taking the next action to maximize overall reward (which means escape). At every step, the following actions will be performed sequentially in this naive scenario -Query Q-table for values pertaining to the different actions you can perform at your current state.Take action pertaining to the highest q-value.Record the new state and reward received and use it to update the Q-table using the Bellman Equation. We’ll get here shortly.Go to step 1.Learning VisualizationFinal learned representation of the Q-table rendered visually on to the maze world. It is implemented from scratch in the codebase using numpy.Given all state transition rules are defined (which in this case is quite simple given the basic nature of the maze world), after a sufficient number of repeating these iterations, the agent builds a “vector field map” per-say of the different actions that should be performed at each location of the maze so as to reach the nearest destination in the minimum time.Shown on the left is the final learned representation of the Q-table.The arrows are visualized by obtaining a vector sum of the different q-values at each location. For example, if we have the following q-values for up, left, right, down — qu, ql, qr, qdThen the arrow, on a 2D plane (Horizontal is X-axis, Vertical is Y-axis) will have its x-component as qr-ql and y-component as qd-quThe length of the arrow is the norm of this vector obtained using the following formula -Therefore, if you start at any location in the maze, you can follow the arrows and reach the nearest destination by avoiding walls and danger zones.Updating the Q-Table while exploring the maze -This is one of the more challenging parts of the problem which greatly affects how soon you’ll be getting your sweet release (it’s not death, let’s remember that haha).Basically, here is the question —You take the highest q-value action at your given state following which, you end up in a new state (let’s hope for simplicity you don’t die for now).Next, you’d like to record whether your action has brought you closer to the nearest destination in the Q-table. How could you do this?All you have here to work with are the following -Existing q-values at the new and old states defined for each action. They might have been randomly initialized or obtained from a previous iteration.The reward you gained for the action you performed to get to the new state from the old state.The action you performed to get to the new state from the old state.How would you change the existing Q-table values you obtained for the old state to make a better decision if you come across it in the future?This is the very basic question which is answered by the Bellman equation in this case -The Bellman Equation. This can be recursively solved to obtain the “Q-values” or “quality values” of different actions given the agent’s current state.Following are the variable definitions -a is the action.s and s’ are the old and new states respectively.𝛾 is the discount factor, a constant between 0 and 1. You need this to prioritize current reward over expected future reward.Q(s) is the q-value of the action a you just took to reach the new state from the old state s.Q(s’) is the maximum q-value at the new state s’.R(s, a) is the reward you immediately receive for performing a to transition from s to s’.The max term is the secret sauce here. This causes the equation to iterate through every a until the maximum value of the expression inside the max term is obtained. It finally returns that value q and the corresponding action a.Every action a performed from state s might lead to new states s’ for each iteration. Therefore each time, the maximum of the q-values defined at s’ is chosen to compute the expression inside max.Once the values q and a are obtained, the Q-table value defined for action a at state s is then overwritten by q.In our case, this representation is the value function (don’t worry if you don’t get this; well, I just pulled an Andrew Ng on you 😈).Running the agent in the maze -Finally, you’ve made it here, congrats! Here is an exclusive RL meme for you from my meme page @ml.exe. You deserve it bud.Don’t worry, healthy narcissism won’t kill you.After a sufficient number of iterations of the Bellman equation, you’ll converge to optimum q-values for each action at each state.When you want to run the agent, simply start from any spawn point and blindly do the action with the highest q-value. You’ll reach the nearest destination.However, there are a few caveats to getting this right -Reward policies should be carefully designed. This means correct reward values should be assigned for performing each action at each state. Since this case is so simple, a simple scheme like the following works well -discount_factor = 0.5default_reward = -0.5wall_penalty = -0.6win_reward = 5.0lose_reward = -10.0default_reward is the reward obtained for doing nothing at all. Remember a basic question we asked ourselves in the beginning of this article “Why is it getting hot in here?”; well, here it is. Assigning a small negative reward encourages the agent to seek actions to end its misery rather than sitting around like an obese piece of lard.wall_penalty is the reward received if you bump into a wall while doing the action from your present state. Whenever you bump into a wall, you remain at your original location while receiving this “reward” 🤣.win_reward and lose_reward speak for themselves.You lose a game if you end up on any of the danger zones. Upon dying, you respawn at a randomly chosen location on the grid.In the codebase, you can play around with rewards to see how this affects solution convergence.ConclusionIf you correctly understand the steps cited in this article, you’ll be able to fully understand the codebase I wrote from scratch to implement all of this. You can find it here -ironhide23586/naive-dqn-mazeMaze Solver using Naive Reinforcement Learning with Q-Table construction This is an implementation of the Q-Learning…github.comThe code writes out a video of the agent training and learning as shown in the YouTube video below. You can generate random worlds with varying complexities.If you found this helpful, feel free to follow me for more upcoming articles :)I’m the editor of the following publication which publishes Tech articles related to the usage of AI & ML in digital mapping of the Earth. Feel free to follow to stay updated :)Machine Learning & AI in Digital CartographyCurated cutting edge AI & ML research articles from industry scientists working on Device, Edge, Cloud and Hybrid…medium.comReferences -https://medium.com/analytics-vidhya/bellman-equation-and-dynamic-programming-773ce67fc6a7https://www.instagram.com/ml.exehttps://github.com/ironhide23586https://www.youtube.com/user/mick23586Extras-I’m also a musician. If you dig metal and/or rap — my Soundcloud profile is at https://soundcloud.com/souham-biswasRegular talk-and-gaming live streaming at https://www.twitch.tv/souhamThank you :)Written bySouham BiswasSenior Data Scientist at HERE Maps (a BMW, Audi & Daimler company) | Writer at TowardsDataScience | MusicianFollow20 Sign up for The Daily PickBy Towards Data ScienceHands-on real-world examples, research,  tutorials, and cutting-edge techniques delivered Monday to Thursday. Make learning your daily ritual. Take a lookGet this newsletterBy signing up, you will create a Medium account if you don’t already have one. Review our Privacy Policy for more information about our privacy practices.Check your inboxMedium sent you an email at  to complete your subscription.20 20 Machine LearningGamesReinforcement LearningProgrammingArtificial IntelligenceMore from Towards Data ScienceFollowA Medium publication sharing concepts, ideas, and codes.Read more from Towards Data ScienceMore From Medium5 YouTubers Data Scientists And ML Engineers Should Subscribe ToRichmond Alake in Towards Data Science7 Must-Haves in your Data Science CVElad Cohen in Towards Data Science30 Examples to Master PandasSoner Yıldırım in Towards Data Science21 amazing Youtube channels for you to learn AI, Machine Learning, and Data Science for freeJair Ribeiro in Towards Data ScienceThe Roadmap of Mathematics for Deep LearningTivadar Danka in Towards Data Science4 Types of Projects You Must Have in Your Data Science PortfolioSara A. Metwalli in Towards Data ScienceAn Ultimate Cheat Sheet for Data Visualization in PandasRashida Nasrin Sucky in Towards Data ScienceHow to Get Into Data Science Without a DegreeTerence S in Towards Data ScienceAboutHelpLegalGet the Medium app"
Noisy Networks For Exploration,https://medium.com/mastering-rl-in-minutes/mastering-rl-in-minutes-noisy-networks-for-exploration-c721ee8f8487?source=tag_archive---------3-----------------------,"Reinforcement Learning,Data Science,Artificial Intelligence,Machine Learning,Innovation","Paper: Noisy Networks for ExplorationAuthors: Meire Fortunato, Mohammad Gheshlaghi Azar, Bilal Piot, Jacob Menick, Ian Osband, Alex Graves, Vlad Mnih, Remi Munos, Demis Hassabis, Olivier Pietquin, Charles Blundell, Shane LeggSummary by: Kowshik chilamkurthyAny suggestions and feedback? drop a mail: kowshikchilamkurthy@gmail.comThanks !Mastering RL in MinutesRL for Everyone: Making Research More accessibleFollowReinforcement LearningData ScienceArtificial IntelligenceMachine LearningInnovationWritten byKowshik chilamkurthyFollowRL | ML | ALGO TRADING | TRANSPORTATION | GAME THEORYFollowMastering RL in MinutesFollowExplaining Cutting Edge RL PapersFollowWritten byKowshik chilamkurthyFollowRL | ML | ALGO TRADING | TRANSPORTATION | GAME THEORYMastering RL in MinutesFollowExplaining Cutting Edge RL PapersMore From MediumMachine Learning to Kaggle Caravan Insurance Challenge on RKieran Tan Kah Wang in The StartupFinetuning BERT with Tensorflow estimators in only a few lines of codeArturo Sánchez Palacio in SerendeepiaTransfer Learning In NLPPratik Bhavsar in Modern NLPTraining Your First Distributed PyTorch Lightning Model with Azure MLAaron (Ari) Bornstein in Microsoft AzureHow to implement the successful Machine Learning project in a responsible wayMilan ZdravkovicHow Facebook and Google uses Machine Learning at their bestShivam Prasad UpadhyayDeep Reinforcement Learning with TensorFlow 2.0Roman RingQuantizing Neural Network Models in MXNet for Strict Consistency on BlockchainCortexlabs in Apache MXNetLearn more.Medium is an open platform where 170 million readers come to find insightful and dynamic thinking."
Navigating Into the World of Machine Learning,https://medium.com/swlh/navigating-into-the-world-of-machine-learning-1c1b10ae40b?source=tag_archive---------4-----------------------,"Artificial Intelligence,Neural Networks,Reinforcement Learning,Ensemble Learning,Machine Learning","With the rapid expansion of Machine Learning as a field of research, it’s not easy to keep up with everything that is being invented and discovered.I have created a graph that will make the distinction of the types of machine learning systems easier to understand.It is useful to keep in mind there’s more than one way (ML algorithm) to solve a specific problem. Usually, there are several that fit and it’s your duty to identify which is the best algorithm with the given circumstances of its implementation.Nowadays, everything is being solved with neural networks; why use a sword to cut bread, when a knife would be sufficient?The ML landscape looked from above, has mainly four different directions:In the following weeks, I will delve into each topic and discuss its nuts and bolts.Sum upHope this article helped you get a clearer idea how the ML landscape is generally separated.Did I miss any types of learning? Let me know in the comments below.I’d love to hear your ideas on what you’d like to read next — let me know down below in the comment section!You can always connect with me via LinkedIn.The StartupMedium's largest active publication, followed by +723K people. Follow to join our community.Follow50 Artificial IntelligenceNeural NetworksReinforcement LearningEnsemble LearningMachine Learning50 claps50 clapsWritten byBardh RushitiFollowMachine Learning Engineer | Innately curious about the world.FollowThe StartupFollowMedium's largest active publication, followed by +723K people. Follow to join our community.FollowWritten byBardh RushitiFollowMachine Learning Engineer | Innately curious about the world.The StartupFollowMedium's largest active publication, followed by +723K people. Follow to join our community.More From MediumReal-time object detection & deployment using Tensorflow, Keras and AWS EC2 instanceShirish Gupta in The StartupIntroduction About A Formal Machine Learning ModelReadWrite in ReadWrite[Text-to-SQL] Learning to query tables with natural languageAerin KimHow to evaluate readers text comprehension?Lucas Willems in Glose EngineeringIntroduction to Machine LearningThisuri Bandaranayake in LinkITLearn How Your Chatbot Can Detect IrrelevanceCobus GreylingMachine Learning with Julia / ScikitLearn.jlErdal Sönük in Analytics VidhyaSome AutoML Architectures You Should Know AboutJesus Rodriguez in DataSeriesLearn more.Medium is an open platform where 170 million readers come to find insightful and dynamic thinking."
Revisiting Fundamentals of Experience Replay,https://medium.com/mastering-rl-in-minutes/mastering-rl-in-minutes-revisiting-fundamentals-of-experience-replay-4c8bec978fd2?source=tag_archive---------5-----------------------,"Reinforcement Learning,Data Science,Machine Learning,Artificial Intelligence,Innovation","Paper: Revisiting Fundamentals of Experience ReplayAuthors: William Fedus, Prajit Ramachandran, Rishabh Agarwal, Yoshua Bengio, Hugo Larochelle, Mark Rowland, Will DabneySummary by: Kowshik chilamkurthyAny suggestions and feedback? drop a mail: kowshikchilamkurthy@gmail.comThanks !Mastering RL in MinutesRL for Everyone: Making Research More accessibleFollow3 Reinforcement LearningData ScienceMachine LearningArtificial IntelligenceInnovation3 claps3 clapsWritten byKowshik chilamkurthyFollowRL | ML | ALGO TRADING | TRANSPORTATION | GAME THEORYFollowMastering RL in MinutesFollowExplaining Cutting Edge RL PapersFollowWritten byKowshik chilamkurthyFollowRL | ML | ALGO TRADING | TRANSPORTATION | GAME THEORYMastering RL in MinutesFollowExplaining Cutting Edge RL PapersMore From Medium”Snowplay”Background Noise ComicsHere’s three unnamed tech giants which I’m assuming you can name.rstevensPluto’s AtmosphereCosmic Funnies in The Cosmic CompanionCats: Nature’s MetaphorsrstevensGoodnight Noises EverywhereAubrey Nolan in SpiralboundTonight’s comic thought you quit drinking.rstevens in Diesel SweetiesA Dandelion’s Advice to the Newly DivorcedAshley Topacio in SpiralboundSix Degrees of Separation of my Thoughts and Kevin BaconHilary Fitzgerald CampbellLearn more.Medium is an open platform where 170 million readers come to find insightful and dynamic thinking."
A Distributional Perspective on Reinforcement Learning,https://medium.com/mastering-rl-in-minutes/mastering-rl-in-minutes-a-distributional-perspective-on-reinforcement-learning-124ca327ad2?source=tag_archive---------6-----------------------,"Reinforcement Learning,Machine Learning,Artificial Intelligence,Data Science,Innovation","Paper: A Distributional Perspective on Reinforcement LearningAuthors: Marc G. Bellemare, Will Dabney, Rémi MunosSummary by: Kowshik chilamkurthyAny suggestions and feedback? drop a mail: kowshikchilamkurthy@gmail.comThanks !Mastering RL in MinutesRL for Everyone: Making Research More accessibleFollowReinforcement LearningMachine LearningArtificial IntelligenceData ScienceInnovationWritten byKowshik chilamkurthyFollowRL | ML | ALGO TRADING | TRANSPORTATION | GAME THEORYFollowMastering RL in MinutesFollowExplaining Cutting Edge RL PapersFollowWritten byKowshik chilamkurthyFollowRL | ML | ALGO TRADING | TRANSPORTATION | GAME THEORYMastering RL in MinutesFollowExplaining Cutting Edge RL PapersMore From MediumDissecting BERT Part 1: The EncoderMiguel Romero Calvo in Dissecting BERTAn Introduction to Super Resolution using Deep LearningBharath Raj in BeyondMindsGetting Started with Machine Learning — Comprehensive guide with free resourcesABHISHEK SHAH in The InnovationDeep Learning — What’s the hype about?Harry Hallock in Deep Neuron LabStoring and Retrieving Machine Learning Models at Scale With Distributed Object StorageDaitan in Better ProgrammingFeature Engineering Steps in Machine Learning : Quick start guide : Basicsanuragbisht in AI In Plain EnglishSensei Stories: Tim Converse on Sustaining a Long Career in Machine Learning and Implementing ML…Patrick Faller in Adobe Tech BlogHaar Cascade Classifiers in OpenCV Explained Visually.Mahmoud Harmouch in The StartupLearn more.Medium is an open platform where 170 million readers come to find insightful and dynamic thinking."
Average DQN,https://medium.com/mastering-rl-in-minutes/mastering-rl-in-minutes-average-dqn-d8749d0d92a7?source=tag_archive---------7-----------------------,"Reinforcement Learning,Machine Learning,Data Science,Artificial Intelligence,Innovation","Paper: Averaged-DQN: Variance Reduction and Stabilization for Deep Reinforcement LearningAuthors: Oron Anschel, Nir Baram, Nahum ShimkinSummary by: Kowshik chilamkurthyAny suggestions and feedback? drop a mail: kowshikchilamkurthy@gmail.comThanks !Mastering RL in MinutesRL for Everyone: Making Research More accessibleFollow3 Reinforcement LearningMachine LearningData ScienceArtificial IntelligenceInnovation3 claps3 clapsWritten byKowshik chilamkurthyFollowRL | ML | ALGO TRADING | TRANSPORTATION | GAME THEORYFollowMastering RL in MinutesFollowExplaining Cutting Edge RL PapersFollowWritten byKowshik chilamkurthyFollowRL | ML | ALGO TRADING | TRANSPORTATION | GAME THEORYMastering RL in MinutesFollowExplaining Cutting Edge RL PapersMore From MediumComputational Complexity of Neural NetworksSohaib Ahmad in The StartupBuilding a medical search engine — Step 2: Identifying medical entities in text.Patricio Cerda in PososFamilyGan: Generating a Child’s Face using his ParentsUriel Singer in The StartupProbabilistic Topic ModelsVenus Rohilla in Analytics VidhyaTime Series Analysis & Predictive Modeling Using Supervised Machine LearningSarit Maitra in The StartupValidation techniques for Time-series and Non-time-series datasetsVikashBreaking Down the Black BoxIshaan Dey in The StartupCross-Validation the Right WayVictor Popov in machine_learning_eli5Learn more.Medium is an open platform where 170 million readers come to find insightful and dynamic thinking."
Udacity Deep RL Nanodegree — Reinforcement Learning,https://medium.com/@heegyukim/udacity-deep-reinforced-learning-curriculum-1-%EC%A0%95%EB%A6%AC-22bea682c29d?source=tag_archive---------8-----------------------,"Reinforcement Learning,Udacity Nanodegree,Monte Carlo,Sarsa,Tile Coding","커리큘럼1은 고전적인 강화학습 알고리즘에 대해서 다룬다. 내용들을 정말 간단하게만 정리했다.Agent-Environment Interactionagent와 environment가 상호작용하며 학습하는 것이 Reinforced LearningAgent는 environment에 action을 주고 새로운 상태(state)와 보상(reward)를 받음. 한 번 action을 주고 state, reward를 받는 것을 timestep이라고 함.Task: RL 문제를 나타냄.Episodic Task: 시작과 끝이 명확히 존재하는 테스크, 종료지점에 도달하면 끝남. 한번 시작해서 종료지점까지 진행하는 것을 하나의 episode라고 함. 여러 episode를 경험하면서 학습함.Continuing Task: 영원히 계속되는 테스크Reward Hypothesis: 각 스탭에서의 보상 R(t)의 합을 최대화하도록 학습함.Discounted Return: 미래의 얻을 수 있는 보상의 합(Gt)을 계산할 때, 더 이후의 timestep일 수록 가중치를 줘서 값을 줄임.Markov Decision Process(MDP)set of states Sset of actions AS(t), A(t) = timestep t에서의 상태 S(t), 행동(A(t)R(t) =state s에서 action a를 선택했을 때의 보상을 확률적으로 나타냄Monte Carlo Methods각 state에서 어떤 action을 선택할지를 policy라고 함.Monte Carlo는 무작위 action을 선택한 뒤, 그 reward를 저장함이를 수없이 반복하다보면, 대부분의 state, action에 대한 reward를 알 수 있음. 이를 저장하는 곳이 Q-Table, 이하 QT(state, action) = reward로 표시처음에 학습 단계에서는 무작위로 reward를 파악하고, 실제 test 단계에선 여태까지 학습한 Q Table을 이용해서 state에서 가장 좋은 reward를 주는 action을 가져옴하지만 state, action의 reward는 그때그때 다를 수 있음. 이 때 Q Table의 reward를 갱신하는 방법에는 크게 두가지가 있음First-Visit MC: 처음 선택한 state, action의 reward를 저장하고 갱신하지 않음Every-Visit MC: state, action의 reward들의 평균을 q-table에 저장Greedy Policygreedy policy는 Q Table의 state에서 가장 좋은 reward를 주는 action을 가져오는 정책임epsilon-greedy policy는 (1-eplison) 의 확률로 greedy policy를 선택하고, 나머지 확률로 random policy를 선택하는 것.이를 통해 항상 최적의(greedy) action 을 선택하지 않고, 일정 확률로 모험을 하게됨(eplison : 0 ~ 1 사이의 실수)강화학습은 학습과정에서 모험을 할지(Exploration of uncharted territory) 아니면 현재의 지식을 개척할지(Exploitation of current knowledge) 이 epsilon 값을 통해서 조절할 수 있음.Temporal Difference(TD)이전까지의 방법들은 한 번의 에피소드를 마친 후 새로 발견한 값들을 이용해 Q-Table을 갱신했음.이런 방식은 비효율적, 매 timestep마다 Q-Table을 갱신하면 더 빠르게 학습할 수 있음. 이것을 Temporal Difference라고 함.on-policy: 경험한 내용을 바탕으로 policy를 예측, 개선off-policy: 경험하지 않은 내용도 경험한 내용을 바탕으로 예측하고 개선해나감.On-line Learning: 매 step에서 예측하고 받은 reward를 그대로 학습에 반영하는 방식크게 Sarsa, Sarsamax, Expected Sarsa가 있음Sarsa or Sarsa(0)현재 state에서 policy(e-greedy 같은거)로 action을 선택함, 그러면 next_state를 알게 됨next_state에서 policy로 next_action을 선택하고 Q Table에서 next_reward를 가져옴reward와 Q-table에 있는 reward의 차이 계산 R(t+1) — Q(St, At)거기에 next_reward(= Q(St+1, At+1)) 에 gamma를 곱한 값을 더한 후그 값을 alpha만큼 곱한 후 Q Table에 더함이를 정리하면, 새로운 reward 값을 alpha만큼 Q Table에 반영하고, 새로운 reward에는 next_reward까지 gamma만큼 반영됐다는 것이다.이 때 alpha를 step size라고 함.GLIE (Greedy in the Limit of Infinite Exploration): episode를 계속 진행하면서 점점 e-greedy를 greedy하게 만드는 것(epsilon 값을 줄이는 것). 예를 들어 epsilon = 1 / episode 을 해서 에피소드가 진행할 수록 epsilon을 줄이는 방법이 있다.Sarsamaxsarsamax는 next_action을 고를 때 policy를 통해 고르는 게 아닌, Q Table의 next_state에 해당하는 reward 중에서 가장 높은 reward를 가진 action을 선택하고 next_reward를 받아오는 방법이다. Q-learning 이라고도 한다.Expected SarsaExpected Sarsa 는 next_reward를 계산할 때, next_state의 모든 가능한 next_action의 reward와 그 next_action이 발생할 확률을 곱하고 그 합을 next_reward로 사용한다. 예를 들어 next_state의 action과 reward(Q-table에 있는 값), action을 고를 확률이 아래처럼 있다면action | reward | prob |0      |   10   | 0.5  |1      |   15   | 0.4  |2      |   20   | 0.1  |next_reward = 10 * 0.5 + 15 * 0.4 + 20 * 0.1 = 13next_reward는 13이 된다.TD Control Method 정리Sarsa(혹은 Sarsa(0)라고 함): 스텝크기 alpha가 충분히 작고, epsilon이 GLIE 조건을 만족할 때 최적의 Q값 수렴 보장. on-policy TD Control methodSarsamax: off-policy TD Control. sarsa와 같은 조건하에 수렴 보장Expected Sarsa: on-policy TD Control. sarsa와 같은 조건하에 수렴 보장성능 분석on-policy(Sarsa, Expected Sarsa)가 off-policy(Sarsamax)보다 온라인 성능이 좋음.Expected Sarsa가 일반적으로 Sarsa보다 성능이 좋음.Continous Spaces이전까지의 학습은 state와 action의 값이 유한하고 이산적이었음. 하지만 예를 들어 state가 0~1사이의 무한한 값이라거나 하는 상황에서는 Q-Table을 만들 수 없음Discretization연속적인(Continous) 값을 이산적(Discretization)인 값으로 바꾸는 방법Uniform Discretization: 0~1 사이의 값이라고 하면 동일한 간격으로 나눔. 예를 들면 0~0.2의 값을 0.1로 바꾸고, 0.2~0.4를 0.3으로 바꾸고, …Non-Uniform Discretization: 동일하지 않은 간격으로 나눔. 예를 들면 0~0.4의 값을 0.2로 만들고, 0.4~0.5의 값을 0.45 로 바꾸는 등이러면 state의 범위를 유한하게 만들 수 있고 state-action Q Table을 만들 수 있다.Tile Coding값의 범위를 규칙적인 크기로 나뉜 Tile에 해당하는 값으로 바꿈.예를 들어 0.4 라는 값은 0.3 ~ 0.7의 범위를 이산화한 tile에서는 0.5가 되고, 0.2~0.4의 범위를 이산화한 tile에서는 0.3이 된다. 서로 다른 종류의 타일마다 Q-Table을 만들고 최종 reward를 예측할 때는 모든 Q Table 값의 평균으로 사용한다.Coarse Coding기본적으로 tile coding과 비슷하지만 비규칙적으로 나뉜 영역으로 이산화함.여기까지가 고전적인 강화학습 방법들Function Approximation연속적인 state문제를 근본적으로 해결하기 위해 선형 혹은 비선형 함수 f를 만들고 f(state, action) = reward를 맞추도록 한다. 이 과정에서 딥러닝의 gradient decent를 이용해서 최적의 값을 찾아가게 된다. 앞으로 커리큘럼 2부터는 이런 Deep Reinforced Learning에 대해서 다룰 것이고. 커리큘럼1에서는 고전적인 강화학습 알고리즘에 대해서 다뤘다.Written by김희규Software Engineer at ABLYFollowReinforcement LearningUdacity NanodegreeMonte CarloSarsaTile CodingMore from 김희규FollowSoftware Engineer at ABLYMore From MediumLet’s take a look of Seattle AirBnB Open Data using Python김희규TD in Reinforcement Learning, the Easy WayZiad SALLOUM in Towards Data ScienceIntroduction to the Deadly Triad Issue of Reinforcement LearningQuentin Delfosse in Towards Data ScienceTemporal Difference Learning —The Perceptive Agent in The StartupDeep Reinforcement learning using Proximal Policy OptimizationSurajit SaikiaUsing opinion polling technique for option pricing: StratumsLala RustamliEmerging from the muddle of matricesGraeme KeithHow to quantify the prediction error made by my model?Shuai Guo in Towards Data ScienceAboutHelpLegalGet the Medium app"
Simple Deep Q-Learning with Math,https://medium.com/@devarakonda.vishnu5/simple-deep-q-learning-with-math-1afb0cfdcf0d?source=tag_archive---------9-----------------------,"Artificial Intelligence,Deep Learning,Reinforcement Learning,AI,Deep Q Learning","I will show the mathematics behind and the process of designing a Deep Q-Learning neural network. You should be familiar with a markov decision process(MDP) as it is the basis from which this algorithms is built. Our goal is to find the optimal policy that navigates the MDP on the environment we wish to control.Deep Q-Learning is off-policy and does not consider a policy function when training. It uses an action-value function. This approach defines a function that finds the maximum expected future reward for a policy after taking an action in some state.“Q” aims to maximize the expected future reward for taking an action at a state. Apply the bellman equation to “Q”.At state “S_{t+1}” , “Q_i” outputs the action-values and we pick the maximum value associated with one of the actions. The product of this value and the discount factor plus the current reward defines the total reward for current state and action. If our state and action space is small we can simply use Bellman equation to optimize “Q”. But our spaces are not and we need to go deeper with neural networks.The Neural NetworkStep 1: Build the “Q” function as a neural network with a linear output layer. The max output refers to the action that should be taken for the state input. Step 2: Explore the environment. Randomly choose whether to use “Q” to infer the action or to randomly select an action. Gradually increase the probability that you choose from “Q”. Step 3: Get the reward and the new state from the environment after taking an action. Store your experience as a set of (state1 , action1, reward1, state2) in an experience vector.Step 4: Randomly select sets from your experience vector and use this batch to train the neural network. During training, simply calculate the loss according to the function above and use any optimizer (I used RMSProp) to calculate the gradients and apply them to the neural network.Note: The exact algorithm can be found in the original paper found in the reference below. Although the approach above should theoretically work, it is outdated. If you insist on using DQN, you’re also better off applying prioritized experience replay for better/faster results.ReferencePlaying Atari With Deep Reinforcement LearningWritten byVishnu DevarakondaFollowArtificial IntelligenceDeep LearningReinforcement LearningAIDeep Q LearningMore from Vishnu DevarakondaFollowMore From MediumQ-Learning PrimerJames BowenWhy Big Data And Machine Learning Are Important In Our SocietyTerence Mills in AI.ioHow I used a Pipeline() to solve the Kaggle Disaster Tweet competition QuestionTracyrenee in Python In Plain EnglishAutomated feature engineering with evolutionary strategies.Octavio Gonzalez-LugoIntroducing gobbliJason Nance in RTI Center for Data ScienceCracking the handwritten digits recognition problem with Scikit-learnAriel Segura in Overfitted MicroservicesDocumenting Your Machine Learning Projects Using Advanced Python Techniques (Part 1: Decorators +…Daria Zhukova in The StartupSolving a Rubik’s Cube with Reinforcement Learning (Part 2)Matthew Dalton in Analytics VidhyaAboutHelpLegalGet the Medium app"
"Self-driving car, how do you deal with uncertainties?",https://towardsdatascience.com/dear-self-driving-car-how-do-you-deal-with-uncertainties-dc5a7e2d1c87?source=tag_archive---------0-----------------------,"Reinforcement Learning,Self Driving Cars,Artificial Intelligence,Uncertainty,Machine Learning","Here I present research with Lucas Vogt, Jan Dohmen and Christoph Friebel.TL;DR: Controls for technical systems can be optimized in the simulation. In reality, however, numerous unknowns are waiting for us. In this post, we show how the addition of noise and sensor errors affects the optimization result of a Reinforcement Learning agent.Photo by Kyle Glenn on UnsplashMotivation“Better safe than sorry!”The most car drivers are following this idea because in the long run, it is more advantageous to sometimes obtain a suboptimal result than to push for an optimal result every time. For example, motorists seldom push the performance of their vehicle to the limit, preferring to minimize the possibility of misjudgment, which could lead to accidents, and accepting that they will reach their destination later than is theoretically possible. During the development towards autonomous driving, this observation raises the question of how control algorithms of autonomous vehicles behave under consideration of uncertainties, especially since uncertainties can lead to misjudgments and thus to misbehavior, which could end in accidents.LongiControlDo you want to train a simplified self-driving car with Reinforcement Learning?Just try our new LongiControl Environmenttowardsdatascience.comAs baseline for our study of the before mentioned question, we used the LongiControl environment for OpenAI’s Gym [1], which offers an easily accessible possibility to train agents through Reinforcement Learning (RL). We will summarize the key facts about LongiControl in the following paragraph, but you can find further information in our first article here or the published paper here. Furthermore, you can try out LongiControl by yourself, the source code is available at GitHub here.LongiControl EnvironmentKey Facts In the LongiControl environment, it is the aim that a vehicle completes a single-lane route in a given time as energy-efficient as possible without causing accidents. In summary, this corresponds to the minimization of the total energy E used in the interval from t₀ to T as a function of the power P:According to external requirements, such as other road users or speed limits, the following boundary conditions must be met at the same time:Where v is the velocity, a is the acceleration and a_dot is the jerk, with ()ₗᵢₘ,ₘᵢₙ and ()ₗᵢₘ,ₘₐₓ representing the lower and upper limits respectively. Therefore, the RL agent tries to meet all these criteria through his chosen actions, while minimizing the overall energy consumption of the vehicle.Why consider uncertainty?Not all influences occurring later in the real world application can be seen in advance. The future is not known, sensor errors occur suddenly and the smallest deviations can lead to undesired results:To bridge the gap between the agent behavior in the real world and autonomous driving in simulations, it is important to incorporate the non-deterministic nature of reality into your simulations. This field of research is usually known as “Sim to Real Transfer” and if you want to learn more about it, further examples are given in [2] and [3]. A first step on this endeavor is the implementation of random noise and disturbances in the training environment of RL agents. As a result, the flowchart of the typical agent-environment interaction now looks different than usual. Both the agent’s requested action and the new state of the environment are modified through randomly occurring disturbances.Fig. 1: Agent-Environment interaction without uncertaintiesFig. 2: Agent-environment interaction with uncertaintiesImplemented UncertaintiesWe chose to implement the following four different types of disturbances to create uncertainty in the agent’s training process.Slip occurs in the real world through different effects, the most common are snowfall, rain and leaves on the road. In our scenario slip is important, because it influences the acceleration of the vehicle i. e. the action of the agent.Fig. 3: Impact of slip on the agent’s behaviorTemporary failures of the velocity sensors are an unlikely, but possible, event usually triggered through errors in the data transmission between sensor and controller board. The environment’s state vector contains the current velocity of the vehicle. Therefore, the agent’s behavior is sensitive to velocity changes and needs to learn how to deal with the uncertainty regarding the current vehicle speed.Fig. 4: Impact of sensor failure on the agent’s behaviorFigure 4 shows the effect of the temporarily faulty speed signal in 3 positions. Thereby the speed value suddenly jumps to zero to afterwards display the correct one again. The vehicle continues driving at these points. Only the Reinforcement Learning Agent receives a temporarily faulty signal.Faulty traffic sign detection incorporates two different types of random disturbances. Firstly, the wrong detection of traffic signs, especially speed limits. A camera system could identify a given speed limit of 50 km/h as a 60 km/h. Those errors could lead an agent to wrong assumptions about his environment, which could result in an inappropriate behavior of the agent. As you can see in Figure 5, uncertainty about the upcoming speed limit leads to small alternating acceleration and deceleration phases. These are resulting from the desire to create a smooth transition between different speed zones, but the upcoming speed limit (the target value of the transition) is uncertain, therefore the transition process becomes uncertain i.e. shows random deviations.Fig. 5: Impact faulty traffic sign detection on the agent’s behaviorSecondly, the unprecise measurement of the distance until the next speed limit. The agent receives this distance through the state vector of the environment and chooses his action accordingly. The distance information is necessary to ensure a smooth transition between zones of different speed limits without overshooting the current speed limit. Through the uncertainties, the agent is not able to predict precisely how much time is left until the next speed limit becomes effective. Therefore he starts to accelerate the car to ensure a smooth transition to the next speed limit, but shortly after the acceleration the agent decides to decelerate the car to avoid a reward penalty for speeding. This cycle repeats a few times, because the agent is starting the acceleration processes to early based on uncertain (wrong) information.Fig. 6: Impact of faulty distance measurement on the agent’s behaviorOptimization under uncertaintyPhoto by Conor Luddy on UnsplashBehavior under uncertaintyAs expected the agents, which were trained in an environment with random disturbances, perform better in an uncertain environment than the standard agents, which have no experiences dealing with random events. You can see in Figure 7 and Figure 8 that the agent, which was trained with uncertainties, avoids the overshooting of the speed limits despite the noisy environment and the random deviations of is actions.Fig. 7: Agent trained without uncertaintiesFig. 8: Agent trained with uncertaintiesBehavior without uncertaintyHaving an agent, which considers the effects of possible random deviations, does not mean that the agent always should act as there were disturbances. Therefore, the behavior in a certain environment without deviations is also an interesting indicator of the agent’s capabilities. In Figure 9 and Figure 10, you can see the behavior of two agents in a deviation-free environment. It is obvious, that the uncertain agent is holding a safety distance to the speed limits. Through this behavior the agent gains a short time window to react, if an unexpected random deviation leads to a sudden acceleration of the car, which would otherwise result in an exceeding of the current speed limit.Fig. 9: agent trained without uncertaintiesFig. 10: agent trained with uncertaintiesBehavior control through uncertaintyThe implementation of uncertainties offers, additionally to the increased robustness of the trained agents, the possibility to change the agent’s behavior without changing the structure of the reward calculation. In order to train an agent, whose behavior can be altered after the training process through the end user, it is necessary to vary the parameters of the implemented uncertainties (e. g. standard deviation or expected value) during the training. For example a larger standard deviation of the random disturbances results in a more defensive behavior, because the agent expects to counteract larger deviations. Therefore the end user changes the agent’s expectations through choosing a different behavior mode. The chosen behavior mode alters a value in the environment state vector. This value influences the standard deviation and expected value of the distributions, which are used to calculate the random effects of the uncertainties. In the Figures 11, 12 and 13 you can see how the expectations of the agent are influencing the final behavior.Fig. 11: Large deviations expected → defensive behaviorFig. 12: Normal deviations expected → normal behaviorFig. 13: Small deviations expected → aggressive behaviorSummaryAt the end of this article we will summarize the most important points.In our first article we showed that in the proposed RL environment, which is adapted to the OpenAI Gym standardization, it is easy to prototype and implement state-of-art RL algorithms. Besides, the LongiControl environment is suitable for various examinations. In addition to the comparison of RL algorithms and the evaluation of safety algorithms, investigations in the area of Multi-Objective Reinforcement Learning are also possible. In this, our second, article we present the effects of uncertainty on the behavior of the trained agents. We show that uncertainties alone can be enough to implement an agent with varying behavior and that uncertainties can have a positive impact on the training speed of the agent. Future possible research objectives are the comparison with planning algorithms for known routes and the consideration of very long-term objectives like arriving at a specific time.LongiControl is designed to enable the community to leverage the latest strategies of reinforcement learning to address a real-world and high-impact problem in the field of autonomous driving.Photo by Franki Chamaki on UnsplashEnjoy using it😉References[1] G. Brockman and V. Cheung and L. Pettersson and J. Schneider and J. Schulman and J. Tang and W. Zaremba, OpenAI Gym (2016), CoRR[2] Xue BinPengu.a., Sim-to-Real Transfer of Robotic Control with Dynamics Randomization (2017), CoRR[3] JoshuaTobinu.a., Domain Randomization for Transferring Deep Neural Networks from Simulation to the Real World (2017), CoRRRoman Ließner - Dresden, Saxony, Germany | Professional Profile | LinkedInView Roman Ließner's professional profile on LinkedIn. LinkedIn is the world's largest business network, helping…www.linkedin.comWritten byRoman LiessnerHi, I’m Roman, a doctor of engineering in the field of artificial intelligence and automotive engineering.Follow125 Sign up for The Daily PickBy Towards Data ScienceHands-on real-world examples, research,  tutorials, and cutting-edge techniques delivered Monday to Thursday. Make learning your daily ritual. Take a lookGet this newsletterBy signing up, you will create a Medium account if you don’t already have one. Review our Privacy Policy for more information about our privacy practices.Check your inboxMedium sent you an email at  to complete your subscription.125 125 Reinforcement LearningSelf Driving CarsArtificial IntelligenceUncertaintyMachine LearningMore from Towards Data ScienceFollowA Medium publication sharing concepts, ideas, and codes.Read more from Towards Data ScienceMore From Medium5 YouTubers Data Scientists And ML Engineers Should Subscribe ToRichmond Alake in Towards Data Science7 Must-Haves in your Data Science CVElad Cohen in Towards Data Science21 amazing Youtube channels for you to learn AI, Machine Learning, and Data Science for freeJair Ribeiro in Towards Data ScienceThe Roadmap of Mathematics for Deep LearningTivadar Danka in Towards Data ScienceAn Ultimate Cheat Sheet for Data Visualization in PandasRashida Nasrin Sucky in Towards Data ScienceHow to Get Into Data Science Without a DegreeTerence S in Towards Data Science30 Examples to Master PandasSoner Yıldırım in Towards Data ScienceHow To Build Your Own Chatbot Using Deep LearningAmila Viraj in Towards Data ScienceAboutHelpLegalGet the Medium app"
Unraveling Python’s threading mysteries.,https://towardsdatascience.com/unraveling-pythons-threading-mysteries-e79e001ab4c?source=tag_archive---------1-----------------------,"Python,Multithreading,Reinforcement Learning","Majority of our computers now have multi-core architectures, and terms such as multi-threading often ring in our ears as a way to improve the processing efficiency of applications. Python does offer several tools to parallelize computation, but they are not often well known. Let’s pierce through their secrets in this article.Photo by Mathew Schwartz on UnsplashFirst a small reminder about threading. What is a thread? It’s a lightweight process that runs on your computer, executing it’s own set of instructions. When you run two programs on your computer, you actually create two processes. Each of them has a set of instructions(open your browser or increase the volume) that it wants to have read by the scheduler (the referee that decides what to feed to the processor). The particularity of threads versus processes is that they can share variables.For us in terms of coding, when we run two threads, we allow two pieces of code to run at the same time. However it is different than just executing two programs at the same time since threads give us more control. For example we can share some variables between threads or we can wait for the threads to finish, merge the results and go on with the rest of the code. It is a very powerful tool that can allow faster computation or the ability to handle concurrent events (think of robots with multiple sensor data to process).Let’s digress a bit and analyze the different possibilities Python offers to run computation in parallel. The three laureates are: Threads, Thread Pools, and Multi-Processing.For clarity, let’s first introduce the function that we wish to parallelize. The sleep function, whose purpose is to… sleep.Threading: the most basic tool Python can offer to threadThe Python library let us create Threads manually, for which we can specify the target (the function we wish to execute in this thread) and its arguments. The interface also includes a start function as well as a join function, which will wait until the execution of the thread is over. Joining threads is often desirable when we want to exploit the results returned by the thread. But the basic threading.Thread is quite limited in the sense that it does not let us access the variables returned by the sleep function.2. Thread Pools: (and it’s library concurrent.futures)The thread pool executor provides a more complete set of interfaces for threading. However it’s underlying implementation still uses the Threading library, which gives the same advantages and drawbacks as the previous option. In terms of interface differences, it proposes the concept of future, which will seem familiar to users of C++ 14. The biggest advantage of the futures for us here is that it allows to get the variable returned by the function we are threading using the result() interface.3. Multi-processing (from the library multiprocessing)Multiprocessing is the most complete library that can provide threading capabilities. The main difference with the two others else than providing more interfaces, is its ability to serialize and de-serialize data using a third-party library called pickle. Serialization is the ability to transform data types (int, array etc.) into binaries, sequences of 0 and 1. To do so is required to be able to send data using protocols (tcp/ip, http, udp…) since these protocols are agnostic to the data types we use: the sender might be running his code in Python while the receiver might use C++. In the case of multi-processing, the serialization happens when we pass the function as well as the arguments to the pool object. This allows us to do something incredible: send this thread to be executed on.. another computer! As a matter of such, the multiprocessing library is aimed at enabling shared computation across multiple computers.To note, the multi-processing library provides the apply (sync) and apply_async interfaces, standing for synchronous and asynchronous. In the first case, the threads are forced to return in the same order as they were launched while in the second case the threads come back as soon as they are over. The apply_async provides an additional argument “callback” that gives the possibility to execute a function when the thread returns (to store the result for example).Now it’s time to compare the results of the different threading approaches. We first use the “sleep” function previously mentioned to benchmark the results:We also compute the sleep function sequentially to provide a base result for comparison. Sleeping for 2s each time, we get a total computation time for the sequential approach which seems logical. We get a computation time of 2s for the threading and multiprocessing approaches which means all the thread could run in parallel successfully, but we obtain 4s for the thread pool executor, showing that there is some additional computation cost time in that process. Now this is very nice with only 3 threads running in parallel, but we might want to run more than a thousand threads. Let’s see how it goes with a higher level of difficulty, say 100 threads:As for threading and thread pooling, the results didn’t change from 3 to 100 threads. For the multi-processing approach however, the computation time jumped to 50s! To understand what is happening, let’s look look at the warning that we so thoughtfully placed: “Warning, trying to create more threads than available cores”. As such, multiprocessing will try to dispatch the threads to the available cores (4 in my case) but if no core is available, we can guess that the computation of the threads are queued and thus becoming sequential.We could close off the topic and end up with, multi-processing is terrible, threading library is great. But wait a minute. Up until now, what we did in that threaded function was just sleeping, which means in terms of processing instructions: do nothing. Now, what if we have a thread which is much more greedy on computing resources. Let’s take the example of the counting function:We use 4 threads (number of cores I have on my personal computer) and can see the following results:Here the multi-processing approach is at least 4 times faster the other two. But more importantly, threading takes almost as much time as the sequential approach while the thread pooling approach takes twice as much time, when the initial goal is to optimize the computation time!To understand why this is happening, we need to take a look at the GIL (Global Interpreter Lock). Python is an interpreted language while C or C++ are compiled language. What compilation does is transforming the written code into a language understandable by the processor: binaries. So when the code is executed it is directly read by the scheduler and then the processor. In the case of interpreted languages however, upon launching the program, the code is still written in human readable, here python syntax. So as to be read by the processor, it has to be interpreted at run-time by the so-called Python interpreter. However, a problem arises when threading. The Python interpreter does not allow to interpret multiple threads at the same time and consequently has a lock, the GIL to enforce that. Let’s see a diagram to understand the situation better:In that case, the GIL acts as a bottleneck and nullify the advantages of threading. That explains quite well why the Threading and Pool Threading cases in the comparative analysis performed so poorly. Each of the threads is fighting to get access over the Python interpreter each time their respective count needs to be incremented and this a billion times, whereas when the threads are sleeping, the Python interpreter is only solicited once during the whole duration of the thread existence. If that’s so, why is the multiprocessing approach providing significantly better results? The library is actually tricking the GIL by creating one instance of the Python interpreter for each thread:Thus the bottleneck disappears and the full threading potential can be unlocked.It is worth mentioning that multi-processing still has some down-sides due to the serialization process: not everything can be serialized. Especially, python generators or anything that has a pointer like behavior can’t be serialized (which seems rather normal). Let’s say we have and “agent” object with some unknown content (for example here some pytorch objects), we then end up with an error:We could interpret the differences between Threading and Multiproccessing in terms of computation efficiency. In this second part, we can take a closer look at the main difference as for how resources and variables are managed, especially for shared resources. Let’s consider the code below which makes the threads use a global variable:This code runs totally fine even though we dispatched pieces of code to be evaluated by different processors (the child processes) accessing the same global variable. It means that all the python interpreters were able to access this global variable. So can the python interpreters communicate between each other at runtime? What is the sorcery behind this? Let’s consider a variation of the code below:The purpose of this code is to intentionally change the value of the global variable during runtime from different threads. Not only this, but they are also doing it with different timings (observe the sleep with different inputs).If this variable was really global, when the second thread outputs the value of the global variable, it should already have been incremented quite a bit. However, we see that this value consistently has the value 1 when the thread is printing it. So now we have a better idea of what is happening. When the thread is created (and with it the python interpreter), all the variables are copied to the new thread, no exception made with global variables. In a sense, all the threads are identical copies of the larger process that you are running with slightly different arguments passed to the thread. Now multiprocessing offers different options related to how are the child processes created. We can choose between three start methods, namely spawn, fork and forkserver. We will analyze the first two ones. According to the Python documentation:So the main difference lies in what variables are inherited from the parent process upon creating a child process. Consider the code that was introduced above. If you observed carefully, you could notice that the start method was specified as “fork”. Let’s see what this code actually outputted:Nothing special to see here, we know that the child process was able to access the global variable since it has copied it. Now let’s see what happens when we switch to “spawn”:I can see the surprised expression in your eyes, no, you are not dreaming, you are seeing this multiple times. The “print(“Should only see this once”)” was made at the very beginning of the program and totally out of the loop where the threads are dispatched. Yet this was printed 4 times. So what is happening? The Python documentation only tells us that “The parent process starts a fresh python interpreter process” and that it only inherits the objects necessary for the run() method (understand the function that you are trying to thread). What you need to understand from this is inherited = copied, and not_inherited = re-evaluated. So when we are choosing “spawn”, every instruction of the process is re-interpreted, the function calls as well as variables memory allocation.  Now you can notice the “if __name__ == ‘__main__’:” statement. What this does is signifying to the interpreter that whatever is inside is belonging to the main, and should therefore be inherited by the child process. Everything not in that statement will be re-evaluated for each child process. It means that by default “spawn” is trying to re-evaluate everything, but we do have control over what gets inherited, while for “fork”, every variable gets inherited by default. You could wonder why would this matters, in the case of our global variable it doesn’t change much. But for some objects, the copy constructor (when you are using the = operator) might not be well defined, which makes it unsafe to use the fork method in some occasions. For that reason, it can be seen in the python documentation that the spawn method is moving to be the default start method over all platforms.You might be wondering that this is a rather heavy limitation, if the threads created by multiprocessing are totally closed off, we lose the ability to have a degree of synchronization between the threads. This is not entirely true as the library put at our disposal some tools to do exactly that: Pipes.Result: “Slept for: 2.00”, which means the thread actually waited to receive the data provided by the parent process before moving on. In a way, pipes are an equivalent of C++ futures where we are able to wait for the acquisition of some data provided by a different thread. But of course in this case, since the multiprocessing can be happening on different computers, the data sent through the pipes also need to be serialized under the hood.We spoke about resources management for multiprocessing. For the threading library (and thread-pool executors), things are a bit different since we only have one python interpreter. Let’s see an example:We launch 4 threads at the same time, with a small different delay for each of them to actually start counting.Contrary to multiprocessing, the global variable is here shared across the threads and do not hold a local copy. If you are used to manipulated threads, you would probably be horrified by the above code, shouting out terms as “race condition” or “locks”. A lock (which can be shared across threads), is a sort of gate keeper that only allows one thread to unlock the door of code execution at the same time, so as to prevent variables to be accessed or modified at the same time. Wait, we actually already heard that somewhere: The GIL (Global interpreter lock).So python does already have a lock mechanism to prevent two threads to execute code at the same time. Great news, the above code might not be that horrible. Let’s prove it by printing the total count generated after the execution of the three threads (and removing the sleeps). Since the GIL protects us from thread executing instructions at the same time, no matter which thread is incrementing the global variable, we should add 10e5 * 4 times 1 so a total of 4.000.000.Ok, we need some explanation here. Especially how the GIL is actually working. The description “only let one thread run at the same time” might not be accurate enough to explain the situation. If we delve a bit more into the details, we can see this: “In order to support multi-threaded Python programs, the interpreter regularly releases and reacquires the lock — by default, every ten bytecode instructions”. So this is not the same as allowing only one line of actual python code being read at the same time. To fully understand this, we need to get down a level.What happens when you execute a python program? The python interpreter will first compile the code (at runtime) into something more easily transformed into bytes (the food given to the processor unit). For python, this intermediate code is called bytecode. The full description of the bytecode operations can be found here. The dis module enables us to take a look at what the bytecode looks like for a specific function. If we try to have a glimpse over a function that adds 1 to a global variable:To simply add one constant to a variable, we need 4 bytecode operations, fetching the variables, adding them together and storing the result.Now that we have a better understanding of what is a bytecode operation, we can notice that the GIL comes with settings that we can control: the sys.setcheckinterval() enables us to control every how many bytecodes we want to lock the GIL. But even though we set this to 1 (The GIL gets locked every bytecode instruction), this is not really going to help us. Let’s analyze what happens in the case where we have the GIL locking every 3 bytecodes:The numbers 1 to 4 represent the order in which the bytecode groups are allowed to be processed by the GIL. Now let’s imagine the global variable has initially a value of 0. In each thread, we are trying to add 1 to this variable. At the end of the execution of the bytecode group “1”, the copy of the variable that it got from LOAD_FAST got incremented (INPLACE_ADD) by 1. But the global variable itself was not modified yet as STORE_FAST was not executed. Now it’s the turn of the second thread: since the variable was not stored, it will still make a copy of the global variable with the value 0 and increments 1 to it. Now when the group 3 gets executed, the global_variable finally gets stored with the value 1. But as you can imagine, upon execution of group 4, the local copy also has the value 1 and the global variable will be store once again with a 1, while we would expect it to be 2. The bad news is, it doesn’t matter how frequently we lock the GIL, as long as the part of the “global_variable += 1” equivalent bytecodes get mixed up, we have a race condition.So the GIL is not enough to protect our variables, and we have no choice than using locks to force it on the interpreter to only execute some block of code one at a time across threads:That worked about right, the threads counted up to the correct number, while doing this job in parallel. However, if you look at the total computation time, this is over the roof. Acquiring and releasing the locks is indeed time consuming, and when we need to access the variables as often as we do here, it accumulates to form this very heavy computation time.Now it’s time to sum up our experience with parallel computation in Python:We have two main libraries allowing us to do parallel computation: Threading and Multiprocessing.The Global Intepreter Lock (GIL) restricts python programs in terms of parallel computation efficiency, but Multiprocessing goes around it by creating multiple interpreters.Multiprocessing can make full usage of the multiple cores architecture, and even parallelize the computation over different computers by serializing/de-serializing the data necessary by each thread. However, it does create a copy or re-evaluate the resources of the environment. Each thread evolves in it’s confined environment and cannot exchange with the other threads unless using specific tools.The threading library makes the child processes able to access and modify the same variables, however the GIL does not prevent race conditions and we have to use lock to prevent this from happening.So when to use multiprocessing and when to use Threading? We can analyze two use cases of parallelization for applications.Computation efficiency: the goal is to save computation time. In this case, we want to make full usage of the multi-core architecture, and not be bothered by the GIL. Choice: Multiprocessing.I/O communication: when you might receive data from a number of sources, and wish to have the ability to monitor different data source input at the same time. In this case, you might want all the threads to have the same environment since they might want to modify the same variables, and you might not focus on computation efficiency. You also might want much more many threads than the number of available cores. Choice: Threading.Written byGuillaume CrabéRobotics Software Engineer, dreaming of creating robots with intelligence beyond the scope of the human brain. https://github.com/Guillaume-CrFollow60 Sign up for The Daily PickBy Towards Data ScienceHands-on real-world examples, research,  tutorials, and cutting-edge techniques delivered Monday to Thursday. Make learning your daily ritual. Take a lookGet this newsletterBy signing up, you will create a Medium account if you don’t already have one. Review our Privacy Policy for more information about our privacy practices.Check your inboxMedium sent you an email at  to complete your subscription.60 60 PythonMultithreadingReinforcement LearningMore from Towards Data ScienceFollowA Medium publication sharing concepts, ideas, and codes.Read more from Towards Data ScienceMore From Medium5 YouTubers Data Scientists And ML Engineers Should Subscribe ToRichmond Alake in Towards Data Science7 Must-Haves in your Data Science CVElad Cohen in Towards Data Science30 Examples to Master PandasSoner Yıldırım in Towards Data Science21 amazing Youtube channels for you to learn AI, Machine Learning, and Data Science for freeJair Ribeiro in Towards Data ScienceThe Roadmap of Mathematics for Deep LearningTivadar Danka in Towards Data Science4 Types of Projects You Must Have in Your Data Science PortfolioSara A. Metwalli in Towards Data ScienceAn Ultimate Cheat Sheet for Data Visualization in PandasRashida Nasrin Sucky in Towards Data ScienceHow to Get Into Data Science Without a DegreeTerence S in Towards Data ScienceAboutHelpLegalGet the Medium app"
Paper Summary: Discovering Reinforcement Learning Agents,https://towardsdatascience.com/paper-summary-discovering-reinforcement-learning-agents-3cf9447b6ecd?source=tag_archive---------2-----------------------,"AI,Reinforcement Learning,Deep Learning,Machine Learning,Meta Learning","Photo by Jelleke Vanooteghem on UnsplashIntroductionAlthough the field of deep learning is evolving extremely fast, unique research with the potential to get us closer to Artificial General Intelligence (AGI) is rare and hard to find. One exception to this rule can be found in the field of meta-learning. Recently, meta-learning has also been applied to Reinforcement Learning (RL) with some success. The paper “Discovering Reinforcement Learning Agents” by Oh et al. from DeepMind provides a new and refreshing look at the application of meta-learning to RL.Traditionally, RL relied on hand-crafted algorithms such as Temporal Difference learning (TD-learning) and Monte Carlo learning, various Policy Gradient methods, or combinations thereof such as Actor-Critic models. These RL algorithms are usually finely adjusted to train models for a very specific task such as playing Go or Dota. One reason for this is that multiple hyperparameters such as the discount factor γ and the bootstrapping parameter λ need to be tuned for stable training. Furthermore, the very update rules as well as the choice of predictors such as value functions need to be chosen diligently to ensure good performance of the model. The entire process has to be performed manually and is often tedious and time-consuming.DeepMind is trying to change this with their latest publication. In the paper, the authors propose a new meta-learning approach that discovers the learning objective as well as the exploration procedure by interacting with a set of simple environments. They call the approach the Learned Policy Gradient (LPG). The most appealing result of the paper is that the algorithm is able to effectively generalize to more complex environments, suggesting the potential to discover novel RL frameworks purely by interaction.In this post, I will try to explain the paper in detail and provide additional explanation where I had problems with understanding. Hereby, I will stay close to the structure of the paper in order to allow you to find the relevant parts in the original text if you want to get additional details. Let’s dive in!Meta-Learning and Earlier ApproachesDeep learning (including Deep RL) is known to be extremely data-hungry. Compare that to humans who can learn new skills much more efficiently. For example, people who can ride a mountain bike can also learn how to ride a road bike very quickly. Maybe they can even learn how to ride a motorcycle without too much additional external input. Meta-learning aims to equip machine learning models with a similar capability by “learning to learn”, i.e. learning about the training process in order to adapt more quickly to new data distributions.In the paper, the authors subdivided meta-learning frameworks according to the problem they aim to address:Adapting a model trained on one or multiple tasks to a new task using only a few examples (Few-shot adaptation): this variant is exemplified by general algorithms such as MAML or Reptile, as well as RL² specifically in the context of RL.Meta-learning for online adaptation of a single task: Meta-gradient RL by Xu et al. (also from DeepMind) falls in this category. This algorithm tunes hyperparameters such as γ and the bootstrapping parameter λ online while interacting with the environment. It is also possible to learn intrinsic rewards or auxiliary tasks in this manner.Learning new RL algorithms: Meta-learning new algorithms from interacting with a number of environments has also been attempted by multiple groups already. For instance, the Evolved Policy Gradient method attempts to learn the policy gradient loss function using evolutionary methods. It was also recently shown by researchers from DeepMind that useful knowledge for exploration can be learned as a reward function.All of the above approaches use the concept of a value function and try to generalize it. The framework presented in the described paper attempts, for the first time, to learn its own bootstrapping mechanism instead. Let us now have a look at how this is done.Learned Policy Gradient (LPG)The main goal of the paper is to find the optimal gradient update rule:Let us explain this formula in detail: the optimal update rule, parametrized by η, maximizes the expected return at the end of the lifetime of the agent,Hereby, we sample from a distribution of environments p(ε) and initial agent parameters p(θₒ). This means that after training an agent until the end of its lifetime, we want to achieve the maximal expected return.In order to achieve this without being specific about the type of environment we sample, we require the agent to produce two separate outputs:The predicted policy π(a|s) as is usual in policy gradient algorithms,An m-dimensional categorical prediction vector y(s) with output in the range [0, 1].Both the policy and the prediction vector y(s) are used as input to the meta-network. The meta-network is a backward LSTM producing at each update step a guidance on how to update the parameters, π_hat and y_hat (see Figure 1). At this point, it wasn’t entirely clear to me why a backward LSTM model was chosen. My understanding (although I may be wrong) is that the backward direction (from the end of the environment lifetime to the initial agent state) corresponds to the backward direction in the gradient descent optimization of the agent.Figure 1: The meta-learning process. From: Oh, Junhyuk, et al. “Discovering Reinforcement Learning Algorithms.” arXiv preprint arXiv:2007.08794 (2020).The input to the meta-learning LSTM network iswhere r_t is the reward at time t, d_t indicated episode termination, and γ is the aforementioned discount factor. Since the LSTM does not depend on the observation and action space explicitly, it is largely invariant to the environment. Instead, the observation and action space are taken into account only indirectly, through the policy of the trained agent, π.Updating the agentDuring an inner loop, the agent is updated using the formulaIf you take a closer look at this formula, you will notice that the first term in the expectation is similar to the REINFORCE update, except that π_hat is used instead of the usual expected return G. Since π_hat is generated by the meta-learner, it allows the algorithm the flexibility to specify its own concept of a “value” function.The second term minimizes the Kullback-Leibler divergence (a form of distance metric on distributions) between the predicted and desired y. y provides additional information for the LSTM to discover a useful update rule. The meta-learner may indirectly affect the policy through y.Updating the meta-learnerThe formula for updating the meta-learner LSTM is as follows:This definitely requires some explanation. Ideally, we would like to optimize the formula for the optimal gradient update rule over a distribution of environments, as shown above. As you may notice, the expected return at the end of the lifetime depends on the expectation over the policy with end-of-lifetime parameters and these in turn depend on η. This realization leads us to the idea of calculating the meta-gradient as a policy gradient. The first term in the formula above corresponds to exactly this gradient.The other terms are regularizer terms. These were introduced since meta-learning can be very unstable, especially at the beginning of the training process, when y_hat does not have any semantics attached to it. The first two regularization terms serve to maximize the entropy of both the policy and the prediction vector. Policy entropy regularization is a well-known technique in RL (see e.g. https://arxiv.org/abs/1602.01783). The remaining terms introduce L2-regularisation. This helps to prevent rapid changes in policy and prediction updates.CaveatsAs you may expect, there are some other minor implementation issues to be solved before getting the approach to work.First of all, when training agents across different environments, it is not possible to use a fixed learning rate for all of them. The authors explain this by the fact that the optimal policy π_hat needs to be scaled in accordance to the learning rate update to make the training stable. Additionally, since η and thus π_hat change during training, we have no choice but to dynamically adjust the learning rate during meta-learning (notably, this is only used for training the meta-learner). In the paper it is proposed to use a bandit to sample useful hyperparameters for each lifetime separately and update the sampling distribution according to the end-of-lifetime return.Furthermore, in the supplemental material, the authors note that they reset the lifetime whenever the entropy of the policy becomes 0 (the policy becomes deterministic) in order to prevent training collapse.ExperimentsIn order to train LPG, the authors set up two extremely simple toy environments for the agents. One of them is a simple grid world with rewards at certain pixels in the environment, as shown in the figure below. The second are delayed Markov Decision Processes (MDP). This is simply a sophisticated way to describe environments in which the agent can make a decision at some time step and the decision will reap positive or negative rewards at some later time step. 5 variations of environments for each domain were used, capturing problems such as “delayed reward, noisy reward and long-term credit assignment”. For more details about the experiment setup, please refer to the article.Figure 2: Prediction results from the grid world. From: Oh, Junhyuk, et al. “Discovering Reinforcement Learning Algorithms.” arXiv preprint arXiv:2007.08794 (2020).In my opinion, the two most important questions asked in the paper are:How does LPG discover useful semantics of predictions?Can LPG generalise to more complex environments?Prediction SemanticsFigure 2 shows the visualized predictions from the paper. On the top left, a sample grid world with positive and negative, on the bottom left, a near-optimal policy (white paths) and its values (yellow = high, blue = low). The result of the experiment is shown on the right. These are the 30 components of y for the policy on the bottom left. These were obtained by updating y using the LPG with a fixed policy. Looking at the predictions, we can see that almost all of them have a high correlation with the true values. Some of them have large values around positive rewards, and these values are propagated to neighboring pixels. As the authors point out, this “implicitly shows that the LPG is asking the agent to predict future rewards and use such information for bootstrapping”.To show that the correlation seen in the visualization is really there, a simple 1-layer perceptron is trained to predict true values from y for various discount factors. Although y was generated with a discount factor of 0.995, the trained perceptron could predict true values for lower discount factors down to 0.9 as well. This means that the framework can automatically discover a rich and useful semantics of predictions that the” framework can almost recover the value functions at various horizons, even though such a semantics was not enforced during meta-training”. This is important as it shows that the learned 30-dimensional vector indeed captures additional semantic information compared to a single value function. Note that this semantic information was entirely learned by the algorithm.Figure 3: Generalization to Atari games. From: Oh, Junhyuk, et al. “Discovering Reinforcement Learning Algorithms.” arXiv preprint arXiv:2007.08794 (2020).Generalizing to More Complex EnvironmentsThe other significant result is that LPG can seamlessly generalize to more complex environments such as Atari games, while being trained only on the toy environments described above. As shown in Figure 3, LPG can beat human-level performance in almost half of the games while not being explicitly meta-trained on such complex domains. The meta-algorithm even outperforms the much-used Advantage Actor-Critic (A2C) on some games. As you can see from the figure, the performance improves rapidly as LPG sees more and more training environments. It is conceivable that with specifically designed training environments, the performance can surpass even state-of-the-art specialized algorithms in the future.DiscussionFrom the experiments it seems that the LPG is able to automate to a large degree the discovery of new RL algorithms by interacting with simple (or even more complex) environments. Since teaching humans also mostly relies on creating the appropriate environments for learning instead of fine-tuning update rules, this brings training an algorithm and training humans closer together. Moreover, the framework is able to generalize to much more complex environments than those it was trained on, potentially opening up a new approach to RL based entirely on data. Although the new approach is still lagging behind in performance compared to handcrafted RL algorithms, it can outperform A2C in a few Atari games as well as on the training environments, suggesting that it is not strictly worse than a hand-crafted approach. We also have to take into account the fact that these hand-crafted methods were perfected over years of work, while LPG is trained just using data (if we forget for a minute the training stability issue).Perhaps the most important point, in my opinion, is the fact that this approach scales with computing power and data. This means that as our computers get faster (as they inevitably will), LPG will only get better and better. The model described in the paper was trained using a 16-core TPU-v2 for 24 hours. While this might seem prohibitive for anybody without access to Google’s vast computing resources, in a few years, anybody with a modern PC will have this computing power at his/her disposal. I am strongly convinced that completely data-based algorithms are ultimately the only path to stronger AI.ConclusionIn this paper, the authors attempted for the first time to learn an update rule in RL from scratch, thereby avoiding the tedious process of discovering complex update rules manually. Their approach is fully data-driven and introduces an inductive bias to the learning process, something we may also expect to happen in the human brain. The paper shows that reward prediction and state evaluation emerge naturally during training on toy environments. The strong generalization capabilities of the approach suggest that it might be possible to discover extremely efficient RL algorithms from interactions with possibly simple, procedurally generated environments.Written byBranislav HolländerAI | ML | Other Crazy InterestsFollow28 Sign up for The Daily PickBy Towards Data ScienceHands-on real-world examples, research,  tutorials, and cutting-edge techniques delivered Monday to Thursday. Make learning your daily ritual. Take a lookGet this newsletterBy signing up, you will create a Medium account if you don’t already have one. Review our Privacy Policy for more information about our privacy practices.Check your inboxMedium sent you an email at  to complete your subscription.28 28 AIReinforcement LearningDeep LearningMachine LearningMeta LearningMore from Towards Data ScienceFollowA Medium publication sharing concepts, ideas, and codes.Read more from Towards Data ScienceMore From Medium5 YouTubers Data Scientists And ML Engineers Should Subscribe ToRichmond Alake in Towards Data Science7 Must-Haves in your Data Science CVElad Cohen in Towards Data Science21 amazing Youtube channels for you to learn AI, Machine Learning, and Data Science for freeJair Ribeiro in Towards Data ScienceThe Roadmap of Mathematics for Deep LearningTivadar Danka in Towards Data Science30 Examples to Master PandasSoner Yıldırım in Towards Data ScienceAn Ultimate Cheat Sheet for Data Visualization in PandasRashida Nasrin Sucky in Towards Data ScienceHow to Get Into Data Science Without a DegreeTerence S in Towards Data ScienceHow To Build Your Own Chatbot Using Deep LearningAmila Viraj in Towards Data ScienceAboutHelpLegalGet the Medium app"
Learn to Bet — Use Bayesian Bandits for Decision-Making,https://towardsdatascience.com/learn-to-bet-use-bayesian-bandits-for-decision-making-2e2e489087a5?source=tag_archive---------3-----------------------,"Machine Learning,Decision Making,Artificial Intelligence,Data Science,Reinforcement Learning","We live in uncertainty and from time to time we have to make a choice when given several options none of which we know much. And with time, if we are faced with the same options over and over again, we might be able to learn which option tends to give us the maximal reward, which one is the second best, etc. Generally, such can be said of the human learning process, guided by our goal to maximize the total rewards (or minimize the total loss or regrets).Photo by Markus Spiske on UnsplashMoreover, some important business applications can be modeled this way too. Think about the following situations:(1) Given a bunch of stock tickers, each with varying returns, how would you rationally pick the ones that would maximize your returns?(2) There are three website landing page designs that you would like to try out, how would you pick one design that maximizes your metric, such as conversion rate?(3) Suppose you want to promote your business and have three different advertising venues, how would you choose the one venue that gives you the best value for your budget?These realistic business problems can all be handily abstracted into the following scenario:Suppose you have N slot machines (or bandits), each of which with its own probability of rendering you the reward R. How do you figure out which slot machine to pick over time, in order to secure the reward as much as possible?This statement seems innocently simple, but in reality, the decision-making process is more convoluted in that you are constantly dealing with the bifurcating quandary of staying put with the current relatively good choice or trying out other choices which might be potentially much better, namely, exploitation vs exploration. We need a rational framework to tackle such situations.Photo by Caleb Jones on UnsplashBayesian BanditsBayesian bandits provides an intuitive solution to the problem. Generally speaking, it follows these steps:Make your initial guess about the probability that each bandit gives the reward. In the language of statistics, this is to assume a certain prior distribution over the behavior (give the reward or not) of each bandit. And since we are trying to model probabilities, a handy prior distribution will be the beta distribution, which is generally used to model random variable between 0 and 1. Note that if you know absolutely nothing about the underlying distribution of the bandits’ behaviors, your rational guess would be to treat each bandit as equal, namely, to assume a uniform distribution, which is a special case of the beta distribution.Draw a sample from each bandit, based on its current distribution.Focus on the bandit with the highest sample; determine the amount of reward(regret) you obtain by choosing it, and update the bandit’s distribution accordingly. In the language of statistics, you have a prior distribution with certain parameters over a random variable, and now that you have a new observation of the value of the random variable, you are able to update your prior belief about the random variable, by applying the Bayes rule to elicit the posterior distribution; the end result is having a new set of distribution parameters to account for the new observation:Prior distribution: P(A)New observation: xPosterior distribution: P(A|x) ~ P(x|A)*P(A)namely, posterior distribution is proportional to likelihood*prior distribution4. Go back to Step 2 and repeat.Photo by Tine Ivanič on UnsplashSimulationLet us look at a concrete example with some code.First things first, let us import the necessary packages:import numpy as npfrom scipy.stats import betaimport matplotlib.pyplot as pltLet us define our bandits; these numbers denote the respective bandit’s probability of rendering a reward and you should feel free to modify these numbers and run different simulations:bandits = np.array([0.75, 0.5, 0.6])With the fundamentals cleared, now the following function illustrates the core of Bayesian bandits:def experiment(accumulated_rewards_per_bandit, times_chosen_per_bandit, bandits, num_draws=1):        for _ in range(num_draws):        bandit_chosen =    np.argmax(np.random.beta(1+accumulated_rewards_per_bandit, 1+times_chosen_per_bandit-accumulated_rewards_per_bandit))        reward = get_reward_from_bandit_i(bandits, bandit_chosen)                accumulated_rewards_per_bandit[bandit_chosen] += reward        times_chosen_per_bandit[bandit_chosen] += 1    a = 1+accumulated_rewards_per_bandit    b = 1+times_chosen_per_bandit-accumulated_rewards_per_bandit    return a, bLet us break down the function line by line. Foremost, let us specify the input arguments:accumulated_rewards_per_bandit: the total rewards accumulated per bandit over the trials in one experiment, its size being equal to the number of bandits.times_chosen_per_bandit: the number of times each bandit has been chosen over the trials in one experiment, its size being equal to the number of bandits.bandits: our bandits of various reward probabilities.num_draws: number of trials in one experimentRecall that fundamentally, we are aiming to get a better sense of the reward probabilities of different bandits, which means that we are trying to get a more and more accurate estimation of the underlying distributions, which in our case are all beta distributions.And note that a beta distribution is uniquely defined by two parameters a and b, which in our example stand for number of trials a bandit gets rewards (equal to accumulated rewards in our example) and number of trials a bandit fails to get rewards, respectively.Therefore, our goal becomes that of keeping an accurate tally of these two quantities over the trials.Now it should be clear what we are trying to achieve in the function above. Looping over the number of trials, we first pinpoint the bandit that gives the largest sample, and then determine if we get any reward by picking this bandit in the following line:reward = get_reward_from_bandit_i(bandits, bandit_chosen)Here I use the following naive implementation to determine the reward:def get_reward_from_bandit_i(bandits, i):    return np.random.rand() < bandits[i]You can see that this implementation results in a reward being either 1 or 0. In practice, you might want to design your own reward mechanism to reflect the nature of your problem.After obtaining the reward (or not), we can update the accumulated rewards and the times chosen for our sampled bandit, which will serve as the updated beta distribution parameters a and b in the next iteration.Finally, after looping over all trials, we get the fully updated distribution parameters and return them.Now we are equipped with the core components of Bayesian bandits and let us get started on running a simulation using the following code snippet:if __name__ == ""__main__"":    bandits = np.array([0.75, 0.5, 0.6])    num_draws_per_experiment = [300]    accumulated_rewards_per_bandit = np.zeros(bandits.size)    times_chosen_per_bandit = np.zeros(bandits.size)    for i in num_draws_per_experiment:        a, b = experiment(accumulated_rewards_per_bandit, times_chosen_per_bandit, bandits, i)ResultsIt seems like we need a way to determine how good we are doing. In this article, let us try plot the probability density functions of each set of beta distribution parameters and see if they show any discernible patterns.Let us put into play this function:def plot_beta(x_range, a, b, filename):    x = x_range    y0 = beta.pdf(x, a[0], b[0])    y1 = beta.pdf(x, a[1], b[1])    y2 = beta.pdf(x, a[2], b[2])    plt.plot(x, y0, color='red', lw=2, ls='-', alpha=0.5, label='pdf: 1st bandit')    plt.plot(x, y1, color='green', lw=2, ls='-', alpha=0.5, label='pdf: 2nd bandit')    plt.plot(x, y2, color='blue', lw=2, ls='-', alpha=0.5, label='pdf: 3rd bandit')    plt.legend()    plt.savefig(filename)Running the plot function for different numbers of trials, we get the following pdf plots:pdf plot for each of the bandits for #trials=1, 10, 50, 100pdf plot for each of the bandits for #trials=200, 500, 1000, 2000Note how the first bandit (red), with its reward probability being the top at 0.75, gradually wins out after 100 trials as the shape of its beta distribution shrinks and peaks around 0.75 the true value, whereas the shapes of the distributions of the other two bandits are still relatively flat and wide-dispersed, which hints at uncertainty.Although with sufficiently many trials, the distribution for each bandit will tend to peak around the true value of the reward probability, in this problem we are more interested in finding the best bandit, instead of accurately inferring the hidden reward probabilities. There are several improvements that can be made to our simulation: for example, a learning rate (>1 or <1) can be applied to the update of the accumulated rewards to fine-tune whether we tend to stay with the current good option or explore other options more.In the follow-up articles, I will showcase how Bayesian bandits helps construct a dynamic investment portfolio of stocks, which can be modeled as bandits with underlying reward probabilities that change with time. So follow me and stay tuned for more.Written byJason X. YangEntrepreneur, Writer, Armchair Historian and Linguist, dabbling in startups, tech, cognitive science and arts @NYC. LinkedIn: www.linkedin.com/in/jasonxiyang/Follow170 Sign up for The Daily PickBy Towards Data ScienceHands-on real-world examples, research,  tutorials, and cutting-edge techniques delivered Monday to Thursday. Make learning your daily ritual. Take a lookGet this newsletterBy signing up, you will create a Medium account if you don’t already have one. Review our Privacy Policy for more information about our privacy practices.Check your inboxMedium sent you an email at  to complete your subscription.170 170 Machine LearningDecision MakingArtificial IntelligenceData ScienceReinforcement LearningMore from Towards Data ScienceFollowA Medium publication sharing concepts, ideas, and codes.Read more from Towards Data ScienceMore From Medium5 YouTubers Data Scientists And ML Engineers Should Subscribe ToRichmond Alake in Towards Data Science7 Must-Haves in your Data Science CVElad Cohen in Towards Data Science21 amazing Youtube channels for you to learn AI, Machine Learning, and Data Science for freeJair Ribeiro in Towards Data ScienceThe Roadmap of Mathematics for Deep LearningTivadar Danka in Towards Data Science30 Examples to Master PandasSoner Yıldırım in Towards Data ScienceAn Ultimate Cheat Sheet for Data Visualization in PandasRashida Nasrin Sucky in Towards Data ScienceHow to Get Into Data Science Without a DegreeTerence S in Towards Data ScienceHow To Build Your Own Chatbot Using Deep LearningAmila Viraj in Towards Data ScienceAboutHelpLegalGet the Medium app"
An Optimistic Perspective on Offline RL,https://medium.com/mastering-rl-in-minutes/mastering-rl-in-minutes-an-optimistic-perspective-on-offline-rl-70553feda464?source=tag_archive---------4-----------------------,"Reinforcement Learning,Data Science,Machine Learning,Artificial Intelligence,Innovation","Paper: An Optimistic Perspective on Offline Reinforcement LearningAuthors: Rishabh Agarwal, Dale Schuurmans, Mohammad NorouziSummary by: Kowshik chilamkurthyAny suggestions and feedback? drop a mail: kowshikchilamkurthy@gmail.comThanks !Mastering RL in MinutesRL for Everyone: Making Research More accessibleFollow1 Reinforcement LearningData ScienceMachine LearningArtificial IntelligenceInnovation1 clap1 clapWritten byKowshik chilamkurthyFollowRL | ML | ALGO TRADING | TRANSPORTATION | GAME THEORYFollowMastering RL in MinutesFollowExplaining Cutting Edge RL PapersFollowWritten byKowshik chilamkurthyFollowRL | ML | ALGO TRADING | TRANSPORTATION | GAME THEORYMastering RL in MinutesFollowExplaining Cutting Edge RL PapersMore From MediumSome Regression Techniques with OutliersSandip DuttaFacial Recognition — a visual step by stepPatrick Ryan in The StartupTransfer learning for Deep Neural Networks using TensorFlowSai TejaDemocratized AI — The Black Box ProblemHanne-Torill Mevik in Tech AccountsAn Overview of Signal ClassificationLuke Kerbs in GSI TechnologyWhat’s worth doing in machine learning?Nick Doiron in The StartupPredicting the next 5 minutes of a Cricket Game 🏏 Project MontyDrew JarrettGenetic Artificial Neural NetworksRoman Paolucci in The StartupLearn more.Medium is an open platform where 170 million readers come to find insightful and dynamic thinking."
Deep Reinforcement Learning for Automated Stock Trading,https://towardsdatascience.com/deep-reinforcement-learning-for-automated-stock-trading-f1dad0126a02?source=tag_archive---------0-----------------------,"Deep Reinforcement,Reinforcement Learning,Machine Learning,Stock Trading,Markov Decision Process","Image by Chris on UnsplashNote from Towards Data Science’s editors: While we allow independent authors to publish articles in accordance with our rules and guidelines, we do not endorse each author’s contribution. You should not rely on an author’s works without seeking professional advice. See our Reader Terms for details.This blog is based on our paper: Deep Reinforcement Learning for Automated Stock Trading: An Ensemble Strategy, presented at ICAIF 2020: ACM International Conference on AI in Finance.Our codes are available on Github.AI4Finance-LLC/Deep-Reinforcement-Learning-for-Automated-Stock-Trading-Ensemble-Strategy-ICAIF-2020This repository refers to the codes for ICAIF 2020 paper. Stock trading strategies play a critical role in investment…github.comOur paper is available on SSRN.Deep Reinforcement Learning for Automated Stock Trading: An Ensemble StrategyDate Written: September 11, 2020 Stock trading strategies play a critical role in investment. However, it is…papers.ssrn.comIf you want to cite our paper, the reference format is as follows:Hongyang Yang, Xiao-Yang Liu, Shan Zhong, and Anwar Walid. 2020. Deep Reinforcement Learning for Automated Stock Trading: An Ensemble Strategy. In ICAIF ’20: ACM International Conference on AI in Finance, Oct. 15–16, 2020, Manhattan, NY. ACM, New York, NY, USA.OverviewOne can hardly overestimate the crucial role stock trading strategies play in investment.Profitable automated stock trading strategy is vital to investment companies and hedge funds. It is applied to optimize capital allocation and maximize investment performance, such as expected return. Return maximization can be based on the estimates of potential return and risk. However, it is challenging to design a profitable strategy in a complex and dynamic stock market.Every player wants a winning strategy. Needless to say, a profitable strategy in such a complex and dynamic stock market is not easy to design.Yet, we are to reveal a deep reinforcement learning scheme that automatically learns a stock trading strategy by maximizing investment return.Image by Suhyeon on UnsplashOur Solution: Ensemble Deep Reinforcement Learning Trading StrategyThis strategy includes three actor-critic based algorithms: Proximal Policy Optimization (PPO), Advantage Actor Critic (A2C), and Deep Deterministic Policy Gradient (DDPG).It combines the best features of the three algorithms, thereby robustly adjusting to different market conditions.The performance of the trading agent with different reinforcement learning algorithms is evaluated using Sharpe ratio and compared with both the Dow Jones Industrial Average index and the traditional min-variance portfolio allocation strategy.Copyright by AI4Finance LLCPart 1. Why do you want to use Deep Reinforcement Learning (DRL) for stock trading?Existing works are not satisfactory. Deep Reinforcement Learning approach has many advantages.1.1 DRL and Modern Portfolio Theory (MPT)MPT performs not so well in out-of-sample data.MPT is very sensitive to outliers.MPT is calculated only based on stock returns, if we want to take other relevant factors into account, for example some of the technical indicators like Moving Average Convergence Divergence (MACD), and Relative Strength Index (RSI), MPT may not be able to combine these information together well.1.2 DRL and supervised machine learning prediction modelsDRL doesn’t need large labeled training datasets. This is a significant advantage since the amount of data grows exponentially today, it becomes very time-and-labor-consuming to label a large dataset.DRL uses a reward function to optimize future rewards, in contrast to an ML regression/classification model that predicts the probability of future outcomes.1.3 The rationale of using DRL for stock tradingThe goal of stock trading is to maximize returns, while avoiding risks. DRL solves this optimization problem by maximizing the expected total reward from future actions over a time period.Stock trading is a continuous process of testing new ideas, getting feedback from the market, and trying to optimize the trading strategies over time. We can model stock trading process as Markov decision process which is the very foundation of Reinforcement Learning.1.4 The advantages of deep reinforcement learningDeep reinforcement learning algorithms can outperform human players in many challenging games. For example, on March 2016, DeepMind’s AlphaGo program, a deep reinforcement learning algorithm, beat the world champion Lee Sedol at the game of Go.Return maximization as trading goal: by defining the reward function as the change of the portfolio value, Deep Reinforcement Learning maximizes the portfolio value over time.The stock market provides sequential feedback. DRL can sequentially increase the model performance during the training process.The exploration-exploitation technique balances trying out different new things and taking advantage of what’s figured out. This is difference from other learning algorithms. Also, there is no requirement for a skilled human to provide training examples or labeled samples. Furthermore, during the exploration process, the agent is encouraged to explore the uncharted by human experts.Experience replay: is able to overcome the correlated samples issue, since learning from a batch of consecutive samples may experience high variances, hence is inefficient. Experience replay efficiently addresses this issue by randomly sampling mini-batches of transitions from a pre-saved replay memory.Multi-dimensional data: by using a continuous action space, DRL can handle large dimensional data.Computational power: Q-learning is a very important RL algorithm, however, it fails to handle large space. DRL, empowered by neural networks as efficient function approximator, is powerful to handle extremely large state space and action space.Image by Kevin on UnsplashPart 2: What is Reinforcement Learning? What is Deep Reinforcement Learning? What are some of the related works to use Reinforcement Learning for stock trading?2.1 ConceptsReinforcement Learning is one of three approaches of machine learning techniques, and it trains an agent to interact with the environment by sequentially receiving states and rewards from the environment and taking actions to reach better rewards.Deep Reinforcement Learning approximates the Q value with a neural network. Using a neural network as a function approximator would allow reinforcement learning to be applied to large data.Bellman Equation is the guiding principle to design reinforcement learning algorithms.Markov Decision Process (MDP) is used to model the environment.2.2 Related worksRecent applications of deep reinforcement learning in financial markets consider discrete or continuous state and action spaces, and employ one of these learning approaches: critic-only approach, actor-only approach, or and actor-critic approach.1. Critic-only approach: the critic-only learning approach, which is the most common, solves a discrete action space problem using, for example, Q-learning, Deep Q-learning (DQN) and its improvements, and trains an agent on a single stock or asset. The idea of the critic-only approach is to use a Q-value function to learn the optimal action-selection policy that maximizes the expected future reward given the current state. Instead of calculating a state-action value table, DQN minimizes the mean squared error between the target Q-values, and uses a neural network to perform function approximation. The major limitation of the critic-only approach is that it only works with discrete and finite state and action spaces, which is not practical for a large portfolio of stocks, since the prices are of course continuous.Q-learning: is a value-based Reinforcement Learning algorithm that is used to find the optimal action-selection policy using a Q function.DQN: In deep Q-learning, we use a neural network to approximate the Q-value function. The state is given as the input and the Q-value of allowed actions is the predicted output.2. Actor-only approach: The idea here is that the agent directly learns the optimal policy itself. Instead of having a neural network to learn the Q-value, the neural network learns the policy. The policy is a probability distribution that is essentially a strategy for a given state, namely the likelihood to take an allowed action. The actor-only approach can handle the continuous action space environments.Policy Gradient: aims to maximize the expected total rewards by directly learns the optimal policy itself.3. Actor-Critic approach: The actor-critic approach has been recently applied in finance. The idea is to simultaneously update the actor network that represents the policy, and the critic network that represents the value function. The critic estimates the value function, while the actor updates the policy probability distribution guided by the critic with policy gradients. Over time, the actor learns to take better actions and the critic gets better at evaluating those actions. The actor-critic approach has proven to be able to learn and adapt to large and complex environments, and has been used to play popular video games, such as Doom. Thus, the actor-critic approach fits well in trading with a large stock portfolio.A2C: A2C is a typical actor-critic algorithm. A2C uses copies of the same agent working in parallel to update gradients with different data samples. Each agent works independently to interact with the same environment.PPO: PPO is introduced to control the policy gradient update and ensure that the new policy will not be too different from the previous one.DDPG: DDPG combines the frameworks of both Q-learning and policy gradient, and uses neural networks as function approximators.Part 3: How to use DRL to trade stocks?Image by Markus on Unsplash3.1 DataWe track and select the Dow Jones 30 stocks (at 2016/01/01) and use historical daily data from 01/01/2009 to 05/08/2020 to train the agent and test the performance. The dataset is downloaded from Compustat database accessed through Wharton Research Data Services (WRDS).The whole dataset is split in the following figure. Data from 01/01/2009 to 12/31/2014 is used for training, and the data from 10/01/2015 to 12/31/2015 is used for validation and tuning of parameters. Finally, we test our agent’s performance on trading data, which is the unseen out-of-sample data from 01/01/2016 to 05/08/2020. To better exploit the trading data, we continue training our agent while in the trading stage, since this will help the agent to better adapt to the market dynamics.Copyright by AI4Finance LLC3.2 MDP model for stock trading:• State 𝒔 = [𝒑, 𝒉, 𝑏]: a vector that includes stock prices 𝒑 ∈ R+^D, the stock shares 𝒉 ∈ Z+^D, and the remaining balance 𝑏 ∈ R+, where 𝐷 denotes the number of stocks and Z+ denotes non-negative integers.• Action 𝒂: a vector of actions over 𝐷 stocks. The allowed actions on each stock include selling, buying, or holding, which result in decreasing, increasing, and no change of the stock shares 𝒉, respectively.• Reward 𝑟(𝑠,𝑎,𝑠′):the direct reward of taking action 𝑎 at state 𝑠 and arriving at the new state 𝑠′.• Policy 𝜋 (𝑠): the trading strategy at state 𝑠, which is the probability distribution of actions at state 𝑠.• Q-value 𝑄𝜋 (𝑠, 𝑎): the expected reward of taking action 𝑎 at state 𝑠 following policy 𝜋 .The state transition of our stock trading process is shown in the following figure. At each state, one of three possible actions is taken on stock 𝑑 (𝑑 = 1, …, 𝐷) in the portfolio.Selling 𝒌[𝑑] ∈ [1,𝒉[𝑑]] shares results in 𝒉𝒕+1[𝑑] = 𝒉𝒕 [𝑑] − 𝒌[𝑑],where𝒌[𝑑] ∈Z+ and𝑑 =1,…,𝐷.Holding, 𝒉𝒕+1[𝑑]=𝒉𝒕[𝑑].Buying 𝒌[𝑑] shares results in 𝒉𝒕+1[𝑑] = 𝒉𝒕 [𝑑] + 𝒌[𝑑].At time 𝑡 an action is taken and the stock prices update at 𝑡+1, accordingly the portfolio values may change from “portfolio value 0” to “portfolio value 1”, “portfolio value 2”, or “portfolio value 3”, respectively, as illustrated in Figure 2. Note that the portfolio value is 𝒑𝑻 𝒉 + 𝑏.Copyright by AI4Finance LLC3.3 Constraints:Market liquidity: The orders can be rapidly executed at the close price. We assume that stock market will not be affected by our reinforcement trading agent.Nonnegative balance: the allowed actions should not result in a negative balance.Transaction cost: transaction costs are incurred for each trade. There are many types of transaction costs such as exchange fees, execution fees, and SEC fees. Different brokers have different commission fees. Despite these variations in fees, we assume that our transaction costs to be 1/1000 of the value of each trade (either buy or sell).Risk-aversion for market crash: there are sudden events that may cause stock market crash, such as wars, collapse of stock market bubbles, sovereign debt default, and financial crisis. To control the risk in a worst-case scenario like 2008 global financial crisis, we employ the financial turbulence index that measures extreme asset price movements.3.4 Return maximization as trading goalWe define our reward function as the change of the portfolio value when action 𝑎 is taken at state 𝑠 and arriving at new state 𝑠 + 1.The goal is to design a trading strategy that maximizes the change of the portfolio value 𝑟(𝑠𝑡,𝑎𝑡,𝑠𝑡+1) in the dynamic environment, and we employ the deep reinforcement learning method to solve this problem.Image by Isaac on Unsplash3.5 Environment for multiple stocks:State Space: We use a 181-dimensional vector (30 stocks * 6 + 1) consists of seven parts of information to represent the state space of multiple stocks trading environmentBalance: available amount of money left in the account at current time stepPrice: current adjusted close price of each stock.Shares: shares owned of each stock.MACD: Moving Average Convergence Divergence (MACD) is calculated using close price.RSI: Relative Strength Index (RSI) is calculated using close price.CCI: Commodity Channel Index (CCI) is calculated using high, low and close price.ADX: Average Directional Index (ADX) is calculated using high, low and close price.Action Space:For a single stock, the action space is defined as {-k,…,-1, 0, 1, …, k}, where k and -k presents the number of shares we can buy and sell, and k ≤h_max while h_max is a predefined parameter that sets as the maximum amount of shares for each buying action.For multiple stocks, therefore the size of the entire action space is (2k+1)^30.The action space is then normalized to [-1, 1], since the RL algorithms A2C and PPO define the policy directly on a Gaussian distribution, which needs to be normalized and symmetric.class StockEnvTrain(gym.Env):“””A stock trading environment for OpenAI gym””” metadata = {‘render.modes’: [‘human’]}def __init__(self, df, day = 0): self.day = day self.df = df # Action Space # action_space normalization and shape is STOCK_DIM self.action_space = spaces.Box(low = -1, high = 1,shape = (STOCK_DIM,))   # State Space # Shape = 181: [Current Balance]+[prices 1–30]+[owned shares 1–30]  # +[macd 1–30]+ [rsi 1–30] + [cci 1–30] + [adx 1–30] self.observation_space = spaces.Box(low=0, high=np.inf, shape = (181,)) # load data from a pandas dataframe self.data = self.df.loc[self.day,:] self.terminal = False  # initalize state self.state = [INITIAL_ACCOUNT_BALANCE] + \ self.data.adjcp.values.tolist() + \ [0]*STOCK_DIM + \ self.data.macd.values.tolist() + \ self.data.rsi.values.tolist() + \ self.data.cci.values.tolist() + \ self.data.adx.values.tolist() # initialize reward self.reward = 0 self.cost = 0 # memorize all the total balance change self.asset_memory = [INITIAL_ACCOUNT_BALANCE] self.rewards_memory = [] self.trades = 0 #self.reset() self._seed()3.6 Trading agent based on deep reinforcement learningA2CCopyright by AI4Finance LLCA2C is a typical actor-critic algorithm which we use as a component in the ensemble method. A2C is introduced to improve the policy gradient updates. A2C utilizes an advantage function to reduce the variance of the policy gradient. Instead of only estimates the value function, the critic network estimates the advantage function. Thus, the evaluation of an action not only depends on how good the action is, but also considers how much better it can be. So that it reduces the high variance of the policy networks and makes the model more robust.A2C uses copies of the same agent working in parallel to update gradients with different data samples. Each agent works independently to interact with the same environment. After all of the parallel agents finish calculating their gradients, A2C uses a coordinator to pass the average gradients over all the agents to a global network. So that the global network can update the actor and the critic network. The presence of a global network increases the diversity of training data. The synchronized gradient update is more cost-effective, faster and works better with large batch sizes. A2C is a great model for stock trading because of its stability.DDPGCopyright by AI4Finance LLCDDPG is an actor-critic based algorithm which we use as a component in the ensemble strategy to maximize the investment return. DDPG combines the frameworks of both Q-learning and policy gradient, and uses neural networks as function approximators. In contrast with DQN that learns indirectly through Q-values tables and suffers the curse of dimensionality problem, DDPG learns directly from the observations through policy gradient. It is proposed to deterministically map states to actions to better fit the continuous action space environment.PPOWe explore and use PPO as a component in the ensemble method. PPO is introduced to control the policy gradient update and ensure that the new policy will not be too different from the older one. PPO tries to simplify the objective of Trust Region Policy Optimization (TRPO) by introducing a clipping term to the objective function.The objective function of PPO takes the minimum of the clipped and normal objective. PPO discourages large policy change move outside of the clipped interval. Therefore, PPO improves the stability of the policy networks training by restricting the policy update at each training step. We select PPO for stock trading because it is stable, fast, and simpler to implement and tune.Ensemble strategyOur purpose is to create a highly robust trading strategy. So we use an ensemble method to automatically select the best performing agent among PPO, A2C, and DDPG to trade based on the Sharpe ratio. The ensemble process is described as follows:Step 1. We use a growing window of 𝑛 months to retrain our three agents concurrently. In this paper, we retrain our three agents at every three months.Step 2. We validate all three agents by using a 3-month validation rolling window followed by training to pick the best performing agent which has the highest Sharpe ratio. We also adjust risk-aversion by using turbulence index in our validation stage.Step 3. After validation, we only use the best model with the highest Sharpe ratio to predict and trade for the next quarter.from stable_baselines import SACfrom stable_baselines import PPO2from stable_baselines import A2Cfrom stable_baselines import DDPGfrom stable_baselines import TD3from stable_baselines.ddpg.policies import DDPGPolicyfrom stable_baselines.common.policies import MlpPolicyfrom stable_baselines.common.vec_env import DummyVecEnvdef train_A2C(env_train, model_name, timesteps=10000): “””A2C model””” start = time.time() model = A2C(‘MlpPolicy’, env_train, verbose=0) model.learn(total_timesteps=timesteps) end = time.time() model.save(f”{config.TRAINED_MODEL_DIR}/{model_name}”) print(‘Training time (A2C): ‘, (end-start)/60,’ minutes’) return modeldef train_DDPG(env_train, model_name, timesteps=10000): “””DDPG model””” start = time.time() model = DDPG(‘MlpPolicy’, env_train) model.learn(total_timesteps=timesteps) end = time.time() model.save(f”{config.TRAINED_MODEL_DIR}/{model_name}”) print(‘Training time (DDPG): ‘, (end-start)/60,’ minutes’) return modeldef train_PPO(env_train, model_name, timesteps=50000): “””PPO model””” start = time.time() model = PPO2(‘MlpPolicy’, env_train) model.learn(total_timesteps=timesteps) end = time.time() model.save(f”{config.TRAINED_MODEL_DIR}/{model_name}”) print(‘Training time (PPO): ‘, (end-start)/60,’ minutes’) return modeldef DRL_prediction(model, test_data, test_env, test_obs): “””make a prediction””” start = time.time() for i in range(len(test_data.index.unique())):   action, _states = model.predict(test_obs)   test_obs, rewards, dones, info = test_env.step(action)   # env_test.render() end = time.time()3.7 Performance evaluationsWe use Quantopian’s pyfolio to do the backtesting. The charts look pretty good, and it takes literally one line of code to implement it. You just need to convert everything into daily returns.import pyfoliowith pyfolio.plotting.plotting_context(font_scale=1.1): pyfolio.create_full_tear_sheet(returns = ensemble_strat, benchmark_rets=dow_strat, set_context=False)Copyright by AI4Finance LLCCopyright by AI4Finance LLCReferences:A2C:Volodymyr Mnih, Adrià Badia, Mehdi Mirza, Alex Graves, Timothy Lillicrap, Tim Harley, David Silver, and Koray Kavukcuoglu. 2016. Asynchronous methods for deep reinforcement learning. The 33rd International Conference on Machine Learning (02 2016). https://arxiv.org/abs/1602.01783DDPG:Timothy Lillicrap, Jonathan Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa, David Silver, and Daan Wierstra. 2015. Continuous control with deep reinforcement learning. International Conference on Learning Representations (ICLR) 2016 (09 2015). https://arxiv.org/abs/1509.02971PPO:John Schulman, Sergey Levine, Philipp Moritz, Michael Jordan, and Pieter Abbeel. 2015. Trust region policy optimization. In The 31st International Conference on Machine Learning. https://arxiv.org/abs/1502.05477John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. 2017. Proximal policy optimization algorithms. arXiv:1707.06347 (07 2017). https://arxiv.org/abs/1707.06347Written byBruce Yanghttps://github.com/AI4Finance-LLCFollow616 13 Sign up for The Daily PickBy Towards Data ScienceHands-on real-world examples, research,  tutorials, and cutting-edge techniques delivered Monday to Thursday. Make learning your daily ritual. Take a lookGet this newsletterBy signing up, you will create a Medium account if you don’t already have one. Review our Privacy Policy for more information about our privacy practices.Check your inboxMedium sent you an email at  to complete your subscription.616 616 13 Deep ReinforcementReinforcement LearningMachine LearningStock TradingMarkov Decision ProcessMore from Towards Data ScienceFollowA Medium publication sharing concepts, ideas, and codes.Read more from Towards Data ScienceMore From Medium5 YouTubers Data Scientists And ML Engineers Should Subscribe ToRichmond Alake in Towards Data Science7 Must-Haves in your Data Science CVElad Cohen in Towards Data Science21 amazing Youtube channels for you to learn AI, Machine Learning, and Data Science for freeJair Ribeiro in Towards Data ScienceThe Roadmap of Mathematics for Deep LearningTivadar Danka in Towards Data Science30 Examples to Master PandasSoner Yıldırım in Towards Data ScienceAn Ultimate Cheat Sheet for Data Visualization in PandasRashida Nasrin Sucky in Towards Data ScienceHow to Get Into Data Science Without a DegreeTerence S in Towards Data ScienceHow To Build Your Own Chatbot Using Deep LearningAmila Viraj in Towards Data ScienceAboutHelpLegalGet the Medium app"
Deep Q-Network (DQN)-II,https://towardsdatascience.com/deep-q-network-dqn-ii-b6bf911b6b2c?source=tag_archive---------1-----------------------,"Artificial Intelligence,Reinforcement Learning,Deep Learning,Deep R L Explained,Towards Data Science","This is the second post devoted to Deep Q-Network (DQN), in the “Deep Reinforcement Learning Explained” series, in which we will analyse some challenges that appear when we apply Deep Learning to Reinforcement Learning. We will also present in detail the code that solves the OpenAI Gym Pong game using the DQN network introduced in the previous post.Challenges in Deep Reinforcement LearningUnfortunately, reinforcement learning is more unstable when neural networks are used to represent the action-values, despite applying the wrappers introduced in the previous section. Training such a network requires a lot of data, but even then, it is not guaranteed to converge on the optimal value function. In fact, there are situations where the network weights can oscillate or diverge, due to the high correlation between actions and states.In order to solve this, in this section we will introduce two techniques used by the Deep Q-Network:Experience ReplayTarget NetworkThere are many more tips and tricks that researchers have discovered to make DQN training more stable and efficient, and we will cover the best of them in future posts in this series.Experience ReplayWe are trying to approximate a complex, nonlinear function, Q(s, a), with a Neural Network. To do this, we must calculate targets using the Bellman equation and then consider that we have a supervised learning problem at hand. However, one of the fundamental requirements for SGD optimization is that the training data is independent and identically distributed and when the Agent interacts with the Environment, the sequence of experience tuples can be highly correlated. The naive Q-learning algorithm that learns from each of these experiences tuples in sequential order runs the risk of getting swayed by the effects of this correlation.We can prevent action values from oscillating or diverging catastrophically using a large buffer of our past experience and sample training data from it, instead of using our latest experience. This technique is called replay buffer or experience buffer. The replay buffer contains a collection of experience tuples (S, A, R, S′). The tuples are gradually added to the buffer as we are interacting with the Environment. The simplest implementation is a buffer of fixed size, with new data added to the end of the buffer so that it pushes the oldest experience out of it.The act of sampling a small batch of tuples from the replay buffer in order to learn is known as experience replay. In addition to breaking harmful correlations, experience replay allows us to learn more from individual tuples multiple times, recall rare occurrences, and in general make better use of our experience.As a summary, the basic idea behind experience replay is to storing past experiences and then using a random subset of these experiences to update the Q-network, rather than using just the single most recent experience. In order to store the Agent’s experiences, we used a data structure called a deque in Python’s built-in collections library. It’s basically a list that you can set a maximum size on so that if you try to append to the list and it is already full, it will remove the first item in the list and add the new item to the end of the list. The experiences themselves are tuples of [observation, action, reward, done flag, next state] to keep the transitions obtained from the environment.Experience = collections.namedtuple(‘Experience’,            field_names=[‘state’, ‘action’, ‘reward’,            ‘done’, ‘new_state’])class ExperienceReplay:  def __init__(self, capacity):      self.buffer = collections.deque(maxlen=capacity)  def __len__(self):      return len(self.buffer)  def append(self, experience):      self.buffer.append(experience)    def sample(self, batch_size):      indices = np.random.choice(len(self.buffer), batch_size,                replace=False)      states, actions, rewards, dones, next_states =              zip([self.buffer[idx] for idx in indices])      return np.array(states), np.array(actions),                      np.array(rewards,dtype=np.float32),              np.array(dones, dtype=np.uint8),                 np.array(next_states)Each time the Agent does a step in the Environment, it pushes the transition into the buffer, keeping only a fixed number of steps (in our case, 10k transitions). For training, we randomly sample the batch of transitions from the replay buffer, which allows us to break the correlation between subsequent steps in the environment.Most of the experience replay buffer code is quite straightforward: it basically exploits the capability of the deque library. In the sample() method, we create a list of random indices and then repack the sampled entries into NumPy arrays for more convenient loss calculation.Target NetworkRemember that in Q-Learning, we update a guess with a guess, and this can potentially lead to harmful correlations. The Bellman equation provides us with the value of Q(s, a) via Q(s’, a’) . However, both the states s and s’ have only one step between them. This makes them very similar, and it’s very hard for a Neural Network to distinguish between them.When we perform an update of our Neural Networks’ parameters to make Q(s, a) closer to the desired result, we can indirectly alter the value produced for Q(s’, a’) and other states nearby. This can make our training very unstable.To make training more stable, there is a trick, called target network, by which we keep a copy of our neural network and use it for the Q(s’, a’) value in the Bellman equation.That is, the predicted Q values of this second Q-network called the target network, are used to backpropagate through and train the main Q-network. It is important to highlight that the target network’s parameters are not trained, but they are periodically synchronized with the parameters of the main Q-network. The idea is that using the target network’s Q values to train the main Q-network will improve the stability of the training.Later, when we present the code of the training loop, we will enter in more detail how to code the initialization and use of this target network.Deep Q-Learning AlgorithmThere are two main phases that are interleaved in the Deep Q-Learning Algorithm. One is where we sample the environment by performing actions and store away the observed experienced tuples in a replay memory. The other is where we select the small batch of tuples from this memory, randomly, and learn from that batch using a gradient descent (SGD) update step.These two phases are not directly dependent on each other and we could perform multiple sampling steps then one learning step, or even multiple learning steps with different random batches. In practice, you won’t be able to run the learning step immediately. You will need to wait till you have enough tuples of experiences in D.The rest of the algorithm is designed to support these steps. We can summarize the previous explanations with this pseudocode for the basic DQN algorithm that will guide our implementation of the algorithm:In the beginning, we need to create the main network and the target networks, and initialize an empty replay memory D. Note that memory is finite, so we may want to use something like a circular queue that retains the d most recent experience tuples. We also need to initialize the Agent, one of the main components, which interacts with the Environment.Note that we do not clear out the memory after each episode, this enables us to recall and build batches of experiences from across episodes.Coding the Training LoopHyperparameters and execution timeBefore going into the code, mention that DeepMind’s Nature paper contained a table with all the details about hyperparameters used to train its model on all 49 Atari games used for evaluation. DeepMind kept all those parameters the same for all games, but trained individual models for every game. The team’s intention was to show that the method is robust enough to solve lots of games with varying complexity, action space, reward structure, and other details using one single model architecture and hyperparameters.However, our goal in this post is to solve just the Pong game, a quite simple and straightforward game in comparison to other games in the Atari test set, so the hyperparameters in the paper are are not the most suitable for a didactic post like this one. For this reason, we decided to use more personalized parameter values for our Pong Environment that converges to mean score of 19.0 in a reasonable wall time, depending on the GPU type that colab assigns to our execution (about a couple of hours at most). Remember that we can know the type of GPU that has been assigned to our runtime environment with the command !nvidia-smi.Let’s start introducing the code in more detail. The entire code of this post can be found on GitHub (and can be run as a Colab google notebook using this link). We skip the import details of the packages, it is quite straightforward, and we focus on the explanation of the hyperparameters:DEFAULT_ENV_NAME = “PongNoFrameskip-v4” MEAN_REWARD_BOUND = 19.0 gamma = 0.99                    orbatch_size = 32                 replay_size = 10000             learning_rate = 1e-4            sync_target_frames = 1000        replay_start_size = 10000      eps_start=1.0eps_decay=.999985eps_min=0.02These DEFAULT_ENV_NAME identify the Environment to train on and MEAN_REWARD_BOUNDthe reward boundary to stop training. We will consider that the game has converged when our agent reaches an average of 19 games won (out of 21) in the last 100 games. The remaining parameters indicate:gammais the discount factorbatch_size, the minibatch sizelearning_rateis the learning ratereplay_sizethe replay buffer size (maximum number of experiences stored in replay memory)sync_target_framesindicates how frequently we sync model weights from the main DQN network to the target DQN network (how many frames in between syncing)replay_start_size the count of frames (experiences) to add to replay buffer before starting training.Finally, the hyperparameters related to the epsilon decay schedule are the same as the previous post:eps_start=1.0eps_decay=.999985eps_min=0.02AgentOne of the main components we need is an Agent, which interacts with the Environment, and saves the result of the interaction into the experience replay buffer. The Agent class that we will design already save directly the result of the interacts with the Environment into the experience replay buffer, performing these three steps of the sample phase indicated in the portion of the previous pseudocode:First of all, during the Agent’s initialization, we need to store references to the Environment and experience replay buffer D indicated as an argument in the creation of the Agent’s object as exp_buffer:class Agent:     def __init__(self, env, exp_buffer):        self.env = env        self.exp_buffer = exp_buffer        self._reset()def _reset(self):        self.state = env.reset()        self.total_reward = 0.0In order to perform Agent’s steps in the Environment and store its results in the experience replay memory we suggest the following code:def play_step(self, net, epsilon=0.0, device=”cpu”):    done_reward = None    if np.random.random() < epsilon:       action = env.action_space.sample()    else:       state_a = np.array([self.state], copy=False)       state_v = torch.tensor(state_a).to(device)       q_vals_v = net(state_v)       _, act_v = torch.max(q_vals_v, dim=1)       action = int(act_v.item())The method play_step uses an ϵ-greedy(Q) policy to select actions at every time step. In other words, with the probability epsilon (passed as an argument), we take the random action; otherwise, we use the past model to obtain the Q-values for all possible actions and choose the best.After obtaining the action the method performs the step in the Environment to get the next observation: next_state, reward and is_done:    new_state, reward, is_done, _ = self.env.step(action)    self.total_reward += rewardFinally, the method stores the observation in the experience replay buffer, and then handle the end-of-episode situation:    exp = Experience(self.state,action,reward,is_done,new_state)    self.exp_buffer.append(exp)    self.state = new_state    if is_done:       done_reward = self.total_reward       self._reset()    return done_rewardThe result of the function is the total accumulated reward if we have reached the end of the episode with this step, or None if not.Main LoopIn the initialization part, we create our environment with all required wrappers applied, the main DQN neural network that we are going to train, and our target network with the same architecture. We also create the experience replay buffer of the required size and pass it to the agent. The last things we do before the training loop are to create an optimizer, a buffer for full episode rewards, a counter of frames and a variable to track the best mean reward reached (because every time the mean reward beats the record, we will save the model in a file):env = make_env(DEFAULT_ENV_NAME)net = DQN(env.observation_space.shape,          env.action_space.n).to(device)target_net = DQN(env.observation_space.shape,          env.action_space.n).to(device)buffer = ExperienceReplay(replay_size)agent = Agent(env, buffer)epsilon = eps_startoptimizer = optim.Adam(net.parameters(), lr=learning_rate)total_rewards = []frame_idx = 0best_mean_reward = NoneAt the beginning of the training loop, we count the number of iterations completed and update epsilon as we introduced in the previous post. Next, the Agent makes a single step in the Environment (using as arguments the current neural network and value for epsilon). Remember that this function returns a non-None result only if this step is the final step in the episode. In this case, we report the progress in the console (count of episodes played, mean reward for the last 100 episodes and the current value of epsilon):while True:  frame_idx += 1  epsilon = max(epsilon*eps_decay, eps_min)  reward = agent.play_step(net, epsilon, device=device)  if reward is not None:     total_rewards.append(reward)     mean_reward = np.mean(total_rewards[-100:])     print(“%d: %d games, mean reward %.3f, (epsilon %.2f)” %           (frame_idx, len(total_rewards), mean_reward, epsilon))After, every time our mean reward for the last 100 episodes reaches a maximum, we report this in the console and save the current model parameters in a file. Also, if this mean rewards exceed the specified MEAN_REWARD_BOUND ( 19.0 in our case) then we stop training. The third if, helps us to ensure our experience replay buffer is large enough for training:if best_mean_reward is None or         best_mean_reward < mean_reward:             torch.save(net.state_dict(),                        DEFAULT_ENV_NAME + “-best.dat”)             best_mean_reward = mean_reward             if best_mean_reward is not None:             print(“Best mean reward updated %.3f” %                   (best_mean_reward))if mean_reward > MEAN_REWARD_BOUND:             print(“Solved in %d frames!” % frame_idx)             breakif len(buffer) < replay_start_size:             continueLearn phaseNow we will start to describe the part of the code, from the main loop, that refers to the phase where the network learn (a portion of the previous pseudocode):The whole code that we wrote for implementing this part is as follows:batch = buffer.sample(batch_size) states, actions, rewards, dones, next_states = batchstates_v = torch.tensor(states).to(device)next_states_v = torch.tensor(next_states).to(device)actions_v = torch.tensor(actions).to(device)rewards_v = torch.tensor(rewards).to(device)done_mask = torch.ByteTensor(dones).to(device)state_action_values = net(states_v).gather(1,                           actions_v.unsqueeze(-1)).squeeze(-1)next_state_values = target_net(next_states_v).max(1)[0]next_state_values[done_mask] = 0.0next_state_values = next_state_values.detach()expected_state_action_values=next_state_values * gamma + rewards_vloss_t = nn.MSELoss()(state_action_values,                      expected_state_action_values)optimizer.zero_grad()loss_t.backward()optimizer.step()if frame_idx % sync_target_frames == 0:   target_net.load_state_dict(net.state_dict())We are going to dissect it to facilitate its description since it is probably the most complex part to understand.The first thing to do is to sample a random mini-batch of transactions from the replay memory:batch = buffer.sample(batch_size) states, actions, rewards, dones, next_states = batchNext, the code wraps individual NumPy arrays with batch data in PyTorch tensors and copies them to GPU ( we are assuming that the CUDA device is specified in arguments):states_v = torch.tensor(states).to(device)next_states_v = torch.tensor(next_states).to(device)actions_v = torch.tensor(actions).to(device)rewards_v = torch.tensor(rewards).to(device)done_mask = torch.ByteTensor(dones).to(device)This code inspired by the code of Maxim Lapan. It is written in a form to maximally exploit the capabilities of the GPU by processing (in parallel) all batch samples with vector operations. But explained step by step it can be understood without problems.Then, we pass observations to the first model and extract the specific Q-values for the taken actions using the gather() tensor operation. The first argument to this function call is a dimension index that we want to perform gathering on. In this case, it is equal to 1, because it corresponds to actions dimension:state_action_values = net(states_v).gather(1,                           actions_v.unsqueeze(-1)).squeeze(-1)The second argument is a tensor of indices of elements to be chosen. Here it is a bit more complex to explain the code. Let’s try it!. Maxim Lapan suggest to use the functions unsqueeze() and squeeze(). Because the index should have the same number of dimensions as the data we are processing (2D in our case) it apply a unsqueeze()to the action_v (that is a 1D) to compute the index argument for the gather functions. Finally, to remove the extra dimensions we have created, we will use the squeeze()function. Let’s try to illustrate what a gather does in summary on a simple example case with a batch of four entries and four actions:Note that the result of gather() applied to tensors is a differentiable operation that will keep all gradients with respect to the final loss value.Now that we have calculated the state-action values for every transition in the replay buffer, we need to calculate target “y” for every transition in the replay buffer too. Both vectors are the ones we will use in the loss function. To do this, remember that we must use the target network.In the following code, we apply the target network to our next state observations and calculate the maximum Q-value along the same action dimension, 1:next_state_values = target_net(next_states_v).max(1)[0]Function max() returns both maximum values and indices of those values (so it calculates both max and argmax). Because in this case, we are interested only in values, we take the first entry of the result.Remember that if the transition in the batch is from the last step in the episode, then our value of the action doesn’t have a discounted reward of the next state, as there is no next state from which to gather the reward:next_state_values[done_mask] = 0.0Although we cannot go into detail, it is important to highlight that the calculation of the next state value by the target neural network shouldn’t affect gradients. To achieve this, we use thedetach() function of the PyTorch tensor, which makes a copy of it without connection to the parent’s operation, to prevent gradients from flowing into the target network’s graph:next_state_values = next_state_values.detach()Now, we can calculate the Bellman approximation value for the vector of targets (“y”), that is the vector of the expected state-action value for every transition in the replay buffer:expected_state_action_values=next_state_values * gamma + rewards_vWe have all the information required to calculate the mean squared error loss:loss_t = nn.MSELoss()(state_action_values,                      expected_state_action_values)The next piece of the training loop updates the main neural network using the SGD algorithm by minimizing the loss:optimizer.zero_grad()loss_t.backward()optimizer.step()Finally, the last line of the code syncs parameters from our main DQN network to the target DQN network every sync_target_frames:if frame_idx % sync_target_frames == 0:   target_net.load_state_dict(net.state_dict())And so far the code for the main loop!What is next?This is the second of three posts devoted to present the basics of Deep Q-Network (DQN), in which we present in detail the algorithm. In the next post, we will talk about the performance of the algorithm and also show how we can use it.Deep Reinforcement Learning Explained - Jordi TORRES.AIContent of this seriesWritten byJordi TORRES.AIProfessor at UPC Barcelona Tech & Barcelona Supercomputing Center. Research focuses on Supercomputing & Artificial Intelligence https://torres.ai @JordiTorresAIFollow99 1 Sign up for The Daily PickBy Towards Data ScienceHands-on real-world examples, research,  tutorials, and cutting-edge techniques delivered Monday to Thursday. Make learning your daily ritual. Take a lookGet this newsletterBy signing up, you will create a Medium account if you don’t already have one. Review our Privacy Policy for more information about our privacy practices.Check your inboxMedium sent you an email at  to complete your subscription.99 99 1 Artificial IntelligenceReinforcement LearningDeep LearningDeep R L ExplainedTowards Data ScienceMore from Towards Data ScienceFollowA Medium publication sharing concepts, ideas, and codes.Read more from Towards Data ScienceMore From Medium5 YouTubers Data Scientists And ML Engineers Should Subscribe ToRichmond Alake in Towards Data Science7 Must-Haves in your Data Science CVElad Cohen in Towards Data Science21 amazing Youtube channels for you to learn AI, Machine Learning, and Data Science for freeJair Ribeiro in Towards Data Science30 Examples to Master PandasSoner Yıldırım in Towards Data ScienceThe Roadmap of Mathematics for Deep LearningTivadar Danka in Towards Data ScienceAn Ultimate Cheat Sheet for Data Visualization in PandasRashida Nasrin Sucky in Towards Data ScienceHow to Get Into Data Science Without a DegreeTerence S in Towards Data Science4 Types of Projects You Must Have in Your Data Science PortfolioSara A. Metwalli in Towards Data ScienceAboutHelpLegalGet the Medium app"
Snake Played by a Deep Reinforcement Learning Agent,https://towardsdatascience.com/snake-played-by-a-deep-reinforcement-learning-agent-53f2c4331d36?source=tag_archive---------2-----------------------,"Deep Q Learning,Snake,Experience Replay,Reinforcement Learning,Artificial Neural Network","Image via melmagazine.comEver since I watched the Netflix documentary AlphaGo, I have been fascinated by Reinforcement Learning. Reinforcement Learning is comparable with learning in real life: you see something, you do something, and your act has positive or negative consequences. You learn from the consequences and adjust your actions accordingly. Reinforcement Learning has many applications, like autonomous driving, robotics, trading and gaming. In this post, I will show how the computer can learn to play the game Snake using Deep Reinforcement Learning.The BasicsIf you are familiar with Deep Reinforcement Learning, you can skip the following two sections.Reinforcement LearningThe concept behind Reinforcement Learning (RL) is easy to grasp. An agent learns by interacting with an environment. The agent chooses an action, and receives feedback from the environment in the form of states (or observations) and rewards. This cycle continues forever or until the agent ends in a terminal state. Then a new episode of learning starts. Schematically, it looks like this:Reinforcement Learning: an agent interacts with the environment by choosing actions and receiving observations (or states) and rewards.The goal of the agent is to maximize the sum of the rewards during an episode. In the beginning of the learning phase the agent explores a lot: it tries different actions in the same state. It needs this information to find the best actions possible for the states. When the learning continues, exploration decreases. Instead, the agent will exploit his moves: this means he will choose the action that maximizes the reward, based on his experience.Deep Reinforcement LearningDeep Learning uses artificial neural networks to map inputs to outputs. Deep Learning is powerful, because it can approximate any function with only one hidden layer¹. How does it work? The network exists of layers with nodes. The first layer is the input layer. Then the hidden layers transform the data with weights and activation functions. The last layer is the output layer, where the target is predicted. By adjusting the weights the network can learn patterns and improve its predictions.As the name suggests, Deep Reinforcement Learning is a combination of Deep Learning and Reinforcement Learning. By using the states as the input, values for actions as the output and the rewards for adjusting the weights in the right direction, the agent learns to predict the best action for a given state.Deep Reinforcement Learning in ActionLet’s apply these techniques to the famous game Snake. I bet you know the game, the goal is to grab as many apples as possible while not walking into a wall or the snake’s body. I build the game in Python with the turtle library.Me playing Snake.Defining Actions, Rewards and StatesTo prepare the game for a RL agent, let’s formalize the problem. Defining the actions is easy. The agent can choose between going up, right, down or left. The rewards and state space are a bit harder. There are multiple solutions, and one will work better than the other. For now, let’s try the following. If the snake grabs an apple, give a reward of 10. If the snake dies, the reward is -100. To help the agent, give a reward of 1 if the snake comes closer to the apple, and a reward of -1 if the snake moves away from the apple.There are a lot of options for the state: you can choose to give scaled coordinates of the snake and the apple or to give directions to the location of the apple. An important thing to do is to add the location of obstacles (the wall and body) so the agent learns to avoid dying. Below a summary of actions, state and rewards. Later in the article you can see how adjustments to the state affect performance.Actions, rewards and stateCreating the Environment and the AgentBy adding some methods to the Snake program, it’s possible to create a Reinforcement Learning environment. The added methods are: reset(self), step(self, action) and get_state(self) . Besides this it’s necessary to calculate the reward every time the agent takes a step (check out run_game(self)).The agent uses a Deep Q Network to find the best actions. The parameters are:# epsilon sets the level of exploration and decreases over timeparam[‘epsilon’] = 1 param[‘epsilon_min’] = .01param[‘epsilon_decay’] = .995# gamma: value immediate (gamma=0) or future (gamma=1) rewardsparam[‘gamma’] = .95# the batch size is needed for replaying previous experiencesparam[‘batch_size’] = 500# neural network parametersparam[‘learning_rate’] = 0.00025param[‘layer_sizes’] = [128, 128, 128]If you are interested in the code, you can find it on my GitHub.Snake Played by the AgentNow it is time for the key question! Does the agent learn to play the game? Let’s find out by observing how the agent interacts with the environment.The first games, the agent has no clue:The first games.The first apple! It still seems like the agent doesn’t know what he is doing…Finds the first apple… and hits the wall.End of game 13 and beginning of game 14:Improving!The agent learns: it doesn’t take the shortest path but finds his way to the apples.Game 30:Good job! New high score!Wow, the agent avoids the body of the snake and finds a fast way to the apples, after playing only 30 games!Playing with the State SpaceThe agent learns to play snake (with experience replay), but maybe it’s possible to change the state space and achieve similar or better performance. Let’s try the following four state spaces:State space ‘no direction’: don’t give the agent the direction the snake is going.State space ‘coordinates’: replace the location of the apple (up, right, down and/or left) with the coordinates of the apple (x, y) and the snake (x, y). The coordinates are scaled between 0 and 1.State space ‘direction 0 or 1’: the original state space.State space ‘only walls’: don’t tell the agent when the body is up, right, down or left, only tell it if there’s a wall.Can you make a guess and rank them from the best state space to the worst after playing 50 games?An agent playing snake prevents seeing the answer :)Made your guess?Here is a graph with the performance using the different state spaces:Defining the right state accelerates learning! This graph shows the mean return of the last twenty games for the different state spaces.It is clear that using the state space with the directions (the original state space) learns fast and achieves the highest return. But the state space using the coordinates is improving, and maybe it can reach the same performance when it trains longer. A reason for the slow learning might be the number of possible states: 20⁴*2⁴*4 = 1,024,000 different states are possible (the snake canvas is 20*20 steps, there are 2⁴ options for obstacles, and 4 options for the current direction). For the original state space the number of possible states is equal to: 3²*2⁴*4 = 576 (3 options each for above/below and left/right). 576 is more than 1,700 times smaller than 1,024,000. This influences the learning process.Playing with the RewardsWhat about the rewards? Is there a better way to program them?Recall that our rewards were formatted like this:Blooper #1: Walk in CirclesWhat if we change the reward -1 to 1? By doing this, the agent will receive a reward of 1 every time it survives a time step. This can slow down learning in the beginning, but in the end the agent won’t die, and that’s a pretty important part of the game!Well, does it work? The agent quickly learns how to avoid dying:Agent receives a reward of 1 for surviving a time step.-1, come back please!Blooper #2: Hit the WallNext try: change the reward for coming closer to the apple to -1, and the reward of grabbing an apple to 100, what will happen? You might think: the agent receives a -1 for every time step, so it will run to the apples as fast as possible! This could be the truth, but there’s another thing that might happen…The agent runs into the nearest wall to minimize the negative return.Experience ReplayOne secret behind fast learning of the agent (only needs 30 games) is experience replay. In experience replay the agent stores previous experiences and uses these experiences to learn faster. At every normal step, a number of replay steps (batch_size parameter) is performed. This works so well for Snake because given the same state action pair, there is low variance in reward and next state.Blooper #3: No Experience ReplayIs experience replay really that important? Let’s remove it! For this experiment a reward of 100 for eating an apple is used.This is the agent without using experience replay after playing 2500 games:Training without experience replay. Even though the agent played 2500 (!) games, the agent can’t play snake. Fast playing, otherwise it would take days to reach the 10000 games.After 3000 games, the highest number of apples caught in one game is 2.After 10000 games, the highest number is 3… Was this 3 learning or was it luck?It seems indeed that experience replay helps a lot, at least for these parameters, rewards and this state space. How many replay steps per step are necessary? The answers might surprise you. To answer this question we can play with the batch_size parameter (mentioned in the section Creating the Environment and the Agent). In the original experiment the value of batch_size was 500.An overview of returns with different experience replay batch sizes:Training 200 games with 3 different batch sizes: 1 (no experience replay), 2 and 4. Mean return of previous 20 episodes.Even with batch size 2 the agent learns to play the game. In the graph you can see the impact of increasing the batch size, the same performance is reached more than 100 games earlier if batch size 4 is used instead of batch size 2.ConclusionsThe solution presented in this article gives results. The agent learns to play snake and achieves a high score (number of apples eaten) between 40 and 60 after playing 50 games. That is way better than a random agent!The attentive reader would say: ‘The maximum score for this game is 399. Why doesn’t the agent achieve a score of anything close to 399? There’s a huge difference between 60 and 399!’ That’s right! And there is a problem with the solution from this article: the agent does not learn to avoid enclosing. The agent learns to avoid obstacles directly surrounding the snake’s head, but it can’t see the whole game. So the agent will enclose itself and die, especially when the snake is longer.Enclosing.An interesting way to solve this problem is to use pixels and Convolutional Neural Networks in the state space². Then it is possible for the agent to ‘see’ the whole game, instead of just nearby obstacles. It can learn to recognize the places it should go to avoid enclosing and get the maximum score.[1] K. Hornik, M. Stinchcombe, H. White, Multilayer feedforward networks are universal approximators (1989), Neural networks 2.5: 359–366[2] Mnih et al, Playing Atari with Deep Reinforcement Learning (2013)Written byHennie de HarderData ScientistFollow102 1 Sign up for The Daily PickBy Towards Data ScienceHands-on real-world examples, research,  tutorials, and cutting-edge techniques delivered Monday to Thursday. Make learning your daily ritual. Take a lookGet this newsletterBy signing up, you will create a Medium account if you don’t already have one. Review our Privacy Policy for more information about our privacy practices.Check your inboxMedium sent you an email at  to complete your subscription.102 102 1 Deep Q LearningSnakeExperience ReplayReinforcement LearningArtificial Neural NetworkMore from Towards Data ScienceFollowA Medium publication sharing concepts, ideas, and codes.Read more from Towards Data ScienceMore From Medium5 YouTubers Data Scientists And ML Engineers Should Subscribe ToRichmond Alake in Towards Data Science7 Must-Haves in your Data Science CVElad Cohen in Towards Data Science21 amazing Youtube channels for you to learn AI, Machine Learning, and Data Science for freeJair Ribeiro in Towards Data Science30 Examples to Master PandasSoner Yıldırım in Towards Data ScienceThe Roadmap of Mathematics for Deep LearningTivadar Danka in Towards Data ScienceAn Ultimate Cheat Sheet for Data Visualization in PandasRashida Nasrin Sucky in Towards Data ScienceHow to Get Into Data Science Without a DegreeTerence S in Towards Data Science4 Types of Projects You Must Have in Your Data Science PortfolioSara A. Metwalli in Towards Data ScienceAboutHelpLegalGet the Medium app"
Teaching AI to Learn How Humans Plan Efficiently,https://towardsdatascience.com/teaching-ai-to-learn-how-humans-plan-efficiently-1d031c8727b?source=tag_archive---------3-----------------------,"Reinforcement Learning,Machine Learning,Data Science,AI,Editors Pick","Image by Isaac on UnsplashHuman planning is hierarchical. Whether planning something simple like cooking dinner or something complex like a trip abroad, we usually begin with a rough mental sketch of the goals we want to achieve (“go to India, then return back home”). This sketch is then progressively refined into a detailed sequence of sub-goals (“book flight ticket”, “pack luggage”), sub-sub-goals, and so on, down to the actual sequence of bodily movements that is much more complicated than the original plan.Efficient planning requires knowledge of the abstract high-level concepts that constitute the essence of hierarchical plans. Yet how humans learn such abstractions remains a mystery.Here, we show that humans spontaneously form such high-level concepts in a way that allows them to plan efficiently given the tasks, rewards, and structure of their environment. We also show that this behavior is consistent with a formal model of the underlying computations, thus grounding these findings in established computational principles and relating them to previous studies of hierarchical planning.Example of hierarchical planning [6]The figure above depicts an example of hierarchical planning, namely how someone might plan to get from their office in Cambridge to purchase a dream wedding dress in Patna, India. Circles represent states and arrows represent actions that transition between states. Each state represents a cluster of states in the lower level. Thicker arrows indicate transitions between higher-level states, which often come to mind first.A Bayesian perspectiveWhen applied to computationally intelligent agents, hierarchical planning could enable models with advanced planning abilities. Hierarchical representations can be modeled from a Bayesian viewpoint by assuming a generative process for the structure of a particular environment. Existing work on this problem includes the development of a computational framework for acquiring hierarchical representations under a set of simplified assumptions on the hierarchy, i.e. modeling how people create clusters of states in their mental representations of reward-free environments in order to facilitate planning.In this work, we contribute a Bayesian cognitive model of hierarchical discovery that combines knowledge of clustering and rewards to predict cluster formation, and compare the model to data obtained from humans.We analyze situations with both static and dynamic reward mechanisms, finding that humans generalize information about rewards to high-level clusters and use information about rewards to create clusters, and that reward generalization and reward-based cluster formation can be predicted by our proposed model.Theoretical backgroundA key area where psychology and neuroscience combine is the formal understanding of human behavior in relation to assigned actions. We ask:What is the planning and methodology employed by human agents when faced with accomplishing some task? How do humans discover useful abstractions?This is especially interesting in light of the unique ability of humans and animals to adapt to new environments. Previous literature on animal learning suggests that this flexibility stems from a hierarchical representation of goals that allows for complex tasks to be broken up into low-level subroutines that can be extended across a variety of contexts.ChunkingThe process of chunking occurs when actions are stitched together into temporally extended action sequences that achieve distant goals. Chunking is often the result of the transfer of learning from a goal-directed system to a habitual system, which executes actions in a stereotyped way.From a computational standpoint, such a hierarchical representation allows for agents to quickly execute actions in an open loop, reuse familiar action sequences whenever a known problem is encountered, learn faster by tweaking established action sequences to solve problems reminiscent of those seen previously, and plan over extended time horizons. Agents do not need to be concerned with the minuscule tasks associated with goal achievement, e.g., the goal of going to the store being broken down into leaving the house, walking, and entering the store as opposed to getting up out of bed, moving the left foot forward, then the right one, etc.Hierarchical reinforcement learningThe question of how agents should make rewarding decisions is the subject of reinforcement learning. Hierarchical reinforcement learning (HRL) has become the prevailing framework for representing hierarchical learning and planning. Within research on modeling of HRL, several ideas have been presented around potential methods of model construction.We focus on the idea that people spontaneously organize their environment into clusters of states that constrain planning. Such hierarchical planning is more efficient in time and memory than naive or flat planning, which include low-level actions and is consistent with people’s limited working memory capacity [3].In the diagram below, thick nodes and edges indicate that they must all be considered and maintained in short-term memory in order to compute the plan, and gray arrows indicate cluster membership. We observe that planning how to get from state s to state g in the low-level graph G takes at least as many steps as actually executing the plan (top), introducing high-level graph H alleviates this problem reduces computational costs (middle), and extending the hierarchy recursive further reduces the time and memory involved in planning (bottom).Hierarchical representations reduce the computational costs of planning [6]Solway et al. provide a formal definition of an optimal hierarchy, but they do not specify how the brain might discover it [2]. We hypothesize that an optimal hierarchy depends on the structure of environment, including both graph structure and the distribution of observable features of the environment, specifically rewards.ModelWe assume that agents represent their environment as a graph, where nodes are states in the environment and edges are transitions between states. The states and transitions may be abstract or as concrete as subway stations and the train lines traveling between them.StructureWe represent the observable environment as graph G = (V, E) and the latent hierarchy as H. Both G and H are unweighted, undirected graphs. H consists of clusters, where each low-level node in G belongs to exactly one cluster, and bridges, or high-level edges, that connect these clusters. Bridges can exist between clusters k and k’ only if there is an edge between some v, v’ ∈ V such that v ∈ k and v’∈ k’, i.e., each high-level edge in H has a corresponding low-level edge in G.In the illustration below, colors denote cluster assignments. Black edges are considered during planning, while gray edges are ignored by the planner. Thick edges correspond to transitions across clusters. The transition between clusters w and z is accomplished via a bridge.Example high-level graph (top) and low-level graph (bottom) [6]Prior to the addition of rewards, the learning algorithm discovers optimal hierarchies given the following constraints:Small clustersDense connectivity within clustersSparse connectivity across clustersHowever, we do not want clusters to be too small — in the extreme, each node is its own cluster, which renders the hierarchy useless. Additionally, while we want sparse connectivity across clusters, we want to maintain bridges across clusters in order to preserve properties of the underlying graphs.We use the discrete-time stochastic Chinese Restaurant Process (CRP) as a prior for clusters. The discovery of hierarchies can be accomplished by inverting the generative model to obtain the posterior probability of hierarchy H. The generative model formally presented in [6] generates such hierarchies.RewardsIn the context of the graph G, rewards can be interpreted as observable features of vertices. Because people often cluster based on observable features, it is reasonable to model clusters induced by rewards [5]. Furthermore, we assume that each state delivers a randomly determined reward and that the agent’s goal is to maximize the total reward [8].Since we hypothesize that clusters induce rewards, we model each cluster as having an average reward. Each node in that cluster has an average reward drawn from a distribution centered around the average cluster reward. Finally, each observed reward is drawn from a distribution centered around the average reward of that node. A formal discussion is provided in [1].To simplify inference, we first assume that rewards are constant, static. We label rewards that can change between observations with some fixed probability as dynamic.We conducted two experiments to test our hypothesis about human behavior and understand how well it could be predicted by our model. In particular, we studied to what degree clusters drive inferences about rewards, and to what degree rewards drive the formation of clusters. For each experiment, we collected human data and compared it to the predictions of the model.Clusters induce rewardsThe goal of the first experiment was to understand how rewards generalize within state clusters. We tested whether graph structure drives cluster formations and whether people generalize a reward observed at one node to the cluster that the node belongs to.SetupThe experiment was conducted by asking 32 human subjects to choose a node to visit next as specified in the following scenario. Participants were randomly presented with either the graph below or a flipped version of it, to ensure that bias of handedness or graph structure was not introduced. We predicted that participants would choose the node adjacent to the labeled one that was located in the larger cluster, i.e. the gray node to the left of the blue one in the first case, and the gray node to the right of the blue one in the second case.Participants were presented with the following task and associated graph:You work in a large gold mine that is composed of multiple individual mines and tunnels. The layout of the mines is shown in the diagram below (each circle represents a mine, and each line represents a tunnel). You are paid daily, and are paid $10 per gram of gold you found that day. You dig in exactly one mine per day, and record the amount of gold (in grams) that mine yielded that day. Over the last few months, you have discovered that, on average, each mine yields about 15 grams of gold per day. Yesterday, you dug in the blue mine in the diagram below, and got 30 grams of gold. Which of the two shaded mines will you dig in today? Please circle the mine you choose.Graph of mines presented to participants [1]We expected most participants to automatically identify the following clusters, with nodes colored in peach and lavender to denote the different clusters, and make a decision about which mine to select with these clusters in mind. It was hypothesized that participants would select a peach-colored node as opposed to a lavender one, since the node with label 30, a fairly larger than average reward, is in the peach-colored cluster.Graph of mines presented to participants, with likely clusters shown [1]InferenceWe approximated Bayesian inference over H using Metropolis-within-Gibbs sampling [4], which updates each component of H by sampling from its posterior, conditioning on all other components in a single Metropolis-Hastings step. We employed a Gaussian random walk as the proposal distribution for continuous components, and the conditional CRP prior as the proposal distribution for cluster assignments [7]. The approach can be interpreted as stochastic hill climbing with respect to a utility function defined by the posterior.ResultsThere were 32 participants in each of the human and simulated groups. The top three clusterings outputted by the model are shown below (left panel). All top three results were the same, indicating that the model identified the colored groupings with high confidence. The results for participants, as well as those for the static rewards model, are visualized in the bar chart below (right panel), depicting the proportion of human and simulated subjects who chose to visit node 2 next. The solid black line indicates the mean and the dotted black lines indicate the 2.5th and 97.5th percentiles.Results of the rewards generalization within clusters experiment [1]The listed p-values in the table below were calculated via a right-tailed binomial test, where the null was assumed to be a binomial distribution over choosing left or right gray node. The significance level was taken to be 0.05, and both the human experimental results and modeling results were statistically significant.Actions taken by humans and the static rewards model [1]Rewards induce clustersIn the second experiment, the goal was to determine whether rewards induce clusters. We predicted that nodes with the same reward positioned adjacent to each other would be clustered together, even if the structure of the graph alone would not induce clusters.Recall that Solway et. al showed that people prefer paths that cross the fewest hierarchy boundaries [2]. Therefore, between two otherwise identical paths, the only reason to prefer one over the other would be because it crosses fewer hierarchy boundaries. One possible counterargument to this is that people pick the path with higher rewards. However, in our setup detailed below, rewards are given only in the goal state, not cumulatively over the path taken. Additionally, the magnitude of rewards was varied between trials. Therefore, it is unlikely that people would favor a path because nodes along that path had higher rewards.SetupThis experiment was conducted on the web using Amazon Mechanical Turk (MTurk). Participants were given the following context about the task:Imagine you are a miner working in a network of gold mines connected by tunnels. Every mine yields a certain amount of gold (points) each day. On each day, your job is to navigate from a starting mine to a target mine and collect the points from the target mine. On some days, you will be free to choose any mine you like. On those days, you should try to pick the mine that yields the most points. On other days, only one mine will be available. The points of that mine will be written in green and the other mines will be grayed out. On those days, you should navigate to the available mine. The points of each mine will be written on it. The current mine will be highlighted with a thick border. You can navigate between mines using the arrow keys (up, down, left, right). Once you reach the target mine, press the space key to collect the points and start the next day. There will be 100 days (trials) in the experiment.The graph below (left panel) was presented to participants. As in the previous experiment, participants were randomly given either the configuration shown in or the horizontally-flipped version of the same graph in order to control for potential left-right asymmetry. Expected induced clusters are depicted as well, with nodes numbered for reference (right panel).Graph of mines presented to MTurk participants (left), with likely clusters shown (right) [1]We will refer to the first case, where participants are free to navigate to any mine, as free-choice and the second case, where participants navigate to a specified mine, as fixed-choice. Participants received a monetary reward for each trial to discourage random responses.At each trial, reward values were changed with probability 0.2. New rewards were drawn uniformly at random from the interval [0, 300]. However, the grouping of rewards remained the same across trials: nodes 1, 2, and 3 always had one reward value, nodes 4, 5, and 6 had a different reward value, and nodes 7, 8, 9, and 10 had a third reward value.The first 99 trials allowed the participant to develop a hierarchy of clusters. The final trial, which acted as the test trial, asked participants to navigate from node 6 to node 1. Assuming that rewards induced the clusters shown in above, we predicted that more participants would take the path through node 5, which crosses only one cluster boundary, over the one through node 7, which crosses two cluster boundaries.InferenceWe modeled the fixed-choice case, with the assumption that the tasks in all 100 trials were all the same as the 100th trial presented to participants, the test trial. First, we assumed static rewards, where the rewards remained constant across all trials. Next, we assumed dynamic rewards, where rewards changed for each trial.In contrast to the previous experiment, where the participant picks a single node the model predicts that node, this experiment is concerned with the second node of the full path the participant chose to take from the start node to the goal node. Therefore, in order to compare the model to human data, we used a variant of breadth-first search, hereafter referred to as hierarchical BFS, to predict a path from the start node (node 6) to the goal (node 1).Static rewards. For each subject, we sampled from the posterior using Metropolis-within-Gibbs sampling and chose the most probable hierarchy, i.e., the hierarchy with the highest posterior probability. Then, we used hierarchical BFS to first find a path between clusters and then between the nodes within the clusters.Dynamic rewards. For dynamic rewards, we used online inference. For each simulated participant, we allowed the sampling for each trial to progress only 10 steps. Then, we saved the hierarchy, and added information about the modified rewards. Next, we allowed sampling to progress again, starting from the saved hierarchy. As in the human experiment, at the beginning of each trial, there was a 0.2 probability that the rewards would be re-randomized to new values, although the rewards were always equal within clusters. This inference method simulated how human participants might learn cumulatively over the course of many trials. We assumed, for the purpose of this experiment, that people keep only one hierarchy in mind at a time, rather than updating multiple hierarchies in parallel. We also modified the log posterior to penalize disconnected clusters, because such clusters became much more common under this type of inference.ResultsThere were 95 participants in each of the human and two simulated groups. The null hypothesis is represented by an equal number of participants choosing a path through node 5 and through node 7, since in the absence of any other information and given that both paths are of equal length, a participant is equally likely to choose either.Actions taken by humans and the static and dynamic rewards models [1]As given in the table above, the results of the human experiment and static rewards modeling were statistically significant at α = 0.05. Furthermore, as shown below, the results of the human experiment are in the 90th percentile of a normal distribution centered around 0.5, the expected proportion given the null hypothesis. In the figure, we include clusterings identified by the static rewards model (first row), the static rewards model with cluster formation between disconnected components penalized second row), and the dynamic rewards model (third row).Clusters identified by simulations [1]Static rewards. We used 1000 iterations of Metropolis-within-Gibbs sampling to generate each sample, with a burnin and lag of 1 each. The simulation under static rewards certainly favors paths through node 5, to a level that is statistically significant. Moreover, since its purpose is to model human behavior, this result is meaningful in light of the human data being statistically significant as well (0.0321 < α = 0.05).Human and simulated subjects’ choices [1]Dynamic rewards. In order to mimic the human trials, we ran 100 trials, each with 10 iterations of Metropolis-within-Gibbs to sample from the posterior. The burnin and lag were again set to 1. The online inference method appears to have modeled human data better than modeling for static rewards, even though the group of simulated participants under dynamic rewards modeling is farther from the hypothesis than the group simulated under static rewards modeling. 56 human participants and 54 simulated participants under dynamic rewards modeling chose to go through node 5 (a 3.4% difference), compared to 64 simulated participants under static rewards modeling (an 18.5% difference).The bar chart above visualizes the proportion of human and simulated subjects whose chosen path’s second node was node 5. The solid black line indicates the expected proportion given the null hypothesis and the dotted black lines indicate the 10th and 90th percentiles.ConclusionsHumans seem to spontaneously organize environments into clusters of states that support hierarchical planning, enabling them to tackle challenging problems by breaking them down into sub-problems at various levels of abstraction. People constantly rely on such hierarchical presentations to accomplish tasks big and small — from planning one’s day, to organizing a wedding, to getting a PhD — often succeeding on the very first attempt.We have shown that an optimal hierarchy depends not only on graph structure, but also on observable characteristics of the environment, i.e., the distribution of rewards.We built hierarchical Bayesian models to understand how clusters induce static rewards and how both static and dynamic rewards induce clusters, and found that most results were statistically significant in terms of how closely our models captured human actions. All data and code files for all the simulations and experiments presented are available in the GitHub repository linked here. We hope that the model presented, as well as related results in a recent paper, pave the way for future studies to investigate the neural algorithms that support the essential cognitive ability of hierarchical planning.References[1] A. Kumar and S. Yagati, Reward Generalization and Reward-Based Hierarchy Discovery for Planning (2018), MIT[2] A. Solway, C. Diuk, N. Córdova, D. Yee, A. Barto, Y. Niv, and M. Botvinick, Optimal behavioral hierarchy (2014), PLOS Computational Biology[3] G. Miller, The magic number seven plus or minus two: Some limits on our capacity for processing information (1956), The Psychological Review[4] G. Roberts and J. Rosenthal, Examples of Adaptive MCMC (2009), Journal of Computational and Graphical Statistics[5] J. Balaguer, H. Spiers, D. Hassabis, and C. Summerfield, Neural mechanisms of hierarchical planning in a virtual subway network (2016), Neuron[6] M. Tomov, S. Yagati, A. Kumar, W. Yang, and S. Gershman, Discovery of hierarchical representations for efficient planning (2020), PLOS Computational Biology[7] R. Neal, Markov Chain Sampling Methods for Dirichlet Process Mixture Models (2000), Journal of Computational and Graphical Statistics[8] R. Sutton and A. Barto, Reinforcement Learning: An Introduction (2018), The MIT PressWritten byAgni KumarMachine Learning Engineer | MIT ’20 | agnikumar.ea@gmail.comFollow283 1 Sign up for The Daily PickBy Towards Data ScienceHands-on real-world examples, research,  tutorials, and cutting-edge techniques delivered Monday to Thursday. Make learning your daily ritual. Take a lookGet this newsletterBy signing up, you will create a Medium account if you don’t already have one. Review our Privacy Policy for more information about our privacy practices.Check your inboxMedium sent you an email at  to complete your subscription.283 283 1 Reinforcement LearningMachine LearningData ScienceAIEditors PickMore from Towards Data ScienceFollowA Medium publication sharing concepts, ideas, and codes.Read more from Towards Data ScienceMore From Medium5 YouTubers Data Scientists And ML Engineers Should Subscribe ToRichmond Alake in Towards Data Science7 Must-Haves in your Data Science CVElad Cohen in Towards Data Science21 amazing Youtube channels for you to learn AI, Machine Learning, and Data Science for freeJair Ribeiro in Towards Data ScienceThe Roadmap of Mathematics for Deep LearningTivadar Danka in Towards Data Science30 Examples to Master PandasSoner Yıldırım in Towards Data ScienceAn Ultimate Cheat Sheet for Data Visualization in PandasRashida Nasrin Sucky in Towards Data ScienceHow to Get Into Data Science Without a DegreeTerence S in Towards Data ScienceHow To Build Your Own Chatbot Using Deep LearningAmila Viraj in Towards Data ScienceAboutHelpLegalGet the Medium app"
Deep Q-Network (DQN)-I,https://towardsdatascience.com/deep-q-network-dqn-i-bce08bdf2af?source=tag_archive---------4-----------------------,"Artificial Intelligence,Reinforcement Learning,Deep Learning,Deep R L Explained,Towards Data Science","In the previous post, we have presented solution methods that represent the action-values in a small table. We referred to this table as a Q-table. In the next three posts of the “Deep Reinforcement Learning Explained” series, we will introduce the reader to the idea of using neural networks to expand the size of the problems that we can solve with reinforcement learning presenting the Deep Q-Network (DQN), that represents the optimal action-value function as a neural network, instead of a table. In this post, we will do an overview of DQN as well as introduce the OpenAI Gym framework of Pong. In the next two posts, we will present the algorithm and its implementation.Atari 2600 gamesThe Q-learning method that we have just covered in previous posts solves the issue by iterating over the full set of states. However often we realize that we have too many states to track. An example is Atari games, that can have a large variety of different screens, and in this case, the problem cannot be solved with a Q-table.The Atari 2600 game console was very popular in the 1980s, and many arcade-style games were available for it. The Atari console is archaic by today’s gaming standards, but its games still are challenging for computers and is a very popular benchmark within RL research (using an emulator)ATARI 2600 (source: Wikipedia)In 2015 DeepMind leveraged the so-called Deep Q-Network (DQN) or Deep Q-Learning algorithm that learned to play many Atari video games better than humans. The research paper that introduces it, applied to 49 different games, was published in Nature (Human-Level Control Through Deep Reinforcement Learning, doi:10.1038/nature14236, Mnih, and others) and can be found here.The Atari 2600 game environment can be reproduced through the Arcade Learning Environment in the OpenAI Gym framework. The framework has multiple versions of each game but for the purpose of this post, the Pong-v0 Environment will be used.We will study this algorithm because it really allows us to learn tips and tricks that will be very useful in future posts in this series. DeepMind’s Nature paper contained a table with all the details about hyperparameters used to train its model on all 49 Atari games used for evaluation. However, our goal here is much more modest: we want to solve just the Pong game.As we have done in some previous posts, the code presented in this post has been inspired by the code of Maxim Lapan who has written an excellent practical book on the subject.The entire code of this post can be found on GitHub and can be run as a Colab google notebook using this link.Our previous examples for FrozenLake, or CartPole, were not demanding from a computation requirements perspective, as observations were small. However, from now on, that’s not the case. The version of code shared in this post converges to a mean score of 19.0 in 2 hours (using a NVIDIA K80). So don’t get nervous during the execution of the training loop. ;-)PongPong is a table tennis-themed arcade video game featuring simple two-dimensional graphics, manufactured by Atari and originally released in 1972. In Pong, one player scores if the ball passes by the other player. An episode is over when one of the players reaches 21 points. In the OpenAI Gym framework version of Pong, the Agent is displayed on the right and the enemy on the left:In Pong, the two paddles move the ball back and forth. The score is kept by the numbers at the top of the screen. (source: torres.ai)There are three actions an Agent (player) can take within the Pong Environment: remaining stationary, vertical translation up, and vertical translation down. However, if we use the method action_space.n we can realize that the Environment has 6 actions:import gymimport gym.spacesDEFAULT_ENV_NAME = “PongNoFrameskip-v4”test_env = gym.make(DEFAULT_ENV_NAME)print(test_env.action_space.n)6Even though OpenAI Gym Pong Environment has six actions:print(test_env.unwrapped.get_action_meanings())[‘NOOP’, ‘FIRE’, ‘RIGHT’, ‘LEFT’, ‘RIGHTFIRE’, ‘LEFTFIRE’]three of the six being redundant (FIRE is equal to NOOP, LEFT is equal to LEFTFIRE and RIGHT is equal to RIGHTFIRE).DQN OverviewAt the heart of the Agent of this new approach, we found a deep neural network instead of a Q-table as we saw in the previous post. It should be noted that the Agent was only given raw pixel data, what a human player would see on screen, without access to the underlying game state, position of the ball, paddles, etc.As a reinforcement signal, it is fed back the change in game score at each time step. At the beginning, when the neural network is initialized with random values, it’s really bad, but overtime it begins to associate situations and sequences in the game with appropriate actions and learns to actually play the game well (that, without a doubt, the reader will be able to verify for himself with the code that will be presented in this series).Input spaceAtari games are displayed at a resolution of 210 by 60 pixels, with 128 possible colors for each pixel:print(test_env.observation_space.shape)(210, 160, 3)This is still technically a discrete state space but very large to process as it is and we can optimize it. To reduce this complexity, it is performed some minimal processing: convert the frames to grayscale, and scale them down to a square 84 by 84 pixel block. Now let’s think carefully if with this fixed image we can determine the dynamics of the game. There is certainly ambiguity in the observation, right? For example, we cannot know in which direction the ball is going). This obviously violates the Markov property.The solution is maintaining several observations from the past and using them as a state. In the case of Atari games, the authors of the paper suggested to stack 4 subsequent frames together and use them as the observation at every state. For this reason, the preprocessing stacks four frames together resulting in a final state space size of 84 by 84 by 4:Input state-space transformation (source: torres.ai)OutputUnlike until now we presented a traditional reinforcement learning setup where only one Q-value is produced at a time, the Deep Q-network is designed to produce in a single forward pass a Q-value for every possible action available in the Environment:(source: torres.ai)This approach of having all Q-values calculated with one pass through the network avoids having to run the network individually for every action and helps to increase speed significantly. Now, we can simply use this vector to take an action by choosing the one with the maximum value.Neural Network ArchitectureThe original DQN Agent used the same neural network architecture, for the all 49 games, that takes as an input an 84x84x4 image.The screen images are first processed by three convolutional layers. This allows the system to exploit spatial relationships, and can sploit spatial rule space. Also, since four frames are stacked and provided as input, these convolutional layers also extract some temporal properties across those frames. Using PyTorch, we can code the convolutional part of the model as:nn.Conv2d(input_shape, 32, kernel_size=8, stride=4),        nn.ReLU(),        nn.Conv2d(32, 64, kernel_size=4, stride=2),        nn.ReLU(),        nn.Conv2d(64, 64, kernel_size=3, stride=1),        nn.ReLU()where input_shape is the observation_space.shape of the Environment.The convolutional layers are followed by one fully-connected hidden layer with ReLU activation and one fully-connected linear output layer that produced the vector of action values:nn.Linear(conv_out_size, 512),         nn.ReLU(),         nn.Linear(512, n_actions)where conv_out_size is the number of values in the output from the convolution layer produced with the input of the given shape. This value is needed to pass to the first fully connected layer constructor and can be hard-coded due it is a function of the input shape (for 84x84 input, the output from the convolution layer will have 3136). However, in order to code a generic model (for all the games) that can accept different input shape, we will use a simple function, _get_conv_out that accepts the input shape and applies the convolution layer to a fake tensor of such a shape:def get_conv_out(self, shape):         o = self.conv(torch.zeros(1, *shape))         return int(np.prod(o.size()))conv_out_size = get_conv_out(input_shape)Another issue to solve is the requirement of feeding convolution output to the fully connected layer. But PyTorch doesn’t have a “flatter” layer and we need to reshape the batch of 3D tensors into a batch of 1D vectors. In our code, we suggest solving this problem in the forward() function, where we can reshape our batch of 3D tensors into a batch of 1D vectors using the view() function of the tensors.The view() function “reshape” a tensor with the same data and number of elements as input, but with the specified shape. The interesting thing of this function is that lets one single dimension be a -1 in which case it’s inferred from the remaining dimensions and the number of elements in the input (the method will do the math in order to fill that dimension). For example, if we have a tensor of shape (2, 3, 4, 6), which is a 4D tensor of 144 elements, we can reshape it into a 2D tensor with 2 rows and 72 columns using view(2,72). The same result could be obtained by view(2,-1), due [144/ (3*4*6) = 2].In our code, actually, the tensor has a batch size in the first dimension and we flatten a 4D tensor (the first dimension is batch size and the second is the color channel, which is our stack of subsequent frames; the third and fourth are image dimensions.)from the convolutional part to 2D tensor as an input to our fully connected layers to obtain Q-values for every batch input.The complete code for class DQN that we just described is written below:import torchimport torch.nn as nnimport numpy as npclass DQN(nn.Module):    def __init__(self, input_shape, n_actions):        super(DQN, self).__init__()self.conv = nn.Sequential(        nn.Conv2d(input_shape[0], 32, kernel_size=8, stride=4),        nn.ReLU(),        nn.Conv2d(32, 64, kernel_size=4, stride=2),        nn.ReLU(),        nn.Conv2d(64, 64, kernel_size=3, stride=1),        nn.ReLU()    )conv_out_size = self._get_conv_out(input_shape)self.fc = nn.Sequential(         nn.Linear(conv_out_size, 512),         nn.ReLU(),         nn.Linear(512, n_actions)    )def _get_conv_out(self, shape):         o = self.conv(torch.zeros(1, *shape))         return int(np.prod(o.size()))def forward(self, x):         conv_out = self.conv(x).view(x.size()[0], -1)         return self.fc(conv_out)We can use the print function to see a summary of the network architecture:DQN(  (conv): Sequential(    (0): Conv2d(4, 32, kernel_size=(8, 8), stride=(4, 4))    (1): ReLU()    (2): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2))    (3): ReLU()    (4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))    (5): ReLU()  )  (fc): Sequential(    (0): Linear(in_features=3136, out_features=512, bias=True)    (1): ReLU()    (2): Linear(in_features=512, out_features=6, bias=True)  ))OpenAI Gym WrappersIn DeepMind’s paper, several transformations (as the already introduced the conversion of the frames to grayscale, and scale them down to a square 84 by 84 pixel block) is applied to the Atari platform interaction in order to improve the speed and convergence of the method. In our example, that uses OpenAI Gym simulator, transformations are implemented as OpenAI Gym wrappers.The full list is quite lengthy and there are several implementations of the same wrappers in various sources. I used the version of Lapan’s Book that is based in the OpenAI Baselines repository. Let’s introduce the code for each one of them.For instance, some games as Pong require a user to press the FIRE button to start the game. The following code corresponds to the wrapper FireResetEnvthat presses the FIRE button in environments that require that for the game to start:class FireResetEnv(gym.Wrapper):   def __init__(self, env=None):       super(FireResetEnv, self).__init__(env)       assert env.unwrapped.get_action_meanings()[1] == ‘FIRE’       assert len(env.unwrapped.get_action_meanings()) >= 3def step(self, action):       return self.env.step(action)def reset(self):       self.env.reset()       obs, _, done, _ = self.env.step(1)       if done:          self.env.reset()       obs, _, done, _ = self.env.step(2)       if done:          self.env.reset()       return obsIn addition to pressing FIRE, this wrapper checks for several corner cases that are present in some games.The next wrapper that we will require is MaxAndSkipEnv that codes a couple of important transformations for Pong:class MaxAndSkipEnv(gym.Wrapper):    def __init__(self, env=None, skip=4):        super(MaxAndSkipEnv, self).__init__(env)        self._obs_buffer = collections.deque(maxlen=2)        self._skip = skipdef step(self, action):        total_reward = 0.0        done = None        for _ in range(self._skip):           obs, reward, done, info = self.env.step(action)           self._obs_buffer.append(obs)           total_reward += reward           if done:               break        max_frame = np.max(np.stack(self._obs_buffer), axis=0)        return max_frame, total_reward, done, infodef reset(self):       self._obs_buffer.clear()       obs = self.env.reset()       self._obs_buffer.append(obs)       return obsOn one hand, it allows us to speed up significantly the training by applying max to N observations (four by default) and returns this as an observation for the step. This is because on intermediate frames, the chosen action is simply repeated and we can make an action decision every N steps as processing every frame with a Neural Network is quite a demanding operation, but the difference between consequent frames is usually minor.On the other hand, it takes the maximum of every pixel in the last two frames and using it as an observation. Some Atari games have a flickering effect (when the game draws different portions of the screen on even and odd frames, a normal practice among Atari 2600 developers to increase the complexity of the game’s sprites), which is due to the platform’s limitation. For the human eye, such quick changes are not visible, but they can confuse a Neural Network.Remember that we already mentioned that before feeding the frames to the neural network every frame is scaled down from 210x160, with three color frames (RGB color channels), to a single-color 84 x84 image using a colorimetric grayscale conversion. Different approaches are possible. One of them is cropping non-relevant parts of the image and then scaling down as is done in the following code:class ProcessFrame84(gym.ObservationWrapper):     def __init__(self, env=None):         super(ProcessFrame84, self).__init__(env)         self.observation_space = gym.spaces.Box(low=0, high=255,                                shape=(84, 84, 1), dtype=np.uint8)def observation(self, obs):         return ProcessFrame84.process(obs)@staticmethod     def process(frame)         if frame.size == 210 * 160 * 3:             img = np.reshape(frame, [210, 160,  3])                                     .astype(np.float32)         elif frame.size == 250 * 160 * 3:             img = np.reshape(frame, [250, 160, 3])                                                   .astype(np.float32)         else:             assert False, “Unknown resolution.”                    img = img[:, :, 0] * 0.299 + img[:, :, 1] * 0.587 +                                           img[:, :, 2] * 0.114             resized_screen = cv2.resize(img, (84, 110),                                          interpolation=cv2.INTER_AREA)             x_t = resized_screen[18:102, :]             x_t = np.reshape(x_t, [84, 84, 1])             return x_t.astype(np.uint8)As we already discussed as a quick solution to the lack of game dynamics in a single game frame, the class BufferWrapper stacks several (usually four) subsequent frames together:class BufferWrapper(gym.ObservationWrapper):    def __init__(self, env, n_steps, dtype=np.float32):        super(BufferWrapper, self).__init__(env)        self.dtype = dtype        old_space = env.observation_space        self.observation_space =                 gym.spaces.Box(old_space.low.repeat(n_steps,                  axis=0),old_space.high.repeat(n_steps, axis=0),                      dtype=dtype)    def reset(self):        self.buffer = np.zeros_like(self.observation_space.low,        dtype=self.dtype)        return self.observation(self.env.reset())def observation(self, observation):        self.buffer[:-1] = self.buffer[1:]        self.buffer[-1] = observation        return self.bufferThe input shape of the tensor has a color channel as the last dimension, but PyTorch’s convolution layers assume the color channel to be the first dimension. This simple wrapper changes the shape of the observation from HWC (height, width, channel) to the CHW (channel, height, width) format required by PyTorch:class ImageToPyTorch(gym.ObservationWrapper):    def __init__(self, env):        super(ImageToPyTorch, self).__init__(env)        old_shape = self.observation_space.shape        self.observation_space = gym.spaces.Box(low=0.0, high=1.0,                                            shape=(old_shape[-1],                                 old_shape[0], old_shape[1]),                                dtype=np.float32)def observation(self, observation):      return np.moveaxis(observation, 2, 0)The screen obtained from the emulator is encoded as a tensor of bytes with values from 0 to 255, which is not the best representation for an NN. So, we need to convert the image into floats and rescale the values to the range [0.0…1.0]. This is done by the ScaledFloatFrame wrapper:class ScaledFloatFrame(gym.ObservationWrapper):     def observation(self, obs):         return np.array(obs).astype(np.float32) / 255.0Finally, it will be helpful for the following simple function make_env that creates an environment by its name and applies all the required wrappers to it:def make_env(env_name):    env = gym.make(env_name)    env = MaxAndSkipEnv(env)    env = FireResetEnv(env)    env = ProcessFrame84(env)    env = ImageToPyTorch(env)     env = BufferWrapper(env, 4)    return ScaledFloatFrame(env)What is next?This is the first of three posts devoted to Deep Q-Network (DQN), in which we provide an overview of DQN as well as an introduction of the OpenAI Gym framework of Pong. In the next two posts (Post 16, Post 17), we will present the algorithm and its implementation, where we will cover several tricks for DQNs to improve their training stability and convergence.Deep Reinforcement Learning Explained by BSC & UPCContent of this seriestorres.aiWritten byJordi TORRES.AIProfessor at UPC Barcelona Tech & Barcelona Supercomputing Center. Research focuses on Supercomputing & Artificial Intelligence https://torres.ai @JordiTorresAIFollow5 Sign up for The Daily PickBy Towards Data ScienceHands-on real-world examples, research,  tutorials, and cutting-edge techniques delivered Monday to Thursday. Make learning your daily ritual. Take a lookGet this newsletterBy signing up, you will create a Medium account if you don’t already have one. Review our Privacy Policy for more information about our privacy practices.Check your inboxMedium sent you an email at  to complete your subscription.5 5 Artificial IntelligenceReinforcement LearningDeep LearningDeep R L ExplainedTowards Data ScienceMore from Towards Data ScienceFollowA Medium publication sharing concepts, ideas, and codes.Read more from Towards Data ScienceMore From Medium5 YouTubers Data Scientists And ML Engineers Should Subscribe ToRichmond Alake in Towards Data Science7 Must-Haves in your Data Science CVElad Cohen in Towards Data Science21 amazing Youtube channels for you to learn AI, Machine Learning, and Data Science for freeJair Ribeiro in Towards Data Science30 Examples to Master PandasSoner Yıldırım in Towards Data ScienceThe Roadmap of Mathematics for Deep LearningTivadar Danka in Towards Data ScienceAn Ultimate Cheat Sheet for Data Visualization in PandasRashida Nasrin Sucky in Towards Data ScienceHow to Get Into Data Science Without a DegreeTerence S in Towards Data Science4 Types of Projects You Must Have in Your Data Science PortfolioSara A. Metwalli in Towards Data ScienceAboutHelpLegalGet the Medium app"
Building a DQN in PyTorch: Balancing Cart Pole with Deep RL,https://blog.gofynd.com/building-a-deep-q-network-in-pytorch-fa1086aa5435?source=tag_archive---------5-----------------------,"Reinforcement Learning,Deep Learning,Artificial Neural Network,Machine Learning,Technology","Design By Nancy KatariaIntroductionHi Geeks, welcome to Part-3 of our Reinforcement Learning Series. In the last two blogs, we covered some basic concepts in RL and also studied the multi-armed bandit problem and its solution methods. This blog will be a bit longer as we will first learn some new concepts and then we will apply Deep Learning to build a deep RL agent. We will then train this agent to balance the cart pole.The code repository corresponding to this blog can be accessed here.The Cart Pole Balancing ProblemWe will be using the CartPole-v0 environment provided by OpenAI GYM. I am still including a complete environment description here for the sake of completeness.DescriptionA pole is attached by an un-actuated joint to a cart, which moves along a frictionless track. The pendulum starts upright, and the goal is to prevent it from falling over by increasing and reducing the cart’s velocity.Cart Pole EnvironmentState SpaceThe observation of this environment is a four tuple :Action SpaceThere are just two possible actions: Left or Right, corresponding to the direction in which the agent can push the cart pole.RewardThe reward is 1 for every step taken, including the termination step.Starting StateAll observations are assigned a uniform random value between ±0.05.Episode Termination1. Pole Angle is more than ±12°2. Cart Position is more than ±2.4 (center of the cart reaches the edge of the display)3. Episode length is greater than 200 (500 for v1).Solved RequirementsConsidered solved when the average reward is greater than or equal to 195.0 over 100 consecutive trials.The Behavior of a Random AgentWe will first check the average reward that a random agent can earn. By Random Agent, I am referring to an agent selecting actions randomly i.e without using any environment information. Running this snippet gave an average reward of 23.3 in my case. It may vary slightly in your case. But still, the problem is far from solved.The Real Question !!!Consider the following interaction between Agent and Environment.Taken From Reinforcement Learning — An IntroductionBased on the received observations and rewards from the environment, the agent selects some action. The agent must have some policy (aka strategy) according to which it selects an action. Just having a policy is not enough, the agent must have a mechanism to improve this strategy as it interacts more and more with the environment. Now (some of ) the questions are:1. How to represent a policy?2. How to evaluate the current policy ( Policy Evaluation)?3. How to improve the policy ( Policy Improvement )?Deep Learning to The RescueIdeally, we should first discuss these issues with traditional methods but that will make this blog very long. To summarize, we will still be using the traditional approaches but with the deep neural networks as function approximators. In the absence of deep neural network, to apply these algorithms we would need to store a table of dimension S x A where S is the number of possible states and A is the number of actions that can be taken in the environment. Even with a simple environment, this table is too large to be usable in practice.Let us see how we can use Deep Learning to address the above concerns :In DL we use neural networks as function approximators. We can represent our policy via Deep NN. This NN will look at the given observation and will tell us which action is best to take in the current state. We refer to such Deep Neural Networks as Policy Network.By policy evaluation, we mean to check how “good’ or “impactful” is our current policy. The loss of Policy Network can be used to check this. In this blog, we will use the Mean Squared Error between predicted and target returns to evaluate our policy network.The Policy Evaluation step gives us the loss value of the current policy network. With this information, we can use Gradient Descent to optimize the weights of the policy network to minimize this loss. In this way, the policy network can be improved.Deep Q-NetworkA DQN is a Q-value function approximator. At each time step, we pass the current environment observations as input. The output is the Q-value corresponding to each possible action.Q-NetworkBut wait… where are the ground truths ???In Supervised Learning, we have a ground truth corresponding to each input data point. The network prediction can be compared against the corresponding ground truth to evaluate its performance. But here we do not have the ground truths or at least not in the popular sense.In most cases, we do not have the exact dynamics of the environment. That means we do not exactly know the value of selecting an action in a state even if the environment dynamics are known, then we would need to run the agent-environment interaction for a sufficiently long time or ideally until the end of the episodes. Then we can go back and update the ground-truth value. Note that this also means that we would need to store the entire sequence of interaction which is not feasible in most scenarios.Discounted Returns as Ground TruthThe value of taking action a in state s i.e q(s, a) can be written as :q(s{t}, a{t} ) = R{t} + γ * R{t+1} + γ² * R{t+2} + γ³ * R{t+3} + …….where γ is the discount factor, the value of which belongs to the interval [0,1]. The idea here is that we care not only for the immediate rewards but also for the future rewards that can result after taking this action.The discount rate determines the present value of future rewards: a reward received k time steps in the future is worth only pow(γ ,k-1) times what it would be worth if it were received immediately¹.With a bit rearrangement, the above equation can be simplified to :q(s{t}, a{t} ) = R{t} + γ * MAX-OVER-ACTION q( s(t+1), a)Training AlgorithmStep-1: Initialize game state and get initial observations.Step-2: Input the observation (obs) to Q-network and get Q-value corresponding to each action. Store the maximum of the q-value in X.Step-3: With a probability, epsilon selects random action otherwise select action corresponding to max q-value. Step-4: Execute the selected action in the game state and collect the generated reward( r{t} ) and next state observation(obs_next).Step-5: Pass these next state observation through Q-network and store the maximum of these Q-values in a variable say q_next_state. If the discount factor is Gamma then the ground truth can be calculated as :Y = r{t} + Gamma * q_next_stateStep-6: Take X as the predicted return of current state and Y as the actual return. Calculate loss and perform an optimization step.Step-7: Set obs = obs_next.Step-8: Repeat Step-2 to Step-7 for n episodes.Balancing Exploration and ExploitationIn the beginning, our agent has no idea of the environment dynamics. So we should let it explore and as it interacts with the environment and it should increasingly exploit its learning along with exploration. There is a need to balance this exploration and exploitation. We can either choose the action corresponding to maximum Q-value(exploitation) or with a small probability, epsilon, a random action can be selected(exploration). In this agent’s training, we started with epsilon = 1 i.e 100% exploration and slowly decrease it to 0.05.Catastrophic Forgetting and Need For Replay BufferThere is a serious issue with the above training process. After each step of agent-environment interaction, we are performing an optimization step. This can lead to catastrophic forgetting.Today’s deep learning methods struggle to learn rapidly in the incremental, online settings that are most natural for the reinforcement learning algorithms emphasized in this book. The problem is sometimes described as one of “catastrophic interference” or “correlated data.” When something new is learned it tends to replace what has previously been learned rather than adding to it, with the result that the benefit of the older learning is lost. Techniques such as “replay buffers” are often used to retain and replay old data so that its benefits are not permanently lost¹.So as you might have guessed by now, we will be using replay buffers to address this problem. The agent will gather the experience in replay buffer and then a random batch of experience will be sampled from this buffer. This batch will be used for training the agent using mini-batch gradient descent.Training Instability and Need for Two Identical Q-NetworkUntil now, the same Q-network is used for predicting the Q-value of the current state and next state. The Q-value of the next state is then used to calculate ground truth. In simple words,We executed our optimization step to bring the prediction close to ground truth but at the same time we are changing the weights of the network which gave us the ground truth. This causes instability in training.The solution is to have another network called Target Network which is an exact copy of the Main Network. This target network is used to generate target values or ground truth. The weights of this network are held fixed for a fixed number of training steps after which these are updated with the weight of Main Network. In this way, the distribution of our target return is also held fixed for some fixed iterations which increase training stability.Also, note that we are using the term policy network and q-network almost interchangeably but these are two different types of networks. Given a state, a policy network generates a probability distribution over actions while a Q-network generates Q-values corresponding to every action.https://quotefancy.com/Coding our DQN AgentIt seems quite natural to wrap our agent in a class. The agent receives state observations and rewards from the environment. It then acts on the environment based on current observation. The Deep Q-Network is the brain of our agent. The agent learns from interactions and adjusts the weight of Q-network accordingly. Let us quickly go through the code :The init function builds two identical deep neural networks. Before that we first seed torch random generator. In this way, the weights of the neural network are initialized deterministically.“Seed is also a Hyper-parameter” 🙂Kindly remove all occurrences of “.cuda()” from this code if you do not have Cuda support on your machine. The variable network_sync_freq donate the number of training steps to take before updating the target network with the weight of the main network. The variable network_sync_counter is incremented after each training step in train() function and is reset to 0 when it reaches network_sync_freq. The variable experience_replay is a deque. In train() function, the Q-value of the current state is estimated using Main Q-Network. The Q-value of the next state is calculated using Target Network, which is then used to calculate the target return.The rest of the code is pretty much self-explanatory.Deep Q-Network AgentDriver codeThe driver code is very simple. We first initialize both environment and agent. Then the replay buffer is filled to its full capacity, 256 in this case. Then we fix it for 4 training steps and during each training step, a batch of length 16 is sampled randomly from this buffer. Then the agent interacts with the environment for the next 128 time steps and collects the experience in the buffer. Note that since it is a deque after it is filled to its full capacity ( which we do before the main training loop), with each new experience inserted into it, one element from the front is also removed.To balance exploration and exploitation, we are using the epsilon-greedy strategy. We first promote full exploration by setting epsilon =1 and update it after each episode to slowly decrease it to 0.05.Driver Code for training the agentSome PlotsThis plot shows how reward varies as we make progress in training. Roughly after 6500 episodes, it scores maximum in each episode.Variation of Reward with episode2. This plot shows the variation in loss value as the training progress.x-axis: epoch, y-axis: loss3. This plot shows the variation of epsilon as the training progress.x-axis: epoch, y-axis: epsilonVideo Time !!!This video shows how gracefully our agent is balancing the cart pole. The Pole almost appears to be still. It scored the maximum score each time I tried. Although taking the average over a large number of episodes is a much better idea.CartPole Balancing via Deep Q-NetworkThe above video is generated by the following code snippet :LimitationsThere are some limitations to our DQN Agent. Let us look at some of them.Hacks… A lot of them !!!As you can easily observe that getting the right values of hyperparameters needs a lot of experimentation. Even the way neural networks are initialized has a significant effect on network training.Online vs Offline trainingDue to the need for a target network to stabilize training and use of replay buffer to address catastrophic forgetting, our agent is not trainable in an online manner.Bad GeneralizationI was not able to get the same agent work in other environments. The reason being is our agent is a very basic one. However, the agent described in the original DQN paper was able to generalize in different environments.ConclusionCombining Deep Learning and Reinforcement learning is very fascinating. Building this DQN and getting it to work was an amazing experience. But still, there are a lot of limitations to this approach. DQN was introduced in 2013. The DQN we implemented in this blog is a much simpler version of the proposed DQN. In the paper, it is described as :We refer to convolutional networks trained with our approach as Deep Q-Networks (DQN).After 2013 a lot of progress has been made in Deep Reinforcement Learning. There is a great compilation of the resources at this link. With this blog, I just tried to scratch the surface. There is a long way to go from here. So we will keep exploring!!!What’s Next: A Journey to AWS DeepRacerOur Team FRacer in Time Trial Race of AWS Deep RacerIn the next few blogs, we will take you through a journey of the AWS Deep Racer Platform. We will describe how we made use of these skills to break into the top 1% global ranking in the AWS DeepRacer Virtual Circuit contest for the month of August 2020.WHO IS THIS GUY?Hi, I am Mohit Pilkhan. I have got a lot of interest in designing AI-powered applications and serving them via cloud and edge devices. Currently, I am a member of the Machine Learning Team at Fynd.To give any feedback please write to mohit.pilkhan.77@gmail.com. Know more about our current research at https://research.fynd.com/.If you notice any discrepancy in this article please leave a comment below so that any issue can be addressed at earliest.ReferencesRichard Sutton and Andrew Barto. Reinforcement Learning: An Introduction.The MIT Press, 2018.Alex Irpan (2018). Deep Reinforcement Learning Doesn’t Work Yet. https://www.alexirpan.com/2018/02/14/rl-hard.html.Stuart Russell, Peter Norvig. Artificial Intelligence — A Modern Approach. Pearson, 2010.Alexander Zai and Brandon Brown. Deep Reinforcement Learning in Action. Manning Publication, 2020.Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan Wierstra, Martin Riedmiller (2013). Playing Atari with Deep Reinforcement Learning. ArXiv, abs/1312.5602.Gym is a toolkit for developing and comparing reinforcement learning algorithms. It supports teaching agents everything from walking to playing games like Pong or PinballSpinning Up is an educational resource produced by OpenAI that makes it easier to learn about deep reinforcement learning (deep RL).The Github repository corresponding to this blog can be accessed here.Building FyndLatest from our product and engineering teamsFollow170 1 Thanks to Rishab Sharma. Reinforcement LearningDeep LearningArtificial Neural NetworkMachine LearningTechnology170 claps170 claps1 responseWritten byMohit PilkhanFollowOpen Source Enthusiast | Machine Learning Engineer | mahakal001.github.io/portfolio/FollowBuilding FyndFollowLatest from our product and engineering teamsFollowWritten byMohit PilkhanFollowOpen Source Enthusiast | Machine Learning Engineer | mahakal001.github.io/portfolio/Building FyndFollowLatest from our product and engineering teamsMore From MediumHow to boost object detection accuracy by understanding dataTushar Kolhe in Building FyndWhy we stopped supporting Cash On Delivery and are loving it!Puneet Sachdev in Building FyndWhy we sell the way we do?Pritha Saha in Building FyndHow we broke into the top 1% of the AWS DeepRacer Virtual CircuitDS in Building FyndHow we broke into the top 1% of the AWS DeepRacer Virtual CircuitDS in Building FyndMulti-Armed Bandit: Solution MethodsMohit Pilkhan in Building FyndReinforcement Learning — A Quick OverviewMohit Pilkhan in Building FyndHow we Reduced our ML Training Costs by 78%!!Arjun Sunil in Building FyndLearn more.Medium is an open platform where 170 million readers come to find insightful and dynamic thinking."
Anatomy of a custom environment for RLlib,https://medium.com/distributed-computing-with-ray/anatomy-of-a-custom-environment-for-rllib-327157f269e5?source=tag_archive---------6-----------------------,"Reinforcement Learning,Machine Learning,Open Source,Python,Rllib","RLlib is an open-source library in Python, based on Ray, which is used for reinforcement learning (RL). This article presents a brief tutorial about how to build custom Gym environments to use with RLlib. You can use this as a starting point for representing your own use cases to solve with reinforcement learning (RL).Note that this article is a follow-up to:Intro to RLlib: Example EnvironmentsRLlib is an open-source library in Python, based on Ray, which is used for reinforcement learning (RL). This article…medium.comIf you haven’t read that previous article already, check it out.Source code for this article is available at https://github.com/DerwenAI/gym_exampleThe material here comes from Anyscale Academy and complements the RLlib documentation. This is intended for those who have:some Python programming experiencesome familiarity with machine learningan introduction to reinforcement learning and RLLib (see previous article)Key takeaways for this article include how to:represent a problem to solve with RLbuild custom Gym environments that work well with RLlibstructure a Git repo to support custom Gym environmentsregister a custom Gym environmenttrain a policy in RLlib using PPOrun a rollout of the trained policyThe backstoryOne of the first things that many people run when they’re learning about RL is the CartPole example environment. OpenAI Gym has an implementation called CartPole-v1 which has an animated rendering that many RL tutorials feature:CartPole-v1 `render()` visualizationThat has become a kind of “Hello World” for reinforcement learning, and the CartPole visualization of how RL trains a policy for an agent is super helpful.However, when it comes time to represent your own use case as an environment for RL, what should you use as a base? While there are many examples for Gym environments, the documentation is sparse. Moreover the those examples use class inheritance in ways that often make their source code difficult to follow. Plus, the requirements for structuring a Python library (as a Git repo) so that it can be registered as a custom environment — they are neither intuitive, simple to troubleshoot, nor especially well-documented.More importantly, we need to discuss the how and why of building a Gym environment to represent problems that are well suited for work with reinforcement learning. Effective problem representation is often a conceptual hurdle for Python developers who are just starting to use RL. Checking through the source code of popular Gym environments, many involve complex simulations and physics that have much to do with that specific use case … however, they contribute very little to understanding how one could build other environments in general.This article attempts to show a “minimum viable environment” for RLlib, one which illustrates and exercises all of the features needed to be an exemplary RL problem. Meanwhile the code is kept simple enough to generalize and adapt for other problems in general. Hopefully this will provide a starting point for representing your own use cases to train and solve using RLlib.Represent a problem for reinforcement learningLet’s work with a relatively simple example, one that illustrates most of the elements of effective problem representation for reinforcement learning. Consider an environment which is simply an array with index values ranging from min to max and with a goal position set at the middle of the array.At the beginning of each episode, the agent starts at a random index other than the goal. The agent does not know the goal’s position and must explore the environment — moving one index at a time — to discover it before the end of an episode.Think of this as a robotics equivalent of the popular children’s guessing game “Hot and Cold” where an agent must decide whether to step left or right, then afterwards gets told “You’re getting warmer” or “You’re getting colder” as feedback. In robotics — or let’s say within the field of control systems in general — there is often a problem of working through real-world sensors and actuators which have error rates or other potential distortions. Those systems must work with imperfect information as feedback. This kind of “Hot and Cold” game is what robots often must learn to “play” during calibration, much like children do.In terms of using RL to train a policy with this environment, at each step the agent will:observe its position in the arraydecide whether to move left or rightreceive a rewardWe want the agent to reach the goal as efficiently as possible, so we need to structure the rewards carefully. Each intermediate step returns a negative reward, while reaching the goal returns a large positive reward. The reward for moving away from the goal needs to be more negative than moving closer to it. That way fewer steps in the correct direction result in larger cumulative rewards.Let’s repeat a subtle point about structuring the rewards: a good intermediate step returns a less negative reward, while a bad intermediate step returns a more negative reward, and achieving the goal returns a large positive reward. That’s an important pattern to follow, otherwise the agent might get trapped in a kind of infinite loop and not learn effective policy.We also need to limit the maximum number of steps in an episode, so that the agent is not merely correct in identifying the goal but also efficient. Too small of a limit will constrain the information that an agent obtains per episode. Too large of a limit will cause extra work with diminishing returns. To keep this simple, we’ll set the limit to the length of the array. Extra credit: if we call the “max steps” limit a hyperparameter, how might you determine an optimal setting for it? Looking ahead several tutorials, here’s a hint.At this point, we’ve represented the problem in terms of a gradient for an agent to explore through some sequence of decisions. The agent only has partial information about the environment (observations and rewards) although that’s enough to explore and learn how to navigate effectively.Define a custom environment in GymSource code for this custom environment is located on GitHub and the bulk of what the following code snippets explore is in the module at https://github.com/DerwenAI/gym_example/blob/master/gym-example/gym_example/envs/example_env.pyYou can follow the full code listing as we describe each block of code in detail to show why and how each function is defined and used. First, we need to import the libraries needed for Gym:import gymfrom gym.utils import seedingThe import for seeding helps manage random seeds for the pseudorandom number generator used by this environment. We’ll revisit that point later in the tutorial. While it’s not required, this feature can become quite useful when troubleshooting problems.Next, we need to define the Python class for our custom environment:class Example_v0 (gym.Env):Our custom environment is named Example_v0 defined as a subclass of gym.Env. Within this class we will define several constant values to describe the “array” in our environment. These aren’t required by Gym, but they help manage the array simulation…Possible index values in the array range from a minimum (left-most position) to a maximum (right-most position). To keep this simple and easy to debug, let’s define the length of the array as 10 with a starting position 1 — in other words, use 1-based indexing. Let’s define constants for the bounds of the array:LF_MIN = 1RT_MAX = 10At each step the agent can either move left or right, so let’s define constants to represent these actions:MOVE_LF = 0MOVE_RT = 1To make the trained policies efficient, we need to place a limit on the maximum number of steps before an episode ends and structure the rewards:MAX_STEPS = 10                                REWARD_AWAY = -2REWARD_STEP = -1REWARD_GOAL = MAX_STEPSA Gym environment class can define an optional metadata dictionary:metadata = {    ""render.modes"": [""human""]  }In this case, we’ll define one metadata parameter render.modes to be a list with ""human"" as it’s only element. That means the environment will support the simple “human” mode of rendering text to the terminal – not something more complex, such as image data to be converted into an animation.Next, we need to define six methods for our environment class. Gym provides some documentation about these methods although arguably it’s not complete. Not all of these are required by Gym or RLlib, even so let’s discuss why and how to implement them — in case you need them for representing your use case.The __init__() methodLet’s define the required __init__() method which initializes the class:def __init__ (self):    self.action_space = gym.spaces.Discrete(2)    self.observation_space = gym.spaces.Discrete(self.RT_MAX + 1)    # possible positions to chose on `reset()`    self.goal = int((self.LF_MIN + self.RT_MAX - 1) / 2)    self.init_positions = list(range(self.LF_MIN, self.RT_MAX))    self.init_positions.remove(self.goal)    # change to guarantee the sequence of pseudorandom numbers    # (e.g., for debugging)    self.seed()    self.reset()This function initializes two required members as Gym spaces:self.action_space – the action space of possible actions taken by the agentself.observation_space – the observation space for what info the agent receives after taking an actionThe action space is based on gym.spaces.Discrete and is a finite array with two values: the constants MOVE_LF = 0 and MOVE_RT = 1 defined above. These determine how the agent communicates back to the environment.The observation space is also based on gym.spaces.Discrete and is a finite array which is the length of the environment array, plus one. Recall that for simplicity we chose above to use a 1-based array.The self.goal and self.init_positions members are specific to this environment and not required by Gym. These members place our goal in the middle of the array, so that it’s not randomly chosen. We decided to do that to help make this environment simpler for a reader to understand, troubleshoot, dissect, recombine parts, etc. While the agent’s initial position is randomized (anywhere in the array other than the goal) the goal stays in place. Later in this tutorial, when we train a policy with RLlib, we’ll show how this example converges quickly and the performance curves for the RL learning metrics have classic shapes — what you hope to see in an RL problem.We call self.seed() to randomize the pseudorandom numbers (more about that later) and then call self.reset() to reset the environment to the start of an episode.The reset() methodNow we’ll define the required reset() method. This resets the state of the environment for a new episode and also returns an initial observation:def reset (self):    self.position = self.np_random.choice(self.init_positions)    self.count = 0    self.state = self.position    self.reward = 0    self.done = False    self.info = {}    return self.stateThe self.position and self.count members are specific to this environment and not required by Gym. These keep track of the agent’s position in the array and how many steps have occurred in the current episode.Gym environments typically use four other members to describe the outcome of a step: self.state, self.reward, self.done, self.info as a convention. For the returned value from a reset, we’ll provide self.state as the initial state, which is the agent’s randomized initial position.The step() methodNow we’ll define the required step() method to handle how an agent takes an action during one step in an episode:def step (self, action):   if self.done:      # should never reach this point      print(""EPISODE DONE!!!"")   elif self.count == self.MAX_STEPS:      self.done = True;   else:      assert self.action_space.contains(action)      self.count += 1      // insert simulation logic to handle an action ...   try:      assert self.observation_space.contains(self.state)   except AssertionError:      print(""INVALID STATE"", self.state)   return [self.state, self.reward, self.done, self.info]In other words, take an action as the only parameter for this step. If the self.done flag is set, then the episode already finished. While that should never happen, let’s trap it as an edge case. If the agent exceeds the maximum number of steps in this episode without reaching the goal, then we set the self.done flag and end the episode.Otherwise, assert that the input action is valid, increment the count of steps, then run through the simulation logic to handle an action. At the end of the function we assert that the resulting state is valid, then return the expected list [self.state, self.reward, self.done, self.info] to complete this action.The block of programming logic required for handling an action is a matter of updating the state of the environment and determining a reward. Let’s review this logic in terms of the two possible actions. When the action is to “move left”, then the resulting state and reward depend on the position of the agent compared with the position of the goal:if action == self.MOVE_LF:    if self.position == self.LF_MIN:        # invalid        self.reward = self.REWARD_AWAY    else:        self.position -= 1    if self.position == self.goal:        # on goal now        self.reward = self.REWARD_GOAL        self.done = 1    elif self.position < self.goal:        # moving away from goal        self.reward = self.REWARD_AWAY    else:        # moving toward goal        self.reward = self.REWARD_STEPIn other words, the agent cannot move further left than self.LF_MIN and any attempt to do so is a wasted move. Otherwise, the agent moves one position to the left. If that move lands the agent on the goal, then the episode is done and the resulting reward is the maximum positive value. If not, then the agent receives a less negative reward for moving toward the goal, and a more negative reward for moving away from the goal.The logic for handling the action to “move right” is written in a similar way:elif action == self.MOVE_RT:    if self.position == self.RT_MAX:        # invalid        self.reward = self.REWARD_AWAY    else:        self.position += 1    if self.position == self.goal:        # on goal now        self.reward = self.REWARD_GOAL        self.done = 1    elif self.position > self.goal:        # moving away from goal        self.reward = self.REWARD_AWAY    else:        # moving toward goal        self.reward = self.REWARD_STEPAfter handling that logic, then update the environment’s state and also define an optional self.info member, which is a Python dictionary that provides diagnostic information that can be useful for troubleshooting:self.state = self.positionself.info[""dist""] = self.goal - self.positionThe contents of self.info can be anything that fits in a Python dictionary. In this case, let’s keep track of the distance between the agent and goal, to measure whether we’re getting closer. Note: this additional info cannot be used by RLlib during training.The render() methodNext, we’ll define the optional render() method, to visualize the state of the environment:def render (self, mode=""human""):    s = ""position: {:2d}  reward: {:2d}  info: {}""    print(s.format(self.state, self.reward, self.info))This is especially helpful for troubleshooting and you can make it as simple or as complex as needed. In this case, we’re merely printing out text to describe the current state, the most recent reward, and the debugging info defined above.The seed() methodNow we’ll define the optional seed() method to set a seed for this environment’s pseudorandom number generator:def seed (self, seed=None):    self.np_random, seed = seeding.np_random(seed)    return [seed]This function returns the list of one or more seeds used, where the first value in the list should be the “main” seed, i.e., the value to be passed in to reproduce a sequence of random numbers. For example, each reset in this environment initializes the agent’s position in the array. For debugging purposes you may want to insure that the sequence of initial positions for the agent stays the same while iterating through the episodes.The close() methodFor the optional close() method, we’ll define how to handle closing an environment:def close (self):    passGym environments automatically close during garbage collection or when a program exists. In this case we used pass as a no-op. Override this function to handle any special clean-up procedures that are required by your use case.There, we did it! We’ve defined a custom environment. Even so, we cannot use it quite yet… First we need to add setup instructions for using its source code as a Python library.Structure the Python library and Git repoNow that we have an environment implemented, next we need to structure the subdirectories for how to import and register it in usage. This becomes a bit tricky, since multiple software libraries will make use of the environment. Python will consider it to be a library to be imported, and also as a class to be instantiated — these require specific naming conventions. Then RLlib will need to have the custom environment registered prior to training. Then Gym will need to construct the environment separately so that we can deserialize a trained policy for a rollout. RLlib and Gym have different means of referencing the environment than the Python import. Therefore we must structure the layout and naming of subdirectories rather carefully.Given that the source code module example_env.py lives in a Git repo, here’s a subdirectory and file layout for the repo:We will consider each component of this layout in turn. As mentioned above, pay attention to the differences between a dash - and an underscore _ in the subdirectory and file names, otherwise various software along the way will get rather persnickety about it.Due to the way that Gym environments get installed and imported by Python, we need to define gym-example/setup.py to describe path names and library dependencies required for installation:from setuptools import setupsetup(name=""gym_example"",      version=""1.0.0"",      install_requires=[""gym""])In other words, this environment implementation depends on the Gym library and its source code will be expected in the gym_example subdirectory.Then we need to define the gym-example/gym_example/__init__.py script so that our custom environment can be registered before usage:from gym.envs.registration import registerregister(    id=""example-v0"",    entry_point=""gym_example.envs:Example_v0"",)In other words, there will be a Python class Example_v0 defined within the envs subdirectory. When we register the environment prior to training in RLlib we’ll use example-v0 as its key. Going into th envs subdirectory, we need to define the script gym-example/gym_example/envs/__init__.py as:from gym_example.envs.example_env import Example_v0We also need to add our source code module example_env.py into the envs subdirectory.Finally, we have a full path described to reach the source code for the custom environment that we defined above.Measure a random-action baselineNow that we have a library defined, let’s use it. Before jumping into the RLlib usage, first we’ll create a simple Python script that runs an agent taking random actions. Source code is available at https://github.com/DerwenAI/gym_example/blob/master/sample.pyThis script serves two purposes: First, it creates a “test harness” to exercise our environment implementation simply and quickly, before we move to train a policy. In other words, we can validate the environment’s behaviors separately. Second, it measures a baseline for how well the agent performs, statistically, by taking random actions without the benefit of reinforcement learning.First we need to import both Gym and our custom environment:import gymimport gym_exampleNow we’ll define a function run_one_episode() which resets the environment initially then runs through all the steps in one episode, returning the cumulative rewards:def run_one_episode (env):    env.reset()    sum_reward = 0    for i in range(env.MAX_STEPS):        action = env.action_space.sample()        state, reward, done, info = env.step(action)        sum_reward += reward        if done:            break    return sum_rewardAt each step, an action is sampled using env.action_space.sample() then used in the env.step(action) call. This is another good reason to use env.seed() for troubleshooting — to force that sequence of “random” actions to be the same each time through. BTW, you may want to sprinkle some debugging breakpoints or print() statements throughout this loop, to see how an episode runs in detail.To use this function, first, we’ll create the custom environment and run it for just one episode:env = gym.make(""example-v0"")sum_reward = run_one_episode(env)Note that we called gym.make(""example-v0"") with the key defined in the previous section, not the name of the Python class or the library path. Given that this code runs as expected, next let’s calculate a statistical baseline of rewards based on random actions:history = []for _ in range(10000):    sum_reward = run_one_episode(env)    history.append(sum_reward)avg_sum_reward = sum(history) / len(history)print(""\nbaseline cumulative reward: {:6.2}"".format(avg_sum_reward))This code block iterates through 10000 episodes to calculate a mean cumulative reward. In practice, the resulting value should be approximately -5.0 give or take a small fraction.Train a policy with RLlibAt last, we’re ready to use our custom environment in RLlib. Let’s definite another Python script to train a policy with reinforcement learning. Source code for this script is at https://github.com/DerwenAI/gym_example/blob/master/train.pyLet’s take care of a few preparations before we start training. Initialize the directory in which to save checkpoints (i.e., serialize a policy to disk) as a subdirectory ./tmp/exa and also the directory in which to write the logs which Ray expects to be at ~/ray_results/ by default:import osimport shutilchkpt_root = ""tmp/exa""shutil.rmtree(chkpt_root, ignore_errors=True, onerror=None)ray_results = ""{}/ray_results/"".format(os.getenv(""HOME""))shutil.rmtree(ray_results, ignore_errors=True, onerror=None)We’ll start Ray running in local mode, i.e., not running on a remote cluster:import rayray.init(ignore_reinit_error=True)BTW, if you ever need to use a debugger to troubleshoot a custom environment, there’s a “local mode” for Ray that forces all tasks into a single process for simpler debugging. Just add another parameter local_mode=True in the ray.init() call.Next we need to register our custom environment:from ray.tune.registry import register_envfrom gym_example.envs.example_env import Example_v0select_env = ""example-v0""register_env(select_env, lambda config: Example_v0())Note how we needed to use both the ""example-v0"" key and the Example_v0() Python class name, and that the Python import requires a full path to the source model.Next we’ll configure the environment to use proximal policy optimization (PPO) and create an agent to train using RLlib:import ray.rllib.agents.ppo as ppoconfig = ppo.DEFAULT_CONFIG.copy()config[""log_level""] = ""WARN""agent = ppo.PPOTrainer(config, env=select_env)The preparations are all in place, and now we can train a policy using PPO. This loop run through 5 iterations. Given that this is a relatively simple environment, that should be enough to show much improvement in the policy by using RLlib:status = ""{:2d} reward {:6.2f}/{:6.2f}/{:6.2f} len {:4.2f} saved {}""n_iter = 5for n in range(n_iter):    result = agent.train()    chkpt_file = agent.save(chkpt_root)    print(status.format(            n + 1,            result[""episode_reward_min""],            result[""episode_reward_mean""],            result[""episode_reward_max""],            result[""episode_len_mean""],            chkpt_file            ))For each iteration, we call result = agent.train() to run the episodes, and then call chkpt_file = agent.save(chkpt_root) to save a checkpoint of the latest policy. Then we print metrics that show how well the learning has progressed. The resulting output should look close to the following: 1 reward -21.00/ -6.96/ 10.00 len 7.83 saved tmp/exa/checkpoint_1/checkpoint-1 2 reward -20.00/  1.24/ 10.00 len 5.51 saved tmp/exa/checkpoint_2/checkpoint-2 3 reward -20.00/  5.89/ 10.00 len 3.90 saved tmp/exa/checkpoint_3/checkpoint-3 4 reward -17.00/  7.19/ 10.00 len 3.30 saved tmp/exa/checkpoint_4/checkpoint-4 5 reward -17.00/  7.83/ 10.00 len 2.92 saved tmp/exa/checkpoint_5/checkpoint-5After the first iteration, the mean cumulative reward is -6.96 and the mean episode length is 7.83 … by the third iteration the mean cumulative reward has increased to 5.89 and the mean episode length has dropped to 3.90 … meanwhile, both metrics continue to improve in subsequent iterations.If you run this loop longer, the training reaches a point of diminishing returns after about the ten iterations. Then you can run Tensorboard from the command line to visualize the RL training metrics from the log files:tensorboard --logdir=$HOME/ray_resultsRecall that our baseline measure for mean cumulative reward was -5.0, so the policy trained by RLlib has improved substantially over an agent taking actions at random. The curves in the Tensorboard visualizations above — such as episode_len_mean and episode_reward_mean — have classic shapes for what you generally hope to see in an RL problem.Apply a trained policy in a rolloutContinuing within the same train.py script, let’s make use of the trained policy through what’s known as a rollout. First, some preparations: we need to restore the latest saved checkpoint for the policy, then create our environment and reset its state:import gymagent.restore(chkpt_file)env = gym.make(select_env)state = env.reset()Now let’s run the rollout through through 20 episodes, rendering the state of the environment at the end of each episode:sum_reward = 0n_step = 20for step in range(n_step):    action = agent.compute_action(state)    state, reward, done, info = env.step(action)    sum_reward += reward    env.render()    if done == 1:        print(""cumulative reward"", sum_reward)        state = env.reset()        sum_reward = 0The line action = agent.compute_action(state) represents most of the rollout magic here for using a policy instead of training one. The resulting output should look close to the following:position:  5  reward: 10  info: {'dist': 0}cumulative reward 10position:  3  reward: -1  info: {'dist': 2}position:  4  reward: -1  info: {'dist': 1}position:  5  reward: 10  info: {'dist': 0}cumulative reward 8position:  7  reward: -1  info: {'dist': -2}position:  6  reward: -1  info: {'dist': -1}position:  5  reward: 10  info: {'dist': 0}cumulative reward 8Great, we have used RLlib to train a reasonably efficient policy for an agent in our Example_v0 custom environment. The rollout shows code for how this could be integrated and deployed in a use case.SummaryThe full source code in Python for this tutorial is in the GitHub repo https://github.com/DerwenAI/gym_exampleUse Git to clone that repo, connect into its directory, then install this custom environment:pip install -r requirements.txtpip install -e gym-exampleIn summary, we’ve created a custom Gym environment to represent a problem to solve with RL. We showed a template for how to implement that environment, how to structure the subdirectories of its Git repo. We created a “test harness” script and analyzed the environment to get a baseline measure of the cumulative reward with an agent taking random actions. Then we used RLlib to train a policy using PPO, saved checkpoints, and evaluated the results by comparing with our random-action baseline metrics. Finally, we ran a rollout of the trained policy, showing how the resulting policy could be deployed in a use case.Hopefully this will provide a starting point for representing your own use cases to train and solve using RLlib.Also, check the Anyscale Academy for related tutorials, discussions, and events. In particular, you can learn much more about reinforcement learning (tools, use cases, latest research, etc.) at the Ray Summit conference which will be held online September 30 through October 1 (free!), with tutorials on September 29 (nominal fee).Kudos to https://deepdreamgenerator.com/ for image processing with deep learning.Distributed Computing with RayRay is a fast and simple framework for distributed…Follow195 1 Thanks to Dean Wampler. Reinforcement LearningMachine LearningOpen SourcePythonRllib195 claps195 claps1 responseWritten byPaco NathanFollowevil mad scientist https://derwen.ai/paco ; lives on a tiny orchard in EcotopiaFollowDistributed Computing with RayFollowRay is a fast and simple framework for distributed computingFollowWritten byPaco NathanFollowevil mad scientist https://derwen.ai/paco ; lives on a tiny orchard in EcotopiaDistributed Computing with RayFollowRay is a fast and simple framework for distributed computingMore From MediumMulti-Agent Reinforcement Learning: The GistAustin Nguyen in The StartupComputer Vision: Intuition behind Panorama snitchingJunhup LimA Handwritten Introduction to Linear and Non-Linear Least-Square Regression, ft.Zakarie Aloui in The StartupServerless Machine Learning Classifier SlackBotJeremy BlytheAutomated Canary Release of TensorFlow Models on KubernetesSrinivasan Parthasarathy in iter8-toolsImplementing Linear Regression Using SklearnPrabhat Pathak in Analytics VidhyaMachine Learning w Sephora Dataset Part 1 — Web ScrapingAudrey TangHow to Visualize Tensorflow Metrics in KibanaM Adel Abdelhady in Fourthline TechLearn more.Medium is an open platform where 170 million readers come to find insightful and dynamic thinking."
"Adaptive Pair Trading under COVID, a Reinforcement Learning Approach",https://towardsdatascience.com/adaptive-pair-trading-under-covid-19-a-reinforcement-learning-approach-ff17e6a8f0d6?source=tag_archive---------7-----------------------,"Reinforcement Learning,Quant Trading,Pair Trading,Stock Market,Artificial Intelligence","Photo by M. B. M. on UnsplashAbstractThis is one of the articles of A.I. Capital Management’s Research Article Series, with the intro article here. This one is about applying RL on market neutral strategies, specifically, optimizing a simple pair trading strategy with RL agent being the capital allocator on per trade basis, while leaving the entrance/exit signal untouched. The goal is to optimize an existing signal’s sequential trade size allocation while letting the agent adapt its actions to market regimes/conditions.Author: Marshall Chang is the founder and CIO of A.I. Capital Management, a quantitative trading firm that is built on Deep Reinforcement Learning’s end-to-end application to momentum and market neutral trading strategies. The company primarily trades the Foreign Exchange markets in mid-to-high frequencies.OverviewPairs trading is the foundation of market neutral strategy, which is one of the most sought-after quantitative trading strategies because it does not profit from market directions, but from the relative returns between a pair of assets, avoiding systematic risk and the Random Walk complexity. The profitability of market neutral strategies lie within the assumed underlying relationship between pairs of assets, however, when such relationship no longer withhold, often during volatile regime-shifting times such as this year with COVID-19, returns generally diminishes for such strategies. In fact, according to HFR (Hedge Fund Research, Inc.), the HFRX Equity Hedge Index, by the end of July, 2020, reported a YTD return of -9.74%[1]; its close relative, the HFRX Relative Value Arbitrage Index, reported a YTD return of -0.85%. There is no secret that for market neutral quants, or perhaps any quants, the challenge is not just to find profitable signals, but more in how to quickly detect and adapt complex trading signals during regime-shifting times.Within the field of market neutral trading, most research have been focusing on uncovering correlations and refining signals, often using proprietary alternative data purchased at high costs to find an edge. However, optimization of capital allocation at trade size and portfolio level is often neglected. We found that lots of pair trading signals, though complex, still utilizes fixed entry thresholds and linear allocations. With the recent advancement of complex models and learning algorithms such as Deep Reinforcement Learning (RL), these class of algorithm is yearning for innovation with non-linear optimization.Methodology — AlphaSpread RL SolutionTo address the detection and adaptation of pair trading strategies through regime shifting times, our unique approach is to solve trade allocation optimization with sequential agent-based solution directly trained on top of existing signal generalization process, with clear tracked improvement and limited overhead of deployment.Internally named as AlphaSpread, this project demonstrates RL sequential trade size allocation’s ROI (Return on Investment) improvement over standard linear trade size allocation on 1 pair spread trading of U.S. S&P 500 equities. We take the existing pair trading strategy with standard allocation per trade as baseline, train RL allocator represented by a deep neural network model in our customized Spread Trading Gym environment, then test on out-of-sample data and aim to outperform baseline ending ROI.Specifically, we select cointegrated pairs based on their stationary spreads our statistical models. Cointegrated pairs are usually within the same industry, but we also include cross sectional pairs that show strong cointegration. The trading signal are generated by reaching pre-defined threshold of z-score on residues predicted by the statistical model using daily close prices. The baseline for this example allocates fixed 50% of overall portfolio to each trading signal, whereas the RL allocator output 0–100% allocation for each trading signal sequentially based on current market condition represented by a lookback of z-score.AlphaSpread — In the video, the red NAV is a signal’s performance into the COVID months, the green one is the same strategy with our RL allocator. We learned that our RL agent can pick up regime shifts early on and allocate accordingly to avoid huge downturns.Results SummaryWe summarize our RL approach’s pairs trading ROI against baseline linear allocation for 107 U.S. Equity pairs traded. The ROI is calculated with ending NAV of testing period against each pair’s $100,000 starting capital. The result is from back-testing on out-of-sample data between 2018 to April 2020 (COVID-19 months included). The RL allocators are trained with data between 2006 and 2017. In both cases fees are not consider in the testing. We have achieved on average 9.82% per pair ROI improvement over baseline approach, with maximum of 55.62% and minimum of 0.08%.In other words, with limited model tuning, this approach is able to result in generalized improvement of ROI through early detecting of regime-shifting and the accordingly capital allocation adaptation by the RL allocator agent.A snapshot of pair trading strategies’ ROIs, comparing base allocation and RL allocationDiscussions of GeneralizationThe goal of this project is to demonstrate out-of-sample generalization of the underlying improvements on a very simple one-pair trading signals, hence providing guidance on adapting such methodology on large scale complex market neutral strategies to be deployed. Below is a discussion of the 3 goals we set out to achieve in this experiment.Repeatability — This RL framework consists of customized pairs trading RL environment used to accurately train and test RL agents, RL training algorithms including DQN, DDPG and Async Actor Critics, RL automatic training roll out mechanism that integrates memory prioritized replay, dynamic model tuning, exploration/exploitation and etc., enabling repeatability for large datasets with minimum customization and hand tuning. The advantage of running RL compared with other machine learning algorithm is that it is an end-to-end system from training data generalization, reward function design, model and learning algorithm selection to output a sequential policy. A well-tuned system requires minimum maintenance and the retraining / readapting of models to new data is done in the same environment.Sustainability — Under the one-pair trading example, the pairs cointegration test and RL training were done using data from 2006 and 2017, and then trained agents run testing from 2018 to early 2020. The training and testing data split are roughly 80:20. With RL automatic training roll out, we can generalize sustainable improvements over baseline return for more than 2 years across hundreds of pairs. The RL agent learns to allocate according to the lookback of z-scores representing the pathway of the pair’s cointegration as well as volatility and is trained with exploration / exploitation to find policy that maximize ending ROI. Compared with traditional supervised and unsupervised learning with static input — output, RL algorithms has built-in robustness for generalization in that it directly learns state-policy values with a reward function that reflects realized P/L. The RL training targets are always non-static in that the training experience improves as the agent interacts with the environment and improves its policy, hence the reinforcement of good behavior and vice versa.Scalability — Train and deploy large scale end-to-end Deep RL trading algorithms is still its infancy in quant trading, but we believe it is the future of alpha in our field, as RL has demonstrated dramatic improvement over traditional ML in the game space (AlphaGo, Dota etc.). This RL framework is well versed to apply to different pair trading strategies that is deployed by market neutral funds. With experience running RL system in multiple avenues of quant trading, we can customize environment, training algorithms and reward function to effectively solve unique tasks in portfolio optimization, powered by RL’s agent based sequential learning that traditional supervised and unsupervised learning models cannot achieve.Key Take AwayIf the signal makes money, it makes money with linear allocation (always trade x unit). But when it doesn’t, obviously we want to redo the signal, let it adapt to new market conditions. However, sometimes that’s not easy to do, and a quick fix might be a RL agent/layer on top of existing signal process. In our case, we let the agent observe a dataset that represents volatility of the spreads, and decide on the pertinent allocation based on past trades and P/L.Background and More DetailsSignal Generalization Process — We first run a linear regression on both assets’ past look back price history (2006–2017 daily price), then we do OLS test to obtain the residual, with which we run unit root test (Augmented Dickey–Fuller test) to check the existence of cointegration. In this example, we set the p-value threshold at 0.5% to reject unit root hypothesis, which results in a universe of 2794 S&P 500 pairs that pass the test. Next phrase is how we set the trigger conditions. First, we normalize the residual to get a vector that follows assumed standard normal distribution. Most tests use two sigma level reaches 95% which is relatively difficult to trigger. To generate enough trading for each pair, we set our threshold at one sigma. After normalization, we obtain a white noise follows N(0,1), and set +/- 1 as the threshold. Overall, the signal generation process is very straight forward. If the normalized residual gets above or below threshold, we long the bearish one and short the bullish one, and vice versa. We only need to generate trading signal of one asset, and the other one should be the opposite directionDeep Reinforcement Learning — The RL training regimes starts with running an exploration to exploitation linear annealed policy to generate training data by running the training environment, which in this case runs the same 2006–2017 historical data as with the cointegration. The memory is stored in groups ofState, Action, Reward, next State, next Action (SARSA)Here we use a mixture of DQN and Policy Gradient learning target, in that our action outputs are continuous (0–100%) yet sample inefficient (within hundreds of trades per pair due to daily frequency). Our training model updates iteratively withQ(State) = reward + Q-max (next States, next Actions)Essentially, RL agent is learning the q value of continuous-DQN but trained with policy gradient on the improvements of each policy, hence avoiding the sample inefficiency (Q learning is guaranteed to converge to training global optimal) and tendency to stuck in local minimum too quickly (avoiding all 0 or 1 outputs for PG). Once the warm-up memories are stored, we train the model (in this case is a 3-layer dense net outputting single action) with the memory data as agent continues to interact with the environment and roll out older memories.Note from Towards Data Science’s editors: While we allow independent authors to publish articles in accordance with our rules and guidelines, we do not endorse each author’s contribution. You should not rely on an author’s works without seeking professional advice. See our Reader Terms for details.ReferenceSutton RS, Barto AG. Reinforcement learning: An introduction. MIT press; 2018.HFRX® Indices Performance Tables. (n.d.). Retrieved August 03, 2020, from https://www.hedgefundresearch.com/family-indices/hfrxWritten byMarshall ChangFounder and CIO of A.I. Capital ManagementFollow35 Sign up for The Daily PickBy Towards Data ScienceHands-on real-world examples, research,  tutorials, and cutting-edge techniques delivered Monday to Thursday. Make learning your daily ritual. Take a lookGet this newsletterBy signing up, you will create a Medium account if you don’t already have one. Review our Privacy Policy for more information about our privacy practices.Check your inboxMedium sent you an email at  to complete your subscription.35 35 Reinforcement LearningQuant TradingPair TradingStock MarketArtificial IntelligenceMore from Towards Data ScienceFollowA Medium publication sharing concepts, ideas, and codes.Read more from Towards Data ScienceMore From Medium5 YouTubers Data Scientists And ML Engineers Should Subscribe ToRichmond Alake in Towards Data Science7 Must-Haves in your Data Science CVElad Cohen in Towards Data Science30 Examples to Master PandasSoner Yıldırım in Towards Data Science21 amazing Youtube channels for you to learn AI, Machine Learning, and Data Science for freeJair Ribeiro in Towards Data ScienceThe Roadmap of Mathematics for Deep LearningTivadar Danka in Towards Data Science4 Types of Projects You Must Have in Your Data Science PortfolioSara A. Metwalli in Towards Data ScienceAn Ultimate Cheat Sheet for Data Visualization in PandasRashida Nasrin Sucky in Towards Data ScienceHow to Get Into Data Science Without a DegreeTerence S in Towards Data ScienceAboutHelpLegalGet the Medium app"
Maze solver using Naive Reinforcement Learning,https://towardsdatascience.com/maze-rl-d035f9ccdc63?source=tag_archive---------8-----------------------,"Machine Learning,Games,Reinforcement Learning,Programming,Artificial Intelligence","This is a short maze solver game I wrote from scratch in python (in under 260 lines) using numpy and opencv. Code link included at the end.The arrows show the learned policy improving with training. Given an agent starts from anywhere, it should be able to follow the arrows from its location, which should guide it to the nearest destination block.I wrote this to understand the fundamentals of Q-Learning and apply the theoretical concepts directly in code from scratch. Follow along if you wanna get your hands dirty with reinforcement learning!Game Objective -Find the optimal movement policy which takes an agent from any starting (shown in black-gray shades on the left) to the closest destination (blue-ish) box while avoiding danger zone (red) and wall (green) boxes.A “policy” can be thought of as the set of “smart-movement” rules which the agent learns to navigate its environment. In this case, they’re visualized as arrows (shown on left). This is done through Q-Learning.Significance -You might ask if making game-playing AIs like these are relevant at all in practical applications and that’s fair. Actually these are toy-problems designed in such a way that, their solutions are broadly applicable.For example, the current example of maze solving can further be extended for autonomous navigation in an occupancy grid to get to the nearest EV charging station.The Q-Learning Algorithm and the Q-Table approach -Q-Learning is centered around the Bellman Equation and finding the q-value for each action at the current state. Finding an optimal policy involves recursively solving this equation multiple times.The Bellman Equation. This can be recursively solved to obtain the “Q-values” or “quality values” of different actions given the agent’s current state.Only the main parts of the Bellman Equation relevant to this implementation will be explained in this article. For a more in-depth primer on the Bellman equation, check reference [1].What is the Q-value?Imagine you are an unfortunate soul stuck in a simple 2D world like the following -Yes, that’s you. You are sad. The orange arrows dictate the displacements you can make in this 2D world.Well, you look sad. You should be. Who wants to be in a 2D world anyway?Well… lets put a smile on that face, shall we? 🎃Given that the only movements you can make are the orange arrows shown in the image on the left (and a no-op operation), you gotta find your way to the nearest exit portal.Given these conditions, at any given stage, you’ll have to make a decision on one of these actions. To do that, your brain does an internal “ranking” of the actions taking many things into consideration. This might include things like -Where is the nearest exit?Are there any danger zones?Where dem walls at boi?Why is it getting hot in here? (We’ll get to this by discussing adding a small -ve reward for every time the agent does nothing)Now you being an advanced human, process these implicitly and assign a quality -value or a “Q-value” to each of the actions (up, down, left, right, no-op) you can take at that point.But how can you make a computer do it?Simple, you somehow assign a numeric q-value to each action at each situation you might encounter. However, this is the naive approach; and as stated in the title, we shall stick to this here. For more advanced stuff, there are tons of other articles where you should be looking.Pretty much like how we humans form perceptions of “good” and “bad” actions based on real-life experiences, the agent has to be trained in a similar way.Now, this brings us to the following question -What is the Q-table?Simply put, this is the memory of experiences per-say you’ll be updating and querying every time you have to make a decision and perform an action in the environment.An accurate visual representation of your relationship with the Q-table is shown on the left.Now, to build the Q-table, you need to collect information about the world. It needs to know of danger zones, walls it could bump in to, and pretty much anything to help you not die soon (much like life itself).To do this, let’s assume you can die a thousand deaths. Yes, sacrifice is necessary for science.Armed with this, you will start at random locations and kind-of begin randomly roaming around until you start forming a perception of the world around you. This perception is shaped by what you encounter while roaming around.You wanna avoid pain. In this sense, actions in situations which lead to -ve rewards. Therefore, you ‘take note of them’ in the Q-table whenever you encounter them.For example, you may hit a wall — that’s bad, cuz you’re bleeding. Now you’ll remember in that situation, whatever action you took which caused you to bleed, shouldn’t be repeated.Sometimes, you’ll even encounter danger zones raging with fire 🔥🧨 which will end your life as soon as you step on them. This is worse than bleeding, which will be quantified by assigning a more -ve reward value for such experiences.Now for the better things in life.Similarly, you’ll also keep track of all the good things (when you receive a +ve reward) which happen during your time in the maze. Well, in this case, there’s only one good thing which can happen - E S C A P E.This just sounds like another way of dying, but hey let’s pretend its more fun cuz it sounds different than death.To do all of this, you’ll basically build a table storing the q-values of performing each and every action in every possible scenario in the environment (do remember that this is naive for a reason).A higher q-value for a given action in a given state means that action will be more likely to be taken by you (the agent).Shown below are two different states with example q-values for each action that can be performed by you (the agent) at those states.In each state, the agent is located in the boxed region in the checkerboard world. For each state, shown to the right are different actions (up, left, right, down, no-op respectively from top to bottom) the agent can take along with their q-values derived from the Q-Table.The q-values then act as a guide towards taking the next action to maximize overall reward (which means escape). At every step, the following actions will be performed sequentially in this naive scenario -Query Q-table for values pertaining to the different actions you can perform at your current state.Take action pertaining to the highest q-value.Record the new state and reward received and use it to update the Q-table using the Bellman Equation. We’ll get here shortly.Go to step 1.Learning VisualizationFinal learned representation of the Q-table rendered visually on to the maze world. It is implemented from scratch in the codebase using numpy.Given all state transition rules are defined (which in this case is quite simple given the basic nature of the maze world), after a sufficient number of repeating these iterations, the agent builds a “vector field map” per-say of the different actions that should be performed at each location of the maze so as to reach the nearest destination in the minimum time.Shown on the left is the final learned representation of the Q-table.The arrows are visualized by obtaining a vector sum of the different q-values at each location. For example, if we have the following q-values for up, left, right, down — qu, ql, qr, qdThen the arrow, on a 2D plane (Horizontal is X-axis, Vertical is Y-axis) will have its x-component as qr-ql and y-component as qd-quThe length of the arrow is the norm of this vector obtained using the following formula -Therefore, if you start at any location in the maze, you can follow the arrows and reach the nearest destination by avoiding walls and danger zones.Updating the Q-Table while exploring the maze -This is one of the more challenging parts of the problem which greatly affects how soon you’ll be getting your sweet release (it’s not death, let’s remember that haha).Basically, here is the question —You take the highest q-value action at your given state following which, you end up in a new state (let’s hope for simplicity you don’t die for now).Next, you’d like to record whether your action has brought you closer to the nearest destination in the Q-table. How could you do this?All you have here to work with are the following -Existing q-values at the new and old states defined for each action. They might have been randomly initialized or obtained from a previous iteration.The reward you gained for the action you performed to get to the new state from the old state.The action you performed to get to the new state from the old state.How would you change the existing Q-table values you obtained for the old state to make a better decision if you come across it in the future?This is the very basic question which is answered by the Bellman equation in this case -The Bellman Equation. This can be recursively solved to obtain the “Q-values” or “quality values” of different actions given the agent’s current state.Following are the variable definitions -a is the action.s and s’ are the old and new states respectively.𝛾 is the discount factor, a constant between 0 and 1. You need this to prioritize current reward over expected future reward.Q(s) is the q-value of the action a you just took to reach the new state from the old state s.Q(s’) is the maximum q-value at the new state s’.R(s, a) is the reward you immediately receive for performing a to transition from s to s’.The max term is the secret sauce here. This causes the equation to iterate through every a until the maximum value of the expression inside the max term is obtained. It finally returns that value q and the corresponding action a.Every action a performed from state s might lead to new states s’ for each iteration. Therefore each time, the maximum of the q-values defined at s’ is chosen to compute the expression inside max.Once the values q and a are obtained, the Q-table value defined for action a at state s is then overwritten by q.In our case, this representation is the value function (don’t worry if you don’t get this; well, I just pulled an Andrew Ng on you 😈).Running the agent in the maze -Finally, you’ve made it here, congrats! Here is an exclusive RL meme for you from my meme page @ml.exe. You deserve it bud.Don’t worry, healthy narcissism won’t kill you.After a sufficient number of iterations of the Bellman equation, you’ll converge to optimum q-values for each action at each state.When you want to run the agent, simply start from any spawn point and blindly do the action with the highest q-value. You’ll reach the nearest destination.However, there are a few caveats to getting this right -Reward policies should be carefully designed. This means correct reward values should be assigned for performing each action at each state. Since this case is so simple, a simple scheme like the following works well -discount_factor = 0.5default_reward = -0.5wall_penalty = -0.6win_reward = 5.0lose_reward = -10.0default_reward is the reward obtained for doing nothing at all. Remember a basic question we asked ourselves in the beginning of this article “Why is it getting hot in here?”; well, here it is. Assigning a small negative reward encourages the agent to seek actions to end its misery rather than sitting around like an obese piece of lard.wall_penalty is the reward received if you bump into a wall while doing the action from your present state. Whenever you bump into a wall, you remain at your original location while receiving this “reward” 🤣.win_reward and lose_reward speak for themselves.You lose a game if you end up on any of the danger zones. Upon dying, you respawn at a randomly chosen location on the grid.In the codebase, you can play around with rewards to see how this affects solution convergence.ConclusionIf you correctly understand the steps cited in this article, you’ll be able to fully understand the codebase I wrote from scratch to implement all of this. You can find it here -ironhide23586/naive-dqn-mazeMaze Solver using Naive Reinforcement Learning with Q-Table construction This is an implementation of the Q-Learning…github.comThe code writes out a video of the agent training and learning as shown in the YouTube video below. You can generate random worlds with varying complexities.If you found this helpful, feel free to follow me for more upcoming articles :)I’m the editor of the following publication which publishes Tech articles related to the usage of AI & ML in digital mapping of the Earth. Feel free to follow to stay updated :)Machine Learning & AI in Digital CartographyCurated cutting edge AI & ML research articles from industry scientists working on Device, Edge, Cloud and Hybrid…medium.comReferences -https://medium.com/analytics-vidhya/bellman-equation-and-dynamic-programming-773ce67fc6a7https://www.instagram.com/ml.exehttps://github.com/ironhide23586https://www.youtube.com/user/mick23586Extras-I’m also a musician. If you dig metal and/or rap — my Soundcloud profile is at https://soundcloud.com/souham-biswasRegular talk-and-gaming live streaming at https://www.twitch.tv/souhamThank you :)Written bySouham BiswasSenior Data Scientist at HERE Maps (a BMW, Audi & Daimler company) | Writer at TowardsDataScience | MusicianFollow20 Sign up for The Daily PickBy Towards Data ScienceHands-on real-world examples, research,  tutorials, and cutting-edge techniques delivered Monday to Thursday. Make learning your daily ritual. Take a lookGet this newsletterBy signing up, you will create a Medium account if you don’t already have one. Review our Privacy Policy for more information about our privacy practices.Check your inboxMedium sent you an email at  to complete your subscription.20 20 Machine LearningGamesReinforcement LearningProgrammingArtificial IntelligenceMore from Towards Data ScienceFollowA Medium publication sharing concepts, ideas, and codes.Read more from Towards Data ScienceMore From Medium5 YouTubers Data Scientists And ML Engineers Should Subscribe ToRichmond Alake in Towards Data Science7 Must-Haves in your Data Science CVElad Cohen in Towards Data Science30 Examples to Master PandasSoner Yıldırım in Towards Data Science21 amazing Youtube channels for you to learn AI, Machine Learning, and Data Science for freeJair Ribeiro in Towards Data ScienceThe Roadmap of Mathematics for Deep LearningTivadar Danka in Towards Data Science4 Types of Projects You Must Have in Your Data Science PortfolioSara A. Metwalli in Towards Data ScienceAn Ultimate Cheat Sheet for Data Visualization in PandasRashida Nasrin Sucky in Towards Data ScienceHow to Get Into Data Science Without a DegreeTerence S in Towards Data ScienceAboutHelpLegalGet the Medium app"
Actor-Critic With TensorFlow 2.x [Part 1 of 2],https://towardsdatascience.com/actor-critic-with-tensorflow-2-x-part-1-of-2-d1e26a54ce97?source=tag_archive---------9-----------------------,"Machine Learning,Data Science,Reinforcement Learning,Artificial Intelligence,TensorFlow","Photo by David Veksler on UnsplashIn this series of articles, we will try to understand the actor-critic method and will implement it in 3 ways i.e naive AC, A2C without multiple workers, and A2C with multiple workers.This is the first part of the series, we will be implementing Naive Actor-Critic using TensorFlow 2.2. Let us first understand what the actor-critic method is and how it works? Knowing the Reinforce Policy gradient method will be beneficial, you can find it here.Overview:If you have read about Reinforce Policy gradient method than you know that its update rule isUpdate Rule for ReinforceIn the Actor-Critic method, we subtract the baseline from the discounted reward. And the common baseline use for these methods is the state value function. So our update rule for actor-critic will look like the following.Actor-Critic update ruleIn Actor-Critic Methods, we have two neural networks namely Actor and a critic. The actor is used for action selection and Critic is used to calculate state value. If you look at the update equation then you can notice that state value is being used as a baseline. Having a baseline helps to determine if an action taken was bad/good or it was the state that was bad/good. You can find very good resources for theory purposes in the reference section.Naive Actor-Critic:In this implementation, we will be updating our neural networks on each timestamp. This implementation differs from A2C where we update our network after every n timestamp. We will implement A2C in the next part of this series.Neural Networks:The neural network can be implemented basically in two ways.One Network for both Actor and Critic functionalities i.e one network with two output layers one for state value and another one for action probabilities.Separate networks, one for actor and another for a critic.We will be using Separate networks for Actor and Critic in this article because I find this one to learn quickly.Code:Actor and Critic Networks:Critic network output one value per state and Actor’s network outputs the probability of every single action in that state.Here, 4 neurons in the actor’s network are the number of actions.Note that Actor has a softmax function in the out layer which outputs action probabilities for each action.Note: number of neurons in hidden layers are very important for the agent learning and vary from environment to environment.Agent class’s init method:Here, we initialize optimizers for our networks. Please note that the learning rate is also important and can vary from the environment and method used.Action Selection:This method makes use of the TensorFlow probabilities library.Firstly, Actor gives out probabilities than probabilities are turned into a distribution using the TensorFlow probabilities library, and then an action is sampled from the distribution.Learn function and losses:We will be making use of the Gradient Tape technique for our custom training.Actor loss is negative of Log probability of action taken multiplied by temporal difference used in q learning.For critic loss, we took a naive way by just taking the square of the temporal difference. You can use the mean square error function from tf2 if you want but then u need to do some modification to temporal difference calculation. We will be using MSE in the next part of this series, so don’t worry.You can find more about the custom training loop at TensorFlow official website.Note: Make sure that you call networks inside with statement (context manager) and only use tensors for the network predictions, Otherwise you will get an error regarding no gradient provided.Trining loop:The agent takes action in environment and then bot networks are updates.For the Lunar lander environment, this implementation performs well.Note: what I noticed while implementing these methods is that the Learning rate and neurons in hidden layers hugely affect the learning.You can find the full code for this article here. Stay tuned for upcoming articles where we will be implementing A2C with and without multiple workers.The Second Part of this series can be accessed here.So, this concludes this article. Thank you for reading, hope you enjoy and was able to understand what I wanted to explain. Hope you read my upcoming articles. Hari Om…🙏References:Reinforcement Learning, Second EditionThe significantly expanded and updated new edition of a widely used text on reinforcement learning, one of the most…mitpress.mit.eduIntuitive RL: Intro to Advantage-Actor-Critic (A2C)Reinforcement learning (RL) practitioners have produced a number of excellent tutorials. Most, however, describe RL in…hackernoon.comAn intro to Advantage Actor Critic methods: let's play Sonic the Hedgehog!by Thomas Simonini An intro to Advantage Actor Critic methods: let's play Sonic the Hedgehog! Since the beginning of…www.freecodecamp.orgMachine Learning with PhilHowdy! At Neuralnet.ai we cover artificial intelligence tutorials in a variety of topics, ranging from reinforcement…www.youtube.comWritten byAbhishek Suran(Cyber Security + Machine Learning)Follow49 Sign up for The Daily PickBy Towards Data ScienceHands-on real-world examples, research,  tutorials, and cutting-edge techniques delivered Monday to Thursday. Make learning your daily ritual. Take a lookGet this newsletterBy signing up, you will create a Medium account if you don’t already have one. Review our Privacy Policy for more information about our privacy practices.Check your inboxMedium sent you an email at  to complete your subscription.49 49 Machine LearningData ScienceReinforcement LearningArtificial IntelligenceTensorFlowMore from Towards Data ScienceFollowA Medium publication sharing concepts, ideas, and codes.Read more from Towards Data ScienceMore From Medium5 YouTubers Data Scientists And ML Engineers Should Subscribe ToRichmond Alake in Towards Data Science7 Must-Haves in your Data Science CVElad Cohen in Towards Data Science21 amazing Youtube channels for you to learn AI, Machine Learning, and Data Science for freeJair Ribeiro in Towards Data ScienceThe Roadmap of Mathematics for Deep LearningTivadar Danka in Towards Data Science30 Examples to Master PandasSoner Yıldırım in Towards Data ScienceAn Ultimate Cheat Sheet for Data Visualization in PandasRashida Nasrin Sucky in Towards Data ScienceHow to Get Into Data Science Without a DegreeTerence S in Towards Data ScienceHow To Build Your Own Chatbot Using Deep LearningAmila Viraj in Towards Data ScienceAboutHelpLegalGet the Medium app"
PILCO explanation,https://medium.com/@tingq111011/pilco-explanation-ef5b69f6349f?source=tag_archive---------0-----------------------,"Pilco,Bayesian Machine Learning,Reinforcement Learning,Model Based,Probability","PILCO is one of the most important Model-Based Reinforcement Learning algorithms draws a lot of attention from researchers. It is using Gaussian Process as the function approximation instead of mainstream Neural Networks. Then the loss function is back-propagated to update the parameters in the controller.This algorithm was the PhD topic of the author Marc Deisenroth. There are more related papers to explain the detail of the algorithm. I think PILCO offered a way to model the uncertainty of the world model, by using covariance matrices.I am writing this blog based on the original paper[Deisenroth, M., & Rasmussen, C. E. (2011)]. And the GitHub repository https://github.com/nrontsis/PILCOFor nearly all the letters and symbols you can found corresponding variables in this repository.Hope it will help on your research. Please bear with my English skills. I am trying to improve it.Happy Hacking! ╭(●｀∀´●)╯╰(●’◡’●)╮Part 1. Gaussian Process.Here comes the PILCO, which is using the Gaussian Process(GP) as the world model. Before we get into the PILCO, we need to understand the Gaussian Process first.Point 1. Gaussian Process is a data-driven learning algorithm(function approximator)Instead of updating parameters of linear functions(like a neural network) to learn, GP do prediction based on how close the unknow points to known points. This distance is measured by a kernel.Another name for these methods is non-parametric learning methods.Leclercq, F. (2018)Point 2. Gaussian Process generalize to unseen points with a kernelA kernel is a measure of how close two data points are. It is used as a covariance function in the Multivariate Gaussian distribution, which is used to model the input, i.e, describes the covariance of the Gaussian process random variables.Commonly used kernels(From GPytorch library):If the data is a 2D data with shape (1,2), then the kernel shape should be (2, 2).Point 3. Gaussian Process Prediction IntuitionIn case you are confused, first thing first, there is no function between x and y. All GP need to predict a new data point is the distance of this new data to other known data. After it has such a distance measure, the predicted value will be linked with the corresponding y.Intuition Example (Not how the GP works):We know a corresponding:when x = 1, y = 5when x = 20, y =47when x = 100, y = 207.Now, we want to know what is y* = ?, when x* = 25 correspondings to.First, we compute the distance in x space. We have distance (25–1 = 24), (25–20 = 5), (100–25 = 75). Total distance will be (24 + 5 + 75 = 104).When we try to predict, we want to say that since the x* = 25 is closer to x=20, we would say, it is more likely that y* is close to 47. But I want to take consideration of other points. So one thing we can do is we can take the weighted sum of known ys. The weight is the inversed and normalized distances.y* = 5 * ((104–24)/(2*104)) + 47 * ((104–5)/(2*104)) + 207* ((104–75)/(2*104)) = 53.15Gaussian Process is a distribution over functions. The weights are decided by the kernel. And the x will only affect the kernel i.e, weights. The predicted y is kinda like a weighted sum of known ys. There is no function link x and y.For Gaussian Process, a formal implication is the Multivariate Gaussian Theorem.Assume the distribution of dependent variable Y is modelled by a Multivariate Gaussian Distribution:Where the K means the kernel for Xs in the known data.Where the K* means the kernel for Xs in the known data and X*s in the unknown data.Where the K** means the kernel for X* in the unknown data.The Y* we want to predict is under the Gaussian Distribution with:Point 4. Hyperparameters and training of Gaussian Process.As we can see from above, there is no “training” at all needed. The training of Gaussian Process is referred to update the hyperparameters(lengthscale, variance, likelihood) with the Maximum Likelihood Optimization.For an RBF kernel:Lengthscale: the “l” in the formulaOutput scale(variance): the “σf” in the first partNoise(likelihood): the “σ n” in the second part, controls noise modelling.Higher “l” values lead to smoother functions and therefore to coarser approximations of the training data. Lower “l” values make functions more wiggly with wide confidence intervals between training data points. “σf” controls the vertical variation of functions drawn from the GP. This can be seen by the wide confidence intervals outside the training data region in the right figure of the second row. “σn” represents the amount of noise in the training data. Higher “σn” values make more coarse approximations which avoids overfitting to noisy data. — — — http://krasserm.github.io/2018/03/19/gaussian-processes/Point 5. DrawbacksThe (K*K^(-1)) is an inverse matrix, which will take O(n³) to compute.Due to the fact that it is data-driven, the learning is offline.Part 2 PILCONow, Let’s get into PILCO.In general, there are three parts involved in the PILCO.(1). Gaussian Process World Model(2). A Loss function that computes how far away a state is from the target.(3). A controller that propose an action.What we want is a controller that can achieve high rewards(or low loss function). That involves taking the derivative of the controller’s parameters with respect to the loss function. Taking such a derivative can be done automatically after we link everything up with mainstream off the shelf libraries like Tensorflow or Pytorch.One thing worth to note in PILCO is that, instead of taking a specific value, function approximator takes the distribution. In other words, the system is processing distributions.Note: World model is different from transition probability.The current state “x_{t-1}” and action “u_{t-1}” are concatenated to form the input.The model “f” will take such input, and predict the next state “x_t”2. The next state is under Gaussian Distribution. The distribution is defined by its’ mean and covariance. The world model is predicting the difference between current state “x_{t-1}” and “x{t}”, given action “u_t”. The covariance of the next state prediction is provided by the world model. It is indicating how sure the world model about this prediction.3. The kernel that the Gaussian Process is a squared exponential (SE) kernel. The “Λ” is the reciprocal of the lengthscale.The alpha is the variance.There is no noise term here for now. It is in the (7)4. In the paper, they write out the Gaussian Process prediction as well. Which is similar to what we had above. The lower case k is the x for the points we want to predict. Noise term was taken consideration with “σε”.Remember, the β is defined as the (K + σ²εI)^-1 * y. It will show up in the code.5. Get the contribution of input states.In order to predict next state x_t, the distribution of concatenated input p(x_{t−1}, u_{t−1}) is needed. Assume that such distribution is a Gaussian distribution. The mean of the concatenated state-action is easy to compute. Only the covariance is the hard part. We need covariance from both u and x. Then concatenate them into a big covariance matrix.p(x, u) = N(0, cov[x_{t−1}, u_{t−1}])However, the u_{t-1} distribution depends on controller p(u|x, θ). To get the distribution of u_{t-1}, they integrate out the influence of state x. The state x’s covariance does not depend on anything, so we can just use it.p(u) = ∫ p(x, u) dx = ∫ p(u|x, θ) * p(x) dx6. With the world model “f”, we can compute the transition probability p(f(x_{t−1})|x_{t−1}).The distribution of states difference ∆t is from the weighted sum of transition probability. We integrate out the random variable x_{t−1} to get the p(∆t).Computing the exact predictive distribution p(∆t) in Eq. (9) is analytically intractable. Therefore, we approximate p(∆t) by a Gaussian using exact moment matching.7. From difference to real next state.For mean, we just add the difference. For covariance, cross-covariance is computed first, then added. The way fo computing cross-covariance is provided in another his paper[Deisenroth, M. P.(2010)].8. Mean predictionNow it’s time to have a look at how to implement it. As the (9) shows, the current states & action x^~_{t-1} are integrated out((13)). And the “mf” is the expectation from GP prediction, which is referred back to the β around equation (7). The “qa” here is equivalent to the k* transpose in the equation (7).Here, for each dimension “a” in the output, it will have one GP corresponding to it. For example in the MountainCar, the state is position and velocity, actions are concatenated. The prediction is the next position and the next velocity.Further, for each entry in the kernel in the “qa” or k*, replace it with the kernel formula (6), it ends up with the formula from “x” to output “q”, no more k.9. Covariance PredictionFor the covariance matrix prediction, they distinguished between diagonal elements “aa” and off-diagonal elements “ab”. “a” and “b” are the row and column of the target covariance matrix.the law of iterated variances(from Wikipedia):We distinguish between diagonal elements and off-diagonal elements: Using the law of iterated variances, we obtain for target dimensions a, b = 1, . . . , DThe off-diagonal terms do not contain the additional term Ex˜t−1 [covf [∆a, ∆b|x˜t−1]] because of the conditional independence assumption of the GP models: Different target dimensions do not covary for given x˜t−1.The computation of μ_a, μ_b are from the last section, the mean prediction.Since the off-diagonal and diagonal will share the diagonal part, First we compute the terms that are common to both the diagonal and off-diagonal entries, with the law of iterated expectation(different law from above E(X) = E(E(X|Y))):the law of iterated expectation(see the end for reference):the probability of rain tomorrow can be calculated by considering both cases (it rained today/it did not rain today) in turn. To use specific numbers, suppose thatThe probability of rain today is 70%If it rains today, it will rain tomorrow with probability 30%If it does not rain today, it will rain tomorrow with probability 90%In this case, the probability of rain tomorrow is0.7 * 0.3 + 0.3 * 0.9 = 0.21 + 0.27 = 0.48 =48%The math here is a little bit annoying. I need to do more Google.╮(๑•́ ₃•̀๑)╭Things should be more clear now. We would like to do the same thing as what we did for the mean prediction. Replacing the mean “m” with β, and then “k(or q)” with the kernel formula.Then for each entry of Q, use the kernel formula (6) to have the detail computation.Now we get the σ_ab done. The rest of the Equation (17) can be done by:10. Loss function (Policy Evaluation)The loss function is defined with the difference between the state and the target state.The policy is evaluated with the expected return. To do it, set the controller to evaluation mode, and then discover the distribution of the states. The expected return is computed with the weighted sum of the cost for each state and the state distribution.11. Policy ImprovementAfter linking the world model and controller together to the loss function. The parameter of the controller can be updated by propagating the loss back to the controller parameters by the Chain Rule.References:Deisenroth, M., & Rasmussen, C. E. (2011). PILCO: A model-based and data-efficient approach to policy search. In Proceedings of the 28th International Conference on machine learning (ICML-11) (pp. 465–472).Leclercq, F. (2018). Bayesian optimization for likelihood-free cosmological inference. Physical Review D, 98(6), 063511.Ebden, M. (2008). Gaussian Processes for Regression: A Quick Introduction.[online] Available at:< http://www. robots. ox. ac. uk/~ mebden/reports. GPtutorial. pdf.Law of Iterated Expectation | Brilliant Math & Science Wiki. (2020). Brilliant. https://brilliant.org/wiki/law-of-iterated-expectation/Deisenroth, M. P.(2010) Efficient Reinforcement Learning using Gaussian Processes. KIT Scientific Publishing, 2010. ISBN 978–3–86644–569–7.Written byTing QiaoFollow2 1 2 2 1 PilcoBayesian Machine LearningReinforcement LearningModel BasedProbabilityMore from Ting QiaoFollowMore From MediumProcessing natural language with neural networks is fiendishly hard! Here’s why…Mark Farragher in The Machine Learning AdvantagePoor Man’s BERT — Why Pruning is Better than Knowledge Distillation ✂️Viktor Karlsson in dair.aiComparing AI Platform Machine Types using YouTube-8MWarrick in Google Cloud - CommunityImage to Image Translation: GAN and Conditional GANKeerti kulkarni in Analytics VidhyaUsing Neural Networks to Understand Your Source CodeJesus Rodriguez in DataSeriesMy Recommendations for Getting Started with NLPelvis in dair.aiTeacher Student Architecture in Plant Disease ClassificationSaamahn MahjouriImplementing Attention Models in PyTorchSumedh Pendurkar in Intel Student AmbassadorsAboutHelpLegalGet the Medium app"
Building Offensive AI Agents for Doom using Dueling Deep Q-learning.,https://towardsdatascience.com/building-offensive-ai-agents-for-doom-using-dueling-deep-q-learning-ab2a3ff7355f?source=tag_archive---------0-----------------------,"Reinforcement Learning,Deep Learning,Doom,OpenAI,AI","IntroductionOver the last few articles, we’ve discussed and implemented Deep Q-learning (DQN)and Double Deep Q Learning (DDQN) in the VizDoom game environment and evaluated their performance. Deep Q-learning is a highly flexible and responsive online learning approach that utilizes rapid intra-episodic updates to it’s estimations of state-action (Q) values in an environment in order to maximize reward. Double Deep Q-Learning builds upon this by decoupling the networks responsible for action selection and TD-target calculation in order to minimize Q-value overestimation, a problem particularly evident when earlier on in the training process, when the agent has yet to fully explore the majority of possible states.Performance of our previous Double DQN Agent in the VizDoom environment, trained for 500 episodes.Inherently, using a single state-action value in judging a situation demands exploring and learning the effects of an action for every single state, resulting in an inherent hindrance to the model’s generalization capabilities. Moreover, not all states are equally relevant within the context of the environment.Preprocessed Frame from our previous agent trained in Pong.Recall our Pong environment from our earlier implementations. Immediately after our agent hits the ball, the value of moving left or right is negligible, as the ball must first travel to the opponent and be returned towards the player. Calculating state-action values at this point to use for training may disrupt the convergence of our agent as a result. Ideally, we would like to be able to identify the value of each action without learning its effects specific to each state, in order to encourage our agent to focus on selecting actions relevant to the environment.Dueling Deep Q-Learning (henceforth DuelDQN) addresses these shortcomings by splitting the DQN network output into two streams: a value stream and an advantage (or action) stream. In doing so, we partially decouple the overall state-action evaluation process. In their seminal paper, Van Hasselt et. al presented a visualization of how DuelDQN affected agent performance in the Atari game Enduro, demonstrating how the agent could learn to focus on separate objectives. Notice how the value stream has learned to focus on the direction of the road, while the advantage stream has learned to focus on the immediate obstacles in front of the agent. In essence, we have gained a level of short-term and medium-term foresight through this approach.To calculate the Q-value of a state-action, we then utilize the advantage function is to tell us the relative importance of an action. The subtraction of the average advantage, calculated across all possible actions in a state, is used to find the relative advantage of our interested action.The Q-value of a state-action according to the DuelDQN architecture.Intuitively, we have partially decoupled the action and state-value estimation processes in order to gain a more reliable appraisal of the environment.ImplementationWe’ll be implementing our approach in the same VizDoomgym scenario as in our last article, Defend The Line, with the same multi-objective conditions. Some characteristics of the environment include:An action space of 3: fire, turn left, and turn right. Strafing is not allowed.Brown monsters that shoot fireballs at the player with a 100% hit rate.Pink monsters that attempt to move close in a zig-zagged pattern to bite the player.Respawned monsters can endure more damage.+1 point for killing a monster.- 1 point for dying.Initial state of the “Defend The Line Scenario”Our Google Colaboratory implementation is written in Python utilizing Pytorch, and can be found on the GradientCrescent Github. Our approach is based on the approach detailed in Tabor’s excellent Reinforcement Learning course. As our DuelDQN implementation is similar to our previous vanilla DQN implementation, the overall high-level workflow is shared, and won’t be repeated here.Let’s start by importing all of the necessary packages, including the OpenAI and Vizdoomgym environments. We’ll also install the AV package necessary for Torchvision, which we’ll use for visualization. Note that the runtime must be restarted after installation is complete before the rest of the notebook can be executed.#Visualization cobe for running within Colab!sudo apt-get update!sudo apt-get install build-essential zlib1g-dev libsdl2-dev libjpeg-dev nasm tar libbz2-dev libgtk2.0-dev cmake git libfluidsynth-dev libgme-dev libopenal-dev timidity libwildmidi-dev unzip# Boost libraries!sudo apt-get install libboost-all-dev# Lua binding dependencies!apt-get install liblua5.1-dev!sudo apt-get install cmake libboost-all-dev libgtk2.0-dev libsdl2-dev python-numpy git!git clone https://github.com/shakenes/vizdoomgym.git!python3 -m pip install -e vizdoomgym/!pip install avNext, we initialize our environment scenario, inspect the observation space and action space, and visualize our environment.import gymimport vizdoomgymenv = gym.make('VizdoomDefendLine-v0')n_outputs = env.action_space.nprint(n_outputs)observation = env.reset()import matplotlib.pyplot as pltfor i in range(22):    if i > 20:    print(observation.shape)    plt.imshow(observation)    plt.show()observation, _, _, _ = env.step(1)Next, we’ll define our preprocessing wrappers. These are classes that inherit from the OpenAI gym base class, overriding their methods and variables in order to implicitly provide all of our necessary preprocessing. We’ll start defining a wrapper to repeat every action for a number of frames, and perform an element-wise maxima in order to increase the intensity of any actions. You’ll notice a few tertiary arguments such as fire_first and no_ops — these are environment-specific, and of no consequence to us in Vizdoomgym.class RepeatActionAndMaxFrame(gym.Wrapper):  #input: environment, repeat  #init frame buffer as an array of zeros in shape 2 x the obs space    def __init__(self, env=None, repeat=4, clip_reward=False, no_ops=0,                 fire_first=False):        super(RepeatActionAndMaxFrame, self).__init__(env)        self.repeat = repeat        self.shape = env.observation_space.low.shape        self.frame_buffer = np.zeros_like((2, self.shape))        self.clip_reward = clip_reward        self.no_ops = no_ops        self.fire_first = fire_first  def step(self, action):        t_reward = 0.0        done = False        for i in range(self.repeat):            obs, reward, done, info = self.env.step(action)            if self.clip_reward:                reward = np.clip(np.array([reward]), -1, 1)[0]            t_reward += reward            idx = i % 2            self.frame_buffer[idx] = obs            if done:                break        max_frame = np.maximum(self.frame_buffer[0], self.frame_buffer[1])        return max_frame, t_reward, done, info  def reset(self):        obs = self.env.reset()        no_ops = np.random.randint(self.no_ops)+1 if self.no_ops > 0    else 0        for _ in range(no_ops):            _, _, done, _ = self.env.step(0)            if done:                self.env.reset()                if self.fire_first:            assert self.env.unwrapped.get_action_meanings()[1] == 'FIRE'            obs, _, _, _ = self.env.step(1)        self.frame_buffer = np.zeros_like((2,self.shape))        self.frame_buffer[0] = obs    return obsNext, we define the preprocessing function for our observations. We’ll make our environment symmetrical by converting it into the standardized Box space, swapping the channel integer to the front of our tensor, and resizing it to an area of (84,84) from its original (320,480) resolution. We’ll also greyscale our environment, and normalize the entire image by dividing by a constant.class PreprocessFrame(gym.ObservationWrapper):  #set shape by swapping channels axis #set observation space to new shape using gym.spaces.Box (0 to 1.0)    def __init__(self, shape, env=None):        super(PreprocessFrame, self).__init__(env)        self.shape = (shape[2], shape[0], shape[1])        self.observation_space = gym.spaces.Box(low=0.0, high=1.0,                                    shape=self.shape, dtype=np.float32)   def observation(self, obs):        new_frame = cv2.cvtColor(obs, cv2.COLOR_RGB2GRAY)        resized_screen = cv2.resize(new_frame, self.shape[1:],                                    interpolation=cv2.INTER_AREA)        new_obs = np.array(resized_screen, dtype=np.uint8).reshape(self.shape)        new_obs = new_obs / 255.0   return new_obsNext, we create a wrapper to handle frame-stacking. The objective here is to help capture motion and direction from stacking frames, by stacking several frames together as a single batch. In this way, we can capture position, translation, velocity, and acceleration of the elements in the environment. With stacking, our input adopts a shape of (4,84,84,1).class StackFrames(gym.ObservationWrapper):  #init the new obs space (gym.spaces.Box) low & high bounds as repeat of n_steps. These should have been defined for vizdooom    #Create a return a stack of observations    def __init__(self, env, repeat):        super(StackFrames, self).__init__(env)        self.observation_space = gym.spaces.Box( env.observation_space.low.repeat(repeat, axis=0),                              env.observation_space.high.repeat(repeat, axis=0),                            dtype=np.float32)        self.stack = collections.deque(maxlen=repeat)    def reset(self):        self.stack.clear()        observation = self.env.reset()        for _ in range(self.stack.maxlen):            self.stack.append(observation)        return  np.array(self.stack).reshape(self.observation_space.low.shape)    def observation(self, observation):        self.stack.append(observation)    return np.array(self.stack).reshape(self.observation_space.low.shape)Finally, we tie all of our wrappers together into a single make_env() method, before returning the final environment for use.def make_env(env_name, shape=(84,84,1), repeat=4, clip_rewards=False,             no_ops=0, fire_first=False):    env = gym.make(env_name)    env = PreprocessFrame(shape, env)    env = RepeatActionAndMaxFrame(env, repeat, clip_rewards, no_ops, fire_first)        env = StackFrames(env, repeat)    return envNext, let’s define our model, a deep Q-network featuring two outputs for the dueling architecture. This is essentially a three layer convolutional network that takes preprocessed input observations, with the generated flattened output fed to a fully-connected layer, after which the output is then split into the value stream (with a single node output), and the advantage stream (with a node output corresponding to the number of actions in the environment).Note there are no activation layers here, as the presence of one would result in a binary output distribution. Our loss is the squared difference of our estimated Q-value of our current state-action and our predicted state-action value. We then attach the RMSProp optimizer to minimize our loss during training.import osimport torch as Timport torch.nn as nnimport torch.nn.functional as Fimport torch.optim as optimimport numpy as npclass DeepQNetwork(nn.Module):    def __init__(self, lr, n_actions, name, input_dims, chkpt_dir):        super(DeepQNetwork, self).__init__()        self.checkpoint_dir = chkpt_dir        self.checkpoint_file = os.path.join(self.checkpoint_dir, name)        self.conv1 = nn.Conv2d(input_dims[0], 32, 8, stride=4)        self.conv2 = nn.Conv2d(32, 64, 4, stride=2)        self.conv3 = nn.Conv2d(64, 64, 3, stride=1)        fc_input_dims = self.calculate_conv_output_dims(input_dims)        self.fc1 = nn.Linear(fc_input_dims,1024)        self.fc2 = nn.Linear(1024, 512)        #Here we split the linear layer into the State and Advantage streams        self.V = nn.Linear(512, 1)        self.A = nn.Linear(512, n_actions)        self.optimizer = optim.RMSprop(self.parameters(), lr=lr)        self.loss = nn.MSELoss()        self.device = T.device('cuda:0' if T.cuda.is_available() else 'cpu')        self.to(self.device)    def calculate_conv_output_dims(self, input_dims):        state = T.zeros(1, *input_dims)        dims = self.conv1(state)        dims = self.conv2(dims)        dims = self.conv3(dims)        return int(np.prod(dims.size()))    def forward(self, state):        conv1 = F.relu(self.conv1(state))        conv2 = F.relu(self.conv2(conv1))        conv3 = F.relu(self.conv3(conv2))        # conv3 shape is BS x n_filters x H x W        conv_state = conv3.view(conv3.size()[0], -1)        # conv_state shape is BS x (n_filters * H * W)        flat1 = F.relu(self.fc1(conv_state))        flat2 = F.relu(self.fc2(flat1))        V = self.V(flat2)        A = self.A(flat2)        return V, A     def save_checkpoint(self):        print('... saving checkpoint ...')        T.save(self.state_dict(), self.checkpoint_file)     def load_checkpoint(self):        print('... loading checkpoint ...')        self.load_state_dict(T.load(self.checkpoint_file))Recall that the update function for dueling deep Q-learning requires the following:The current state sThe current action aThe reward following the current action rThe next state s’The next action a’To supply these parameters in meaningful quantities, we need to evaluate our current policy following a set of parameters and store all of the variables in a buffer, from which we’ll draw data in minibatches during training. Hence, we need a replay memory buffer from which to store and draw observations from.import numpy as npclass ReplayBuffer(object):    def __init__(self, max_size, input_shape, n_actions):        self.mem_size = max_size        self.mem_cntr = 0        self.state_memory = np.zeros((self.mem_size, *input_shape),                                     dtype=np.float32)        self.new_state_memory = np.zeros((self.mem_size, *input_shape),                                         dtype=np.float32)        self.action_memory = np.zeros(self.mem_size, dtype=np.int64)        self.reward_memory = np.zeros(self.mem_size, dtype=np.float32)        self.terminal_memory = np.zeros(self.mem_size, dtype=np.bool)#Identify index and store  the the current SARSA into batch memory    def store_transition(self, state, action, reward, state_, done):        index = self.mem_cntr % self.mem_size        self.state_memory[index] = state        self.new_state_memory[index] = state_        self.action_memory[index] = action        self.reward_memory[index] = reward        self.terminal_memory[index] = done        self.mem_cntr += 1    def sample_buffer(self, batch_size):        max_mem = min(self.mem_cntr, self.mem_size)        batch = np.random.choice(max_mem, batch_size, replace=False)               states = self.state_memory[batch]        actions = self.action_memory[batch]        rewards = self.reward_memory[batch]        states_ = self.new_state_memory[batch]        terminal = self.terminal_memory[batch]     return states, actions, rewards, states_, terminalNext, we’ll define our agent, which differs form our vanilla DQN implementation. Our agent be using an epsilon greedy policy with a decaying exploration rate, in order to maximize exploitation over time. To learn to predict state and advantages values that maximize our cumulative reward, our agent will be using the discounted future rewards obtained by sampling the stored memory.You’’ll notice that we initialize two copies of our DQN as part of our agent, with methods to copy weight parameters of our original network into a target network. While our vanilla approach utilized this setup to generate stationary TD-targets, the presence of dual streams in our DuelDQN approach adds a layer of complexity to the process:States, Actions, Rewards, and Next States (SARS) are retrieved from the Replay Memory.The evaluation network is used to generate the advantage (A_s) and state (V_s) values of the current state .The target network is also used to create the advantage (A_s_) and state (V_s_) values of the next state.The predicted Q-values are generated by summing the advantage and state values of the current state, and subtracting the mean of the current state advantage value for normalization.The target Q-values current state is calculated by summing the advantage and state values of the next state, and subtracting the mean of the next state advantage value for normalization.The TD target is then built by combining the discounted target Q-values with the current state reward.A loss function is calculated by comparing the TD-target with the predicted Q-values, which is then used to train the network.import numpy as npimport torch as T#from deep_q_network import DeepQNetwork#from replay_memory import ReplayBufferclass DuelDQNAgent(object):    def __init__(self, gamma, epsilon, lr, n_actions, input_dims,                 mem_size, batch_size, eps_min=0.01, eps_dec=5e-7,                 replace=1000, algo=None, env_name=None, chkpt_dir='tmp/dqn'):        self.gamma = gamma        self.epsilon = epsilon        self.lr = lr        self.n_actions = n_actions        self.input_dims = input_dims        self.batch_size = batch_size        self.eps_min = eps_min        self.eps_dec = eps_dec        self.replace_target_cnt = replace        self.algo = algo        self.env_name = env_name        self.chkpt_dir = chkpt_dir        self.action_space = [i for i in range(n_actions)]        self.learn_step_counter = 0        self.memory = ReplayBuffer(mem_size, input_dims, n_actions)        self.q_eval = DeepQNetwork(self.lr, self.n_actions,                                    input_dims=self.input_dims,                                    name=self.env_name+'_'+self.algo+'_q_eval',                                    chkpt_dir=self.chkpt_dir)        self.q_next = DeepQNetwork(self.lr, self.n_actions,                                    input_dims=self.input_dims,                                    name=self.env_name+'_'+self.algo+'_q_next',                                    chkpt_dir=self.chkpt_dir)#Epsilon greedy action selection    def choose_action(self, observation):        if np.random.random() > self.epsilon:          # Add dimension to observation to match input_dims x batch_size by placing in list, then converting to tensor            state = T.tensor([observation],dtype=T.float).to(self.q_eval.device)            #As our forward function now has both state and advantage, fetch latter for actio selection            _, advantage = self.q_eval.forward(state)            action = T.argmax(advantage).item()        else:            action = np.random.choice(self.action_space)        return action     def store_transition(self, state, action, reward, state_, done):        self.memory.store_transition(state, action, reward, state_, done)     def sample_memory(self):        state, action, reward, new_state, done = \                                    self.memory.sample_buffer(self.batch_size)     states = T.tensor(state).to(self.q_eval.device)        rewards = T.tensor(reward).to(self.q_eval.device)        dones = T.tensor(done).to(self.q_eval.device)        actions = T.tensor(action).to(self.q_eval.device)        states_ = T.tensor(new_state).to(self.q_eval.device)        return states, actions, rewards, states_, dones     def replace_target_network(self):        if self.learn_step_counter % self.replace_target_cnt == 0:            self.q_next.load_state_dict(self.q_eval.state_dict())     def decrement_epsilon(self):        self.epsilon = self.epsilon - self.eps_dec \                           if self.epsilon > self.eps_min else self.eps_min     def save_models(self):        self.q_eval.save_checkpoint()        self.q_next.save_checkpoint()     def load_models(self):        self.q_eval.load_checkpoint()        self.q_next.load_checkpoint()        def learn(self):        if self.memory.mem_cntr < self.batch_size:            return        self.q_eval.optimizer.zero_grad()        #Replace target network if appropriate        self.replace_target_network()        states, actions, rewards, states_, dones = self.sample_memory()        #Fetch states and advantage actions for current state using eval network        #Also fetch the same for next state using target network        V_s, A_s = self.q_eval.forward(states)        V_s_, A_s_ = self.q_next.forward(states_)        #Indices for matrix multiplication        indices = np.arange(self.batch_size)        #Calculate current state Q-values and next state max Q-value by aggregation, subtracting constant advantage mean               q_pred = T.add(V_s,                        (A_s - A_s.mean(dim=1, keepdim=True)))[indices, actions]                q_next = T.add(V_s_,                        (A_s_ - A_s_.mean(dim=1, keepdim=True))).max(dim=1)[0]        q_next[dones] = 0.0        #Build your target using the current state reward and q_next        q_target = rewards + self.gamma*q_next         loss = self.q_eval.loss(q_target, q_pred).to(self.q_eval.device)        loss.backward()        self.q_eval.optimizer.step()        self.learn_step_counter += 1        self.decrement_epsilon()With all of supporting code defined, let’s run our main training loop. We’ve defined most of this in the initial summary, but let’s recall for posterity.For every step of a training episode, we feed an input image stack into our network to generate a probability distribution of the available actions, before using an epsilon-greedy policy to select the next actionWe then input this into the network, and obtain information on the next state and accompanying rewards, and store this into our buffer. We update our stack and repeat this process over a number of pre-defined steps.At the end of an episode, we feed the next states into our network in order to obtain the next action. We also calculate the next reward by discounting the current one.We generate our target y-values through the Q-learning update function mentioned above, and train our network.By minimizing the training loss, we update the network weight parameters to output improved state-action values for the next policy.We evaluate models by tracking their average score (measured over 100 training steps).env = make_env('VizdoomDefendLine-v0')best_score = -np.infload_checkpoint = Falsen_games = 2000agent = DuelDQNAgent(gamma=0.99, epsilon=1.0, lr=0.0001,input_dims=(env.observation_space.shape),n_actions=env.action_space.n, mem_size=5000, eps_min=0.1,batch_size=32, replace=1000, eps_dec=1e-5,chkpt_dir='/content/', algo='DuelDQNAgent',env_name='vizdoogym')if load_checkpoint:  agent.load_models()fname = agent.algo + '_' + agent.env_name + '_lr' + str(agent.lr) +'_'+ str(n_games) + 'games'figure_file = 'plots/' + fname + '.png'n_steps = 0scores, eps_history, steps_array = [], [], []for i in range(n_games):  done = False  observation = env.reset()  score = 0  while not done:    action = agent.choose_action(observation)    observation_, reward, done, info = env.step(action)    score += reward    if not load_checkpoint:      agent.store_transition(observation, action,reward, observation_, int(done))      agent.learn()    observation = observation_    n_steps += 1scores.append(score)steps_array.append(n_steps)avg_score = np.mean(scores[-100:])if avg_score > best_score:    best_score = avg_score              print('Checkpoint saved at episode ', i)    agent.save_models()print('Episode: ', i,'Score: ', score,' Average score: %.2f' % avg_score, 'Best average: %.2f' % best_score,'Epsilon: %.2f' % agent.epsilon, 'Steps:', n_steps)eps_history.append(agent.epsilon)  if load_checkpoint and n_steps >= 18000:    breakWe’ve graphed the average score of our agents together with our episodic epsilon value, across 500, 1000, and 2000 episodes below.Reward distribution of our agent after 500 episodes.Reward distribution of our agent after 1000 episodes.Reward distribution of our agent after 2000 episodes.Looking at the results and comparing them to our vanilla DQN implementation and Double DQN implementation, you’ll notice a significantly improved improvement rate in distribution across 500, 1000, and 2000 episodes. moreover, with an even more constrained reward oscillation, suggesting improved convergence when compared either implementations.We can visualize the performance of our agent at 500 and 1000 episodes below.Agent performance at 500 episodes.At 500 episodes, the agent has adapted the same strategy previously identified for DQN and DDQN at higher training times, attributed to a convergence at a local minima. Some offensive action is still taken but the primary strategy still relies on friendly fire between the monsters.What about at 1000 episodes?Agent performance at 1000 episodes.Our agent has managed to break out of the localized minima, and discovered an alternative strategy oriented around a more offensive role. This is something neither our DQN and DDQN models were capable of, even at 2000 episodes — demonstrating the utility of the two-stream approach of a DuelDQN in identifying and prioritizing actions relevant to the environment.That wraps up this implementation on Double Deep Q-learning. In our next article, we’ll finish our series on Q-learning approaches by combining all that we’ve learned into a single method, and use it on a more dynamic finale.We hope you enjoyed this article, and hope you check out the many other articles on GradientCrescent, covering applied and theoretical aspects of AI. To stay up to date with the latest updates on GradientCrescent, please consider following the publication and following our Github repositorySourcesSutton et. al, “Reinforcement Learning”Tabor, “Reinforcement Learning in Motion”Simonini, “Improvements in Deep Q Learning*Written byAdrian Yijie XuPhD Student, AI disciple — https://github.com/EXJUSTICE/ https://www.linkedin.com/in/yijie-xu-0174a325/Follow73 Sign up for The Daily PickBy Towards Data ScienceHands-on real-world examples, research,  tutorials, and cutting-edge techniques delivered Monday to Thursday. Make learning your daily ritual. Take a lookGet this newsletterBy signing up, you will create a Medium account if you don’t already have one. Review our Privacy Policy for more information about our privacy practices.Check your inboxMedium sent you an email at  to complete your subscription.73 73 Reinforcement LearningDeep LearningDoomOpenAIAIMore from Towards Data ScienceFollowA Medium publication sharing concepts, ideas, and codes.Read more from Towards Data ScienceMore From Medium5 YouTubers Data Scientists And ML Engineers Should Subscribe ToRichmond Alake in Towards Data Science7 Must-Haves in your Data Science CVElad Cohen in Towards Data Science30 Examples to Master PandasSoner Yıldırım in Towards Data Science21 amazing Youtube channels for you to learn AI, Machine Learning, and Data Science for freeJair Ribeiro in Towards Data ScienceThe Roadmap of Mathematics for Deep LearningTivadar Danka in Towards Data Science4 Types of Projects You Must Have in Your Data Science PortfolioSara A. Metwalli in Towards Data ScienceAn Ultimate Cheat Sheet for Data Visualization in PandasRashida Nasrin Sucky in Towards Data ScienceHow to Get Into Data Science Without a DegreeTerence S in Towards Data ScienceAboutHelpLegalGet the Medium app"
[Deep learning] How to build an emotional chatbot,https://towardsdatascience.com/deep-learning-how-to-build-an-emotional-chatbot-part-1-bert-sentiment-predictor-3deebdb7ea30?source=tag_archive---------1-----------------------,"Deep Learning,Reinforcement Learning,Sentiment Analysis,Bert,Hands On Tutorials","The Guardian: A Robot Wrote This Entire Article. Are You Scared Yet, Human?As the dialogue system is mature nowadays, we can download and apply advanced trained chatbots, such as GPT-3, DialoGPT, Plato and OpenNMT, etc. However, building a chatbot with generating emotional and positive responses, unlike the traditional one, is still a hot research area. (Similar to the AI mentioned in the news, we hope the AI says something positive (e.g., My creator told me to say ‘I come in peace’))Also, an emotional chatbot is very desirable in business, such as improving customer service.In these two articles, I will introduce how to build an emotional chatbot (deep neural network-based dialogue system) based on the paper “ HappyBot: Generating Empathetic Dialogue Responses by Improving User Experience Look-ahead” from HKUST and my trial.The idea is to build a dialogue system combining reinforcement learning, which rewards the positive generated responses and penalizes the negative one.Technically speaking, this task involves building a deep learning model, transfer learning (BERT), sentiment analysis, reinforcement learning, and their implementations (in PyTorch), which are all the hot topics in machine learning.Fine-tunning pre-trained BERT model for sentiment analysisIn part 1, we will build a sentiment predictor in a modern way (using transfer learning from a pre-trained BERT model).Recall our objective is: Build a dialogue system combining reinforcement learning, which rewards the positive generated responses and penalizes the negative one. Some we need a “judge” to decide a sentiment score of each generated sentence. (Sentiment analysis)BERT is currently one of the significant models in NLP tasks published in 2018 by Google. The key idea of BERT is to build the representation for natural language by using a bidirectional deep neural network with the Transformer architecture. The BERT model is frequently applied as a pre-trained model for other NLP tasks.To build a BERT Sentiment predictor (by PyTorch), one can follow the article here: Part 2: BERT Fine-Tuning Tutorial with PyTorch for Text Classification on The Corpus of Linguistic Acceptability (COLA) Dataset.Stanford Sentiment Treebank v2 (SST2)We use the Stanford Sentiment Treebank v2 (SST2) dataset for our task, which can be downloaded here. The SST2 dataset provides 239,232 sentences, where each sentence contains at most 6 sentiment labels from 1 to 25 (from most negative to most positive). We calculated the mean sentiment score for each sentence and grouped them as ’negative’ (for the score ≤10), ’neutral’ (for the 10 < score ≤ 16), and ’positive’ (for the score >16). The final Train/Val/Test data was split as 50/25/25 percent.Building the BERT Sentiment classifierEvery sentence will be tokenized, and the sentences with length larger than 160 words will be pruned. We use the prepared tokenizer from the package transformer.Then we build a multi-class (positive, neutral, and negative) classification model (softmax function) with loss function as cross-entropy from a pre-trained BERT model. Noted that we adopt the gradient clipping for avoiding gradient explosion. Here is the result of the train/valid data:The model training indicates that one epoch is already adequate, while more epochs only improve the training accuracy, but valid accuracy remains the same.After the model training, we apply our model on the testing data with 75.2% accuracy (similar to our model training) :Since our prediction is a probability of getting “positive,” “natural,” and “negative” labels, we need to transform it into a 0–1 sentiment scores as the reward:Sentiment score = 1*P(Positive) + 0.5*P(Natural) + 0*P(Negative)Where outputting 1 = highly positive and 0 = highly negative)Here are some examples:This result will be applied in the Emotional Dialogue system in the next part.ReferenceDevlin, J., Chang, M. W., Lee, K., & Toutanova, K. (2018). Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805.Jamin Shin, Peng Xu, Andrea Madotto, and Pascale Fung. Happybot: Generating empathetic dialogue responses by improving user experience look-ahead. arXiv preprint arXiv:1906.08487, 2019.Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D Manning, Andrew Ng, and Christopher Potts. 2013. Recursive deep models for semantic compositionality over a sentiment treebank. In Proceedings of the 2013 conference on empirical methods in natural language processing, pages 1631–1642.Written byJay HuiData Scientist|Fintech|Machine Learning|AI|Deep learning|NLP| From Math to Data Science https://www.linkedin.com/in/jay-hui-187222120/Follow7 Sign up for The Daily PickBy Towards Data ScienceHands-on real-world examples, research,  tutorials, and cutting-edge techniques delivered Monday to Thursday. Make learning your daily ritual. Take a lookGet this newsletterBy signing up, you will create a Medium account if you don’t already have one. Review our Privacy Policy for more information about our privacy practices.Check your inboxMedium sent you an email at  to complete your subscription.7 7 Deep LearningReinforcement LearningSentiment AnalysisBertHands On TutorialsMore from Towards Data ScienceFollowA Medium publication sharing concepts, ideas, and codes.Read more from Towards Data ScienceMore From Medium5 YouTubers Data Scientists And ML Engineers Should Subscribe ToRichmond Alake in Towards Data Science7 Must-Haves in your Data Science CVElad Cohen in Towards Data Science30 Examples to Master PandasSoner Yıldırım in Towards Data Science21 amazing Youtube channels for you to learn AI, Machine Learning, and Data Science for freeJair Ribeiro in Towards Data ScienceThe Roadmap of Mathematics for Deep LearningTivadar Danka in Towards Data Science4 Types of Projects You Must Have in Your Data Science PortfolioSara A. Metwalli in Towards Data ScienceAn Ultimate Cheat Sheet for Data Visualization in PandasRashida Nasrin Sucky in Towards Data ScienceHow to Get Into Data Science Without a DegreeTerence S in Towards Data ScienceAboutHelpLegalGet the Medium app"
Automating Q-Learning,https://medium.com/@NathanWeatherly/automating-q-learning-d4efe4130199?source=tag_archive---------2-----------------------,"Reinforcement Learning,Artificial Intelligence,Machine Learning,Python,Algorithms","This is the fourth article in my course on Reinforcement Learning. The previous article can be found here. It covers the fundamentals of Q-Learning, so if you are unfamiliar with Q-Tables and learning rates, then you should probably go back and read it. This article will cover how Q-Learning can be automated through the implementation of an Epsilon greedy strategy. This article has a full implementation in python at the GitHub.To recap what we learned last time, Q-Tables are used as a sort of pseudo-function to map each state-action pair to a certain Q-Value. We can use the rewards from the agent interacting with the environment to update the Q-Values and eventually approximate the optimal Q-Function. This article will show you how Q-Learning can be used to create a fully autonomous reinforcement learning program in three parts:Introduction to Exploration/ExploitationDefining an Epsilon greedy strategyImplementationYou might think that our system in the previous article would work well enough to be implemented, but this is actually wrong for one very specific reason. We only went through a couple iterations of the program. If we had kept going, we would find out that because the agent always selects the maximum Q-Value, the first action to give a positive reward would always be picked. This is problematic because there is no guarantee that the first positive reward will actually be the highest reward, so the agent will get stuck picking a sub-optimal action because it knows that the action is good. This introduces the problem known as “Exploration vs Exploitation”.“Multi Armed Bandit” representation. sourceThe most popular way of explaining this problem is through what is called the “Multi Armed Bandit” example. The example plays out like this:There is a person we call the bandit, who represents the agent in reinforcement learning.The bandit has an infinite number of slot machines in front of him and he can play a few of them at one time because he is “Multi Armed.” Each slot machine represents a different action in reinforcement learningThe bandit’s ultimate goal is to make as much money as possible by playing any of the slot machines. The money represents a reward in reinforcement learning.Now that you understand the situation, consider the two strategies that the bandit could take:Immediately start playing only the machines he finds are the most profitable and don’t branch out and try new machines. This is a very conservative strategy and is known as “Exploitation” because the bandit is exploiting his limited knowledge of the different slot machines.Keep trying different slot machines in order to find the slot machines that are the most profitable. This strategy is more risky, but has a much higher reward because the bandit will find slot machines that are more and more profitable as time goes on. This is known as “Exploration” because the bandit is exploring new slot machines to find the best one.Clearly the best strategy would be some balance between these two extremes. For example, the bandit could use half of his arms to play the most profitable machines he has found, and the other half for trying new machines to find ones that are more profitable. The problem of determining the best ratio between the two strategies is known as “Exploration vs Exploitation.”The problem has no definite answer, but one of the simplest proposed solutions that is both intuitive and widely used is reinforcement learning is known as the Epsilon greedy strategy. This strategy is derived from the idea that as time goes on, the bandit should move towards a more exploitation based approach because the probability of finding a machine that improves on the current ones decreases as he finds better machines. Simply put, an Epsilon greedy strategy starts at a high rate of exploration and then moves towards a high rate of exploitation as time goes on. This is very broad and can be done in many ways because the proportion of resources devoted to exploration, ε (Epsilon), can decrease linearly, logarithmically, or by any other function. An example of a linear model would look like this:The amount of exploration, ε, would decrease by 0.01 every pre-defined time period until it hit the minimum value of 0, where the agent would be using purely exploitation. In practice, a minimum value would be set above 0, like 0.01, so that the agent never completely stops exploring. The definition of one time period for this formula can vary, but generally Epsilon is updated every episode. An episode is over when the environment reaches some terminal state. Training length in reinforcement learning is often defined by the number of episodes. For games like Tic-Tac-Toe, one episode would just be one game played to completion.Now that the value of Epsilon is defined, we can use it to modify the process of Q-Learning. Because many environments won’t have multiple actions to take at once, we can’t devote some “arms” to exploration and others to exploitation. To combat this, we can treat the value of Epsilon as the probability that the agent carries out an exploration action. Exploration is carried out by randomly selecting an action from the pool of all actions. Otherwise, exploitation is done by selecting the action with the highest Q-Value, like what was done in the previous article.Now, if we write the entire process of Q-Learning down as pseudo-code, it would look like this:1- Define Environment, Reward Function, Action Space, Observation Space2- Initialize Epsilon, Min Epsilon, Epsilon Decrement, Gamma, Alpha, Q-Table, Number of Episode3- Iterate over Number of Episodes: {3.1- Define the initial State3.2- Iterate until a Terminal State is reached: {3.2(a)- Choose whether to Exploit or Explore based on current Epsilon3.2(b)- Pick action using Q-Table given the State and Exploit/Explore3.2(c)- Take action and receive Next State and Reward from Environment3.2(d)- Calculate Optimal Q-Value using Bellman Equation with Gamma3.2(e)- Update Q-Table using Alpha and Optimal Q-Value }3.3- Update Epsilon using the three Epsilon parameters }1- Define Environment, Reward Function, Action Space, Observation Space2- Initialize Epsilon, Min Epsilon, Epsilon Decrement, Gamma, Alpha, Q-Table, Number of Episode3- Iterate over Number of Episodes: {3.1- Define the initial State3.2- Iterate until a Terminal State is reached: {3.2(a)Choose whether to Exploit or Explore based on current Epsilon3.2(b)Pick action using Q-Table given the State and Exploit/Explore3.2(c)Take action and receive Next State and Reward from Environment3.2(d)Calculate Optimal Q-Value using Bellman Equation with Gamma3.2(e)Update Q-Table using Alpha and Optimal Q-Value }3.3- Update Epsilon using the three Epsilon parameters }}If you are having trouble putting this into practice, my implementation for the Tic-Tac-Toe environment I built here is below as a reference. I have commented out what each block of code does and noted which ones are from the pseudo-code implementation.Now that you better understand the process of Q-Learning, you might be able to see its biggest flaw; It only works when the state and action spaces are discrete. This means that it can’t take continuous values as inputs or output continuous values which severely limits its usefulness. Q-Learning is also very computation heavy and cannot be used for problems with very large input and output spaces because the Q-Table’s size would significantly slow down learning. To solve this, the next article will introduce you to what is known as Deep Q-Network, which integrates Reinforcement Learning with Neural Networks.Written byNathan WeatherlyHigh School Student | Machine Learning | Mathematics | Statistics | Reinforcement Learning | Data ScienceFollowThanks to Matt We. Reinforcement LearningArtificial IntelligenceMachine LearningPythonAlgorithmsMore from Nathan WeatherlyFollowHigh School Student | Machine Learning | Mathematics | Statistics | Reinforcement Learning | Data ScienceMore From MediumWestWorld series helps us to think about how handle Outliers in the Machine LearningFelipe Calixto Filho in Analytics VidhyaMachine Learning for Humans, Part 2.1: Supervised LearningVishal Maini in Machine Learning for HumansActivation Functions, Optimization Techniques, and Loss FunctionsAfaf Athar in Analytics VidhyaSaving and Loading of Keras Sequential and Functional ModelsVishnuvardhan Janapati in The StartupShould You Use Machine Learning?Devin Soni 👑 in Better ProgrammingSlateQ: A scalable algorithm for slate recommendation problemsShishir Kumar in Analytics VidhyaArabic Word Embeddings — A Historical AnalysisKenan AlkiekPractical aspects — Logistic Regression in layman termsPrateek Karkare in AI GraduateAboutHelpLegalGet the Medium app"
Decisions from Data: How Offline Reinforcement Learning Will Change How We Use Machine Learning,https://medium.com/@sergey.levine/decisions-from-data-how-offline-reinforcement-learning-will-change-how-we-use-ml-24d98cb069b0?source=tag_archive---------0-----------------------,"Machine Learning,Reinforcement Learning,Artificial Intelligence,Data Science","A ride sharing company collects a dataset of pricing and discount decisions with corresponding changes in customer and driver behavior, in order to optimize a dynamic pricing strategy. An online vendor records orders and inventory levels to generate an inventory level management policy. An autonomous car company records driving data from human drivers to train an improved end-to-end vision-based driving controller.All of these applications have two things in common: we would consider each of them to be classic examples of how machine learning can enable smarter decisions with data, and each of them is not actually feasible with the kind of machine learning technologies that are widely in use today without manual design of decision rules and state machines. I will discuss how recent advances in the field of offline reinforcement learning can change that in the next few years. I believe that we stand on the cusp of a revolution in how data can inform automated decision-making. Outside of a few applications in advertising and recommender systems, ML-enabled decision making systems have generally relied on supervised learning methods for prediction, followed by manually designed decision rules that utilize these predictions to choose how to act. While reinforcement learning algorithms have made tremendous headway in providing a toolkit for automated end-to-end decision making in research, this toolkit has proven difficult to apply in reality because, in its most common incarnation, it is simply too hard to reconcile with the data-driven machine learning pipelines in use today. This may change once reinforcement learning algorithms can effectively use data, and I will discuss how that might happen.How machine learning systems make decisionsFirst, we must draw a distinction between prediction and decision making. Supervised learning systems make predictions. These predictions can then be used to make decisions, but how a prediction turns into a decision is up to the user. If a model forecasts that customer orders will increase 200% in October, a reasonable decision is to increase inventory levels accordingly. However, the model is not telling us that increasing inventory levels will lead to larger profits. Not only does it not account for the distribution shift induced by acting on the model’s own predictions, it also fails to account for the entirety of the decision making process. Real-world decision making systems face a sequential and iterated problem, where each decision influences future events, which in turn influence future decisions. Here are some of the differences between the assumptions made by supervised predictive modeling systems and the properties of real-world sequential decision making problems:Supervised predictionPredicts manually selected quantities (e.g., number of customer orders)Decisions must be made based on predictions manually, using human intuition and hand-crafted rulesAssumes i.i.d. (independent and identically distributed) dataIgnores feedback, which changes how inputs map to outputs when the learning system itself interacts with the world (e.g., customers may not react the same way to auto-generated recommendations as they did during data collection)Sequential decision makingOnly the objective is manually specified (e.g., maximize profits)Requires outputting near-optimal actions that will lead to desired outcomes (e.g., how to alter inventory levels to maximize profits)Each observation is part of a sequential process, each action influences future observations (not i.i.d.)Feedback is critical, and may be utilized to achieve desired goals through long-term interactionReinforcement learning (RL) is concerned most directly with the decision making problem. RL has attained good results on tasks ranging from playing games to enabling robots to grasp objects. RL algorithms directly aim to optimize long-term performance in the face of a dynamic and changing environment that reacts to each decision. However, most reinforcement learning methods are studied in an active learning setting, where an agent directly interacts with its environment, observes the outcomes of its actions, and uses these attempts to learn through trial and error, as shown below.The classic diagram of reinforcement learning: a fundamentally active and online learning process.Instantiating this framework with real-world data collection is difficult, because partially trained agents interacting with real physical systems require careful oversight and supervision (would you want a partially trained RL policy to make real-world inventory purchasing decisions?). For this reason, most of the work that utilizes reinforcement learning relies either on meticulously hand-designed simulators, which preclude handling complex real-world situations, especially ones with unpredictable human participants, or requires carefully designed real-world learning setups, as in the case of real-world robotic learning. More fundamentally, this precludes combining RL algorithms with the most successful formula in ML. From computer vision to NLP to speech recognition, time and time again we’ve seen that large datasets, combined with large models, can enable effective generalization in complex real-world settings. However, with active online RL algorithms that must recollect their dataset each time a new model is trained, such a formula becomes impractical. Here are some of the differences between the active RL setup and data-driven machine learning:Active (online) reinforcement learningAgent collects data each time it is trainedAgent must collect data using its own (partially trained) policyEither uses narrow datasets (e.g., collected in one environment), or manually designed simulatorsGeneralization can be poor due to small, narrow datasets, or simulators that differ from realityData-driven machine learningData may be collected once and reused for all modelsData can be collected with any strategy, including a hand-engineered system, humans, or just randomlyLarge and diverse datasets can be collected from all available sourcesGeneralization is quite good, due to large and diverse datasetsOffline reinforcement learningTo perform effective end-to-end decision making in the real world, we must combine the formalism of reinforcement learning, which handles feedback and sequential decision making, with data-driven machine learning, which learns from large and diverse datasets, and therefore enables generalization. This necessitates removing the requirement for active data collection and devising RL algorithms that can learn from prior data. Such methods are referred to as batch reinforcement learning algorithms, or offline reinforcement learning (I will use the term “offline reinforcement learning,” since it is more self-explanatory, though the term “batch” is more common in the foundational literature). The diagram below illustrates the differences between classic online reinforcement learning, off-policy reinforcement learning, and offline reinforcement learning:Illustration of (a) online RL, (b) off-policy RL with online data collection, and (c) offline RL.In online RL, data is collected each time the policy is modified. In off-policy RL, old data is retained, and new data is still collected periodically as the policy changes. In offline RL, the data is collected once, in advance, much like in the supervised learning setting, and is then used to train optimal policies without any additional online data collection. Of course, in practical use, offline RL methods can be combined with modest amounts of online finetuning, where after an initial offline phase, the policy is deployed to collect additional data to improve online.Crucially, when the need to collect additional data with the latest policy is removed completely, reinforcement learning does not require any capability to interact with the world during training. This removes a wide range of cost, practicality, and safety issues: we no longer need to deploy partially trained and potentially unsafe policies, we no longer need to figure out how to conduct multiple trials in the real world, and we no longer need to build complex simulators. The offline data for this learning process could be collected from a baseline manually designed controller, or even by humans demonstrating a range of behaviors. These behaviors do not need to all be good either, in contrast to imitation learning methods. This approach removes one of the most complex and challenging parts of a real-world reinforcement learning system.However, the full benefit of offline reinforcement learning goes even further. By making it possible to utilize previously collected datasets, offline RL can utilize large and diverse datasets that are only practical to collect once — datasets on the scale of ImageNet or MS-COCO, which capture a wide, longitudinal slice of real-world situations. For example, an autonomous vehicle could be trained on millions of videos depicting real-world driving. An HVAC controller could be trained using logged data from every single building in which that HVAC system was ever deployed. An algorithm that controls traffic lights to optimize city traffic could utilize data from many different intersections in many different cities. And crucially, all of this could be done end-to-end, training models that directly map rich observations or features directly to decisions that optimize user-specified objective functions.How do offline reinforcement learning algorithms work?The fundamental challenge in offline reinforcement learning is distributional shift. The offline training data comes from a fixed distribution (sometimes referred to as the behavior policy). The new policy that we learn from this data induces a different distribution. Every offline RL algorithm must contend with the resulting distributional shift problem. One widely studied approach in the literature is to employ importance sampling, where distributional shift can lead to high variance in the importance weights. Algorithms based on value functions (e.g., deep Q-learning and actor-critic methods) must contend with distributional shift in the inputs to the Q-function: the Q-function is trained under the state-action distribution induced by the behavior policy, but evaluated, for the purpose of policy improvement, under the distribution induced by the latest policy. Using the Q-function to evaluate or improve a learned policy can result in out-of-distribution actions being passed into the Q-function, leading to unpredictable and likely incorrect predictions. When the policy is optimized so as to maximize its predicted Q-values, this leads to a kind of “adversarial example” problem, where the policy learns to produce actions that “fool” the learned Q-function into thinking they are good.Most successful offline RL methods address this problem with some type of constrained or conservative update, which either avoids excessive distribution shift by limiting how much the learned policy can deviate from the behavior policy, or explicitly regularizes learned value functions or Q-functions so that the Q-values for unlikely actions are kept low, which in turn also limits the distribution shift by dis-incentivizing the policy from taking these unlikely, out-of-distribution actions. The intuition is that we should only allow the policy to take those actions for which the data supports viable predictions.Of course, at this point, we might ask — why should we expect offline RL to actually improve over the behavior policy at all? The key to this is the sequential nature of the decision making problem. While at any one time step, the actions of the learned policy should remain close to the distribution of behaviors we’ve seen before, across time steps, we can combine bits and pieces of different behaviors we’ve seen in the data. Imagine learning to play a new card game. Even if you play your cards at random, on some trials some of your actions will — perhaps by accident — lead to favorable outcomes. By looking back on all of your experiences and combining the best moves into a single policy, you can arrive at a policy that is substantially better than any of your previous plays, despite being composed entirely of actions that you’ve made before.Building on these ideas, recent advances in offline reinforcement learning have led to substantial improvements in the capabilities of offline RL algorithms. A complete technical discussion of these methods is outside the scope of this article, and I would refer the reader to our recent tutorial paper for more details. However, I will briefly summarize several recent advances that I think are particularly exciting:Policy constraints: A simple approach to control distributional shift is to limit how much the learned policy can deviate from the behavior policy. This is especially natural for actor-critic algorithms, where policy constraints can be formalized as using the following type of policy update:The constraint, expressed in terms of some divergence (“D”), limits how far the learned policy deviates from the behavior policy. Examples include KL-divergence constraints and support constraints. This class of methods is summarized in detail in our tutorial. Note that such methods require estimating the behavior policy by using another neural network, which can be a substantial source of error.Implicit constraints: The AWR and AWAC algorithms instead perform offline RL by using an implicit constraint. Instead of explicitly learning the behavior policy, these methods solve for the optimal policy via a weighted maximum likelihood update of the following form:Here, A(s,a) is an estimate of the advantage, which is computed in different ways for different algorithms (AWR uses Monte Carlo estimates, while AWAC uses an off-policy Q-function). Using this type of update to enforce constraints has been explored in a number of prior works (see, e.g., REPS), but has only recently been applied to offline RL. Computing the expectation under the behavior policy only requires samples from the behavior policy, which we can obtain directly from the dataset, without actually needing to estimate what the behavior policy is. This makes AWR and AWAC substantially simpler, and enables good performance in practice.Conservative Q-functions: A very different approach to offline RL, which we explore in our recent conservative Q-learning (CQL) paper, is to not constrain the policy at all, but instead regularize the Q-function to assign lower values to out-of-distribution actions. This prevents the policy from taking these actions, and results in a much simpler algorithm that in practice attains state-of-the-art performance across a wide range of offline RL benchmark problems. This approach also leads to appealing theoretical guarantees, allowing us to show that conservative Q-functions are guaranteed to lower bound the true Q-function with an appropriate choice of regularizer, providing a degree of confidence in the output of the method.Despite these advances, I firmly believe that the most effective and elegant offline RL algorithms have yet to be invented, which is why I consider this research area to be so promising both for its practical applications today and for its potential as a topic of research in the future.What about artificial intelligence?Aside from its practical value, much of the appeal of reinforcement learning also stems from the widely held belief that reinforcement learning algorithms hold at least part of the answer to the development of intelligent machines — AI systems that emulate or reproduce some or all of the capabilities of the human mind. While a complete solution to this puzzle may be far in the future, I would like to briefly address the relevance of offline RL to this (perhaps distant) vision.In its classical definition, the active learning framework of RL reflects a very reasonable model of an adaptive natural learning system: an animal observes a stimulus, adjusts its model, and improves its response to that stimulus to attain larger rewards in the future. Indeed, reinforcement learning originated in the study of natural intelligence, and only made its way into artificial intelligence later. It may therefore seem like a step in the wrong direction to remove the “active” part of this learning framework from consideration.However, I would put forward an alternative argument: in the first few years of your life, your brain processed a broad array of sights, sounds, smells, and motor commands that rival the size and diversity of the largest datasets used in machine learning. While learning online from streaming data is definitely one facet of the AI problem, processing large and diverse experiences seems to be an equally critical facet. Current supervised learning methods operate far more effectively in “batch” mode, making multiple passes over a large dataset, than they do in “online” mode with streaming data. Cracking the puzzle of online continual learning may one day change that, but until then, we can make a great deal of progress with such batch-mode methods. It then stands to reason that a similar logic should be applied to RL: while understanding continual online learning is important, equally important is understanding large-scale learning and generalization, and these facets of the problem will likely be far more practical to tackle in the offline setting, and then extended into the online and continual setting once our understanding of online and continual algorithms catches up to our understanding of large-scale learning and generalization. Utilizing large amounts of data for decision making effectively will need to be a part of any generalizable AI solution, and right now, offline RL offers us the most direct path to study how to do that.Concluding remarksOffline reinforcement learning algorithms hold the promise of turning data into powerful decision-making strategies, enabling end-to-end learning of policies directly from large and diverse datasets and bringing large datasets and large models to bear on real-world decision-making and control problems. However, the full promise of offline RL has not yet been realized, and major technical hurdles remain. Fundamentally, offline RL algorithms must be able to reason about counterfactuals: what will happen if we take a different action? Will the outcome be better, or worse? Such questions are known to be exceptionally difficult for statistical machine learning systems, and while recent innovations in offline RL based around distributional constraints and conservative targets can provide a partial solution, at its core this problem touches on deep questions in the study of causality, distributional robustness, and invariance, and connects at a fundamental level with some of the most challenging problems facing modern machine learning. While this will present major challenges, it also makes this topic particularly exciting to study.For readers interested in learning more about this topic, I would recommend a tutorial article that I’ve co-authored with colleagues on this subject, as well as the “Datasets for Data-Driven Deep Reinforcement Learning” benchmark suite, which includes code and implementations for many of the latest algorithms. Aviral Kumar and I will also be giving a tutorial on offline reinforcement learning at NeurIPS 2020. Hope to see you there!I want to acknowledge helpful feedback from Chelsea Finn and Aviral Kumar on an earlier draft of this article.Written bySergey LevineSergey Levine is a professor at UC Berkeley. His research is concerned with machine learning, decision making, and control, with applications to robotics.Follow674 1 674 674 1 Machine LearningReinforcement LearningArtificial IntelligenceData ScienceMore from Sergey LevineFollowSergey Levine is a professor at UC Berkeley. His research is concerned with machine learning, decision making, and control, with applications to robotics.More From MediumAccelerate your NLP pipelines using Hugging Face Transformers and ONNX RuntimeTianlei Wu in Microsoft AzurePredicting Metrics With Neural NetsTobias Kächele in The StartupUnderstanding Decision TreesVikas Solegaonkar in The StartupA Probabilistic Algorithm to Reduce Dimensions: t — Distributed Stochastic Neighbor Embedding…Rajvi Shah in Towards AIHandwritten Digit Recognition Using PyTorch, Get 99.5% accuracy in 20 k parameters.Ravi vaishnavAnomaly Detection with Azure Stream AnalyticsLuuk Mager in Analytics VidhyaMeet MixNet: Google Brain’s new State of the Art Mobile AI architecture.Less WrightTypes of regression in Machine learning.Anju Rajbangshi in Data Driven InvestorAboutHelpLegalGet the Medium app"
Differentiable Programming and Neural ODEs for Accelerating Model Based Reinforcement Learning and Optimal Control,https://medium.com/swlh/neural-ode-for-reinforcement-learning-and-nonlinear-optimal-control-cartpole-problem-revisited-5408018b8d71?source=tag_archive---------1-----------------------,"Machine Learning,Control,Reinforcement Learning,Artificial Intelligence,Robotics","Strategies learnt under a minute: 1-go swing up (left), resonant incremental swing up with force constraint (right)We will explain the theory in detail first. Feel free to jump to the code section.AbstractWe simplify and accelerate training in model based reinforcement learning problems by using end-to-end differentiable programming in Julia. We compute policy gradients by differentiating through a continuous time neural ODE consisting of the environment and neural network agent, though the technique applies to discrete time also. We train in seconds on a single instance agents for the harder swing up variant of the cartpole problem under different objectives. For comparison, Microsoft Bonsai in its demo seems to take much longer to solve the easier initially upright cartpole balancing problem — representative of traditional reinforcement learning that does not first fit a differentiable environmental model.Motivation: (much) faster reinforcement learningReinforcement learning (RL) is still a baby in the machine learning family. While computer vision, natural language processing, and recommendation systems touch our lives everyday, reinforcement learning is just starting to make an impact. Sure, there are impressive demos by Google’s Alpha Go, OpenAI 5, and Alpha Star. However they required large engineering teams and tons of compute. Even simple games can take dreadful amounts of tuning and millions of training epochs. Part of the problem is we’re treating the environment too much as a black box. State of the art policy gradient algorithms (A2C including PPO) essentially sample this black box and iteratively design consistent policy and value estimators. In some cases the environment is indeed a black box, e.g. reacting to a novel environment for robotics or playing against an external adversary in games.However, in many scenarios we already own the environmental simulation! Examples: video games, self driving simulators, control systems, constrained robotics, industrial automation, and process engineering. Why sample around the system dynamics when we know it already?For model based RL, we should combine the model and neural network agent into a single system loop that as a whole can be differentiated to yield policy gradients for the agent. (Our policy is deterministic wrt state but a stochastic formulation is possible.) In the past the derivative program was made by hand, e.g. optimal control in aeronautics. However, recent advances in differentiable programming and neural ODEs can automate the process! We demonstrate an orders of magnitudes improvement in learning speed on the “Hello World” of reinforcement learning: the cartpole problem.Toy ProblemIn the cartpole problem, the usual goal is to balance an upright pole by moving the base cart as available in the OpenAI Gym. This is actually too easy because the system is fairly linear at small angles :) Instead, we’ll start the pole hanging down and then compute the base movements to bring it up and balanced, aka cartpole swing up. This sweeps through the system’s nonlinearities. We wish to minimize the angle, angular velocity, and cart velocity in the end while staying within a time limit.Source: https://www.ashwinnarayan.com/post/cartpole-dynamics/The System (Environment) is the cartpole, while the Controller (Agent) dictates how to move the cart. The system state space u consists of the cart position, cart velocity, pole angle, and pole angular velocity. The time derivative f describes its time evolution. It’s a function of its current state u and the control action which is the force applied to the cart. We make the controller a neural network g(u, p) with the system state u as input and parameters p as the weights. We specify the system initial condition and let it run, generating a trajectory in state space. We construct a loss functional l acting on the trajectory, penalizing deviations from desired behavior. Goal is to minimize l wrt p, ie seeking the optimal weights for the neural controller.Magic ingredient: differentiable programmingThey key difference from traditional RL is bundling the agent and environment as one differentiable system. For us, the neural network agent’s action affects the system’s time evolution, or the ODE’s time derivative. This “neural ODE” setup were first popularized in the 2018 paper “Neural Ordinary Differential Equations,” winning the best paper award at the prestigious NIPS conference. The original paper used it to approximate residual connections in discrete neural networks to improve parameter efficiency. However, it has since been used for time series and dynamical systems modeling, going back into the field it came from. We train the “neural” part of the neural ODE to be our agent or controller while the system evolves as a function of its state variables and the control signal.You might wonder how on earth we can differentiate this neural network embedded as part of an ODE that is integrated in time to yield a trajectory along which the state is sampled at multiple time points to compute the loss!? Actually, you’ve already answered the question :)The loss depends on the neural network weights through a chain of functions. The chain rule as applied to the NN is called backpropagation. Now we just need to do the same for the ODE. The PDE and inverse problems literature provide the solution as the “adjoint method,” which yields a “dual” adjoint ODE which is integrated backwards in time. Luckily, the language Julia has a powerful automatic differentiation package called Zygote which can do the dirty work ;) We simply code up the simulation and can compute the gradient with just 1 line of code! In Python, Tensorflow and Pytorch perform AD on graphs of predefined neural network building blocks in their libraries, whereas Julia’s Flux (and Zygote underneath) does this for (almost) arbitrary functions written in Julia! Zygote digs into the compiler and automagically applies the chain rule on intermediate instructions. This results in performant statically compiled gradient code.Julia is a modern general purpose Pythonic language but with easier and more performant syntax for scientific computing and differentiable programming. Think of it as a happy marriage between Python, R, Matlab, and C++.CodeComplete code in Julia is at Github.We first construct time derivative of the system. A derivation using Lagrangian mechanics is at https://metr4202.uqcloud.net/tpl/t8-Week13-pendulum.pdf .# physical paramsm = 1 # pole mass kgM = 2 # cart mass kgL = 1 # pole length mg = 9.8 # acceleration constant m/s^2# map angle to [-pi, pi)modpi(theta) = mod2pi(theta + pi) - pi#=system dynamics derivativedu: du/dt, state vector derivative updated inplaceu: state vector (x, dx, theta, dtheta)p: parameter function, here lateral force exerted by cart as a fn of timet: time=#function cartpole(du, u, p, t)  # position (cart), velocity, pole angle, angular velocity  x, dx, theta, dtheta = u  force = p(t)  du[1] = dx  du[2] =    (force + m * sin(theta) * (L * dtheta^2 - g * cos(theta))) /    (M + m * sin(theta)^2)  du[3] = dtheta  du[4] =    (-force * cos(theta) - m * L * dtheta^2 * sin(theta) * cos(theta) + (M + m) * g * sin(theta)) / (L * (M + m * sin(theta)^2))endNext we define our controller neural network as a MLP with 1 hidden layer. If we require a more nonlinear agent we can obviously use a deeper network.# neural network controller, here a simple MLP# inputs: cos(theta), sin(theta), theta_dot# output: cart forcecontroller = FastChain((x, p) -> x, FastDense(3, 8, tanh), FastDense(8, 1))# initial neural network weightspinit = initial_params(controller)We now set up the whole neural ODE and define the ODE solver that integrates it forward in time.#=system dynamics derivative with the controller included=#function cartpole_controlled(du, u, p, t)  # controller force response  force = controller([cos(u[3]), sin(u[3]), u[4]], p)[1]  du[5] = force# plug force into system dynamics  cartpole(du, u[1:4], t -> force, t)end# initial conditionu0 = [0; 0; pi; 0; 0]tspan = (0.0, 1.)N=50tsteps = range(tspan[1], length = N, tspan[2])dt = (tspan[2] - tspan[1]) / N# push!(u0, 0)# set up ODE problemprob = ODEProblem(cartpole_controlled, u0, tspan, pinit)# wrangles output from ODE solverfunction format(pred)  x = pred[1, :]  dx = pred[2, :]theta = modpi.(pred[3, :])  dtheta = pred[4, :]# take derivative of impulse to get force  impulse = pred[5, :]  tmp = (impulse .- circshift(impulse, 1)) / dt  force = [tmp[2],tmp[2:end]...]return x, dx, theta, dtheta, forceend# solves ODEfunction predict_neuralode(p)  tmp_prob = remake(prob, p = p)  solve(tmp_prob, Tsit5(), saveat = tsteps)endWe define our loss function to penalize angular and velocity deviations at the end. We add a penalty for average angular deviation, thus encouraging the controller to swing up the pole faster. We use least squares penalties but you can use any function, including log or even discontinuous penalties!# loss to minimize as a function of neural network parameters pfunction loss_neuralode(p)  pred = predict_neuralode(p)  x, dx, theta, dtheta, force = format(pred)  loss = sum(theta .^ 2) / N + 4theta[end]^2 + dx[end]^2return loss, predendFinally we traini = 0 # training epoch counterdata = 0 # time series of state vector and control signal# callback function after each training epochcallback = function (p, l, pred; doplot = true)  global i += 1global data = format(pred)  x, dx, theta, dtheta, force = data# ouput every few epochs  if i % 50 == 0    println(l)    display(plot(tsteps, theta))    display(plot(tsteps, x))    display(plot(tsteps, force))  endreturn falseendresult = DiffEqFlux.sciml_train(  loss_neuralode,  pinit,  ADAM(0.05),  cb = callback,  maxiters = 1000,)p = result.minimizer# save model and dataopen(io -> write(io, json(p)), ""model.json"", ""w"")open(io -> write(io, json(data)), ""data.json"", ""w"")And animate :)gr()x, dx, theta, dtheta, force = dataanim = Animation()plt=plot(tsteps,[modpi.(theta.+.01),x,force],title=[""Angle"" ""Position"" ""Force""],layout=(3,1))display(plt)savefig(plt,""cartpole_data.png"")for (x, theta) in zip(x, theta)cart = [x - 1 x + 1; 0 0]    pole = [x x + 10*sin(theta); 0 10*cos(theta)]    plt = plot(        cart[1, :],        cart[2, :],        xlim = (-10, 10),        ylim = (-10, 10),        title = ""Cartpole"",        linewidth = 3,    )    plot!(plt, pole[1, :], pole[2, :], linewidth = 6)frame(anim)endgif(anim, ""cartpole_animation.gif"", fps = 10)ResultsTraining to a decent solution takes less than a minute! The controller (agent) neural network first accelerates the cart rapidly to swing the pole. It lets the pole clear the horizon on its angular momentum and then accelerates the cart in the opposite direction to continue the pole’s ascent. Finally, it does a small correction for a standstill at the upright position. Perfecto! We didn’t tell it how to move. It learned on its own!Beauty of the method lies in its versatility. If the objectives and constraints change, we can change the loss function accordingly. If there’s friction and we wish to minimize the energy loss, we simply add a friction term to the model and tag on a frictional power integral in the loss function.To illustrate, suppose our mechanical engineer says the motor can’t generate the 60 N peak force demanded by the controller. Actuator limits are common in real life. Motors, fans, pumps all have finite capacity. To reduce the force, we add a max force penalty to the loss function. Also, we increase the time allotted to 10s.tspan = (0.0, 10.0)...loss = sum(theta .^ 2) / N + 4theta[end]^2 + dx[end]^2 + .1sum(x .^ 2) / N + .001maximum(force.^2)Guess what happens upon training?Ingenious! The AI taps into the natural system resonance like a child on a swing. It periodically accelerates and decelerates to couple energy into the pole, which reduces the peak force required by 10x!CaveatsNeural ODEs can be prone to local minima or underfitting. This isn’t due to the neural network but rather the natural minima of the system. For example, depending on the loss function, the cart can end up not moving. In the language of the calculus of variations, any perturbation to the starting trajectory may in fact increase the loss. In such cases, one needs to adjust the loss function, perhaps penalizing the system for staying still at the beginning. See https://diffeqflux.sciml.ai/dev/examples/local_minima/DiscussionCredit goes to automatic differentiation, namely Julia’s Zygote. It saves us from having to hard code the gradient function of (almost) arbitrary code, just as Tensorflow did so for the restricted case of neural network chains.Neural ODE is a continuum model that requires no time discretization or interpolation of the dynamics. The ODE solver is instructed to sample at whatever time points necessary to compute the loss. Our method is not only well suited for continuous time physical systems, it also works on discrete time systems. AD would work the same on a computer game loop (even faster).Third, we can make the system simulation data driven. There, we can replace the physics (or part thereof) with a system dynamics neural network. We train it on observation data, such as sensors time series from a vehicle or industrial process. Then, we stick in the controller neural network to compute optimal control.AcknowledgmentThanks to the open source Julia community: Chris Rackauckas for DiffEqFlux, Mike Innes for Zygote, and many others..References & TutorialsNeural Ordinary Differential Equations. https://arxiv.org/abs/1806.07366Neural Ordinary Differential Equations with sciml_train. https://diffeqflux.sciml.ai/dev/examples/neural_ode_sciml/Forecasting the weather with neural ODEs. https://sebastiancallh.github.io/post/neural-ode-weather-forecast/A Differentiable Programming System to Bridge Machine Learning and Scientific Computing. https://arxiv.org/abs/1907.07587NoteThis is getting turned into a conference submission. I’d also love to help if you have any other question, idea or problem.Paul ShenMS Electrical Engineering, BS Math, Stanford University https://www.linkedin.com/in/paulxshen/The StartupMedium's largest active publication, followed by +723K people. Follow to join our community.Follow121 2 Machine LearningControlReinforcement LearningArtificial IntelligenceRobotics121 claps121 claps2 responsesWritten byPaul ShenFollowElectrical Engineer | Stanford MS BS | ML + NLP + Signal Processing + Control SystemsFollowThe StartupFollowMedium's largest active publication, followed by +723K people. Follow to join our community.FollowWritten byPaul ShenFollowElectrical Engineer | Stanford MS BS | ML + NLP + Signal Processing + Control SystemsThe StartupFollowMedium's largest active publication, followed by +723K people. Follow to join our community.More From MediumHello World-Implementing Neural Networks With NumPyVasista Ayyagari in The StartupEnsemble Methods: Tuning a XGBoost model with Scikit-LearnSilvan MirandaIntroduction to Text Representations for Language Processing — Part 2Sundaresh Chandran in The StartupAn intuitive introduction to Reinforcement LearningIntisar Tahmid in intelligentmachinesCreate Your Own Open Source Natural Language Processing APICobus GreylingWhat to do when your training and testing data come from different distributionsNezar Assawiel in freeCodeCamp.orgClassification Visualizations with YellowbrickAlex FiotoNatural Language Processing — Emotion Detection With Multi-class, Multi-label Convolutional Neural…Rian van den Ander in The StartupLearn more.Medium is an open platform where 170 million readers come to find insightful and dynamic thinking."
"<strong class=""bt"">Reinforcement Learning frameworks</strong>",https://towardsdatascience.com/reinforcement-learning-frameworks-e349de4f645a?source=tag_archive---------2-----------------------,"Reinforcement Learning,Artificial Intelligence,Deep R L Explained,Towards Data Science,Machine Learning","Welcome to this 20th post that concludes the “Deep Reinforcement Learning Explained” series that presents a practical approach to getting started in the exciting world of Deep Reinforcement Learning.So far, in previous posts, we have been looking at a basic representation of the corpus of RL algorithms (although we have skipped several) that have been relatively easy to program. But from now on, we need to consider both the scale and complexity of the RL algorithms. In this scenario, programming a Reinforcement Learning implementation from scratch can become tedious work with a high risk of programming errors.To address this, the RL community began to build frameworks and libraries to simplify the development of RL algorithms, both by creating new pieces and especially by involving the combination of various algorithmic components. In this post, we will make a general presentation of those Reinforcement Learning frameworks and solving the previous problem of CartPole using RLlib, an open-source library in Python, based on Ray framework.Reinforcement Learning frameworksMotivationBut before continuing, as a motivational example, let’s remember that in the previous post, we presented REINFORCE and its limitations. The research community created many training algorithms to solve it: A2C, A3C, DDPG, TD3, SAC, PPO, among many others. But programming these algorithms from scratch becomes more convoluted than that of REINFORCE. Also, the more involved you become in the field, the more often you will realise that you are writing the same code over and over again.The practical applications of Reinforcement Learning are relatively young compared to other domains as Deep Learning, where well-established frameworks as TensorFlow, PyTorch, or MXnet simplify DL practitioners’ lives. However, the emergence of RL frameworks has already begun and right now we can choose from several projects that greatly facilitate the use of advanced RL methods.Before presenting these RL frameworks, let’s see a bit of their context.Learning from interactions instead of examplesIn the last several years pattern-recognition side has been the focus of much of the work and much of the discussion in the community of Deep Learning. We are using powerful supercomputers that process large labeled data sets (with expert-provided outputs for the training set), and apply gradient-based methods that find patterns in those data sets that can be used to predict or to try to find structures inside the data.This contrasts with the fact that an important part of our knowledge of the world is acquired through interaction, without an external teacher telling us what the outcomes of every single action we take will be. Humans are able to discover solutions to new problems from interaction and experience, acquiring knowledge about the world by actively exploring it.For this reason, current approaches will study the problem of learning from interaction with simulated environments through the lens of Deep Reinforcement Learning (DRL), a computational approach to goal-directed learning from the interaction that does not rely on expert supervision. I.e., a Reinforcement Learning Agent must interact with an Environment to generate its own training data.This motivates interacting with multiple instances of an Environment in parallel to generate faster more experience to learn from. This has led to the widespread use of increasingly large-scale distributed and parallel systems in RL training. This introduces numerous engineering and algorithmic challenges that can be fixed by these frameworks we are talking about.Open source to the rescueIn recent years, frameworks such as TensorFlow or PyTorch (we have spoken extensively about both in this blog) have arisen to help turn pattern recognition into a commodity, making deep learning easier to try and use for practitioners.A similar pattern is beginning to play out in the Reinforcement Learning arena. We are beginning to see the emergence of many open source libraries and tools to address this both by helping in creating new pieces (not writing from scratch), and above all, involving the combination of various prebuild algorithmic components. As a result, these Reinforcement Learning frameworks help engineers by creating higher-level abstractions of the core components of an RL algorithm. In summary, this makes code easier to develop, more comfortable to read, and improves efficiency.Next, I provide a list of the most popular RL frameworks available. I think the readers will benefit by using code from an already-established framework or library. At the time of writing this post, I could mention the most important ones (and I’m sure I’m leaving some of them out):Keras-RLRL CoachReAgentRay+RLlibDopamineTensorforceRLgraphGarageDeeRAcmeBaselinesPFRLStableBaselinesDeciding which one of the RL frameworks listed here to use, depends on your preferences and what you want to do with it exactly. The reader can follow the links for more information.We are using Acme for doing research at our research center. But to describe one of these environments so that the reader can see the possibilities they offer, I have personally opted for RLlib based in Ray for several reasons that I will explain below.RLlib: Scalable Reinforcement Learning using RayWe are using Acme for doing research at our research center. But to describe one of these environments so that the reader can see the possibilities they offer, I have opted for RLlib based in Ray for several reasons that I will explain below.Growth of computing requirementsDeep Reinforcement Learning algorithms involve a large number of simulations adding another multiplicative factor to the computational complexity of Deep Learning in itself. Mostly this is required by the algorithms we have not yet seen in this series, such as the distributed actor-critic methods or multi-agents methods, among others.But even finding the best model often requires hyperparameter tuning and searching among various hyperparameter settings; it can be costly. All this entails the need for high computing power provided by supercomputers based on distributed systems of heterogeneous servers (with multi-core CPUs and hardware accelerators as GPUs or TPUs).Two years ago, when I debuted as an author on Medium, I already explained what this type of infrastructure is like in the article “Supercomputing”. In Barcelona, we now have a supercomputer, named Marenostrum 4, which has a computing power of 13 Petaflops.Barcelona Supercomputing Center will host a new supercomputer next year, Marenostrum 5, which will multiply the computational power by a factor of x17.The current supercomputer MareNostrum 4 is divided into two differentiated hardware blocks: a block of general-purpose and a block-based on an IBM system designed especially for Deep Learning and Artificial Intelligence applications.In terms of hardware, this part of the Marenostrum consists of a 54 node cluster based on IBM Power 9 and NVIDIA V100 with Linux operating system and interconnected by an Infiniband network at 100 Gigabits per second. Each node is equipped with 2 IBM POWER9 processors with 20 physical cores each and 512GB of memory. Each of these POWER9 processors is connected to two NVIDIA V100 (Volta) GPUs with 16GB of memory, a total of 4 GPUs per node.How can this hardware fabric be managed efficiently?System Software StackAccelerating Reinforcement Learning with distributed and parallel systems introduce several challenges in managing the parallelization and distribution of the programs’ execution. To address this growing complexity, new layers of software have begun to be proposed that we stack on existing ones in an attempt to maintain logically separate the different components of the layered software stack of the systemBecause of this key abstraction, we can focus on different software components that today supercomputers incorporate in order to perform complex tasks. I like to mention that Daniel Hillis, who co-founded Thinking Machines Corporation, a company that developed the parallel Connection Machine, says that the hierarchical structure of abstraction is our most important tool in understanding complex systems because it lets us focus on a single aspect of a problem at a time.And this is the case of RLlib, the framework for which I opted, that follows this divide and conquer philosophy with a layered design of the software stack.Software stack of RLlib (source: docs.ray.io)This hierarchical structure of abstraction that allows this functional abstraction is fundamental because it will let us manipulate information without worrying about its underlying representation. Daniel Hillis says that once we figure out how to accomplish a given function, we can put the mechanism inside a ”black box” of a ”building block” and stop thinking about it. The function embodied by the building block can be used over and over, without reference to the details of what’s inside.RayIn short, parallel and distributed computing is a staple of Reinforce Learning algorithms. We need to leverage multiple cores and accelerators (on multiple machines) to speed up RL applications, and Python’s multiprocessing module is not the solution. Some of the RL frameworks, like Ray can handle this challenge excellently.On the official project page, Ray is defined as a fast and simple framework for building and running distributed applications:Providing simple primitives for building and running distributed applications.Enabling end-users to parallelize single machine code, with little to zero code changes.Including a large ecosystem of applications, libraries, and tools on top of the core Ray to enable complex applications.Ray Core provides simple primitives for application building. On top of Ray Core, beside RLlib, there are other libraries for solving problems in machine learning: Tune (Scalable Hyperparameter Tuning), RaySGD (Distributed Training Wrappers), and Ray Serve (Scalable and Programmable Serving).RLlibRLlib is an open-source library for reinforcement learning that offers both high scalability and a unified API for a variety of applications. RLlib natively supports TensorFlow, TensorFlow Eager, and PyTorch, but most of its internals are framework agnostic.At present, this library already has extensive documentation ( API documentation), offering a large number of built-in algorithms in addition to allowing the creation of custom algorithms.The key concepts in RLlib are Policies, Samples, and Trainers. In a nutshell, Policies are Python classes that define how an agent acts in an environment. All data interchange in RLlib is in the form of Sample batches that encode one or more fragments of a trajectory. Trainers are the boilerplate classes that put the above components together, managing algorithm configuration, optimizer, training metrics, the workflow of the execution parallel components, etc.Later in this series, when we have advanced more in distributed and multi-agent algorithms, we will present in more detail these key components of RLlib.TensorFlow or PyTorchIn a previous post, TensorFlow vs. PyTorch: The battle continues, I showed that the battle between deep learning heavyweights TensorFlow and PyTorch is fully underway. And in this regard, the option taken by RLlib, allowing users to seamlessly switch between TensorFlow and PyTorch for their reinforcement learning work, also seems very appropriate.To allow users to easily switch between TensorFlow and PyTorch as a backend in RLlib, RLlib includes the “framework” trainer config. For example, to switch to the PyTorch version of an algorithm, we can specify {""framework"":""torch""}. Internally, this tells RLlib to try to use the torch version of a policy for an algorithm (check out the examples of PPOTFPolicy vs. PPOTorchPolicy).Solving Cartpole Environment with RLlibNow, we will show a toy example to get you started and show you how to solve OpenAI Gym’s Cartpole Environment with PPO algorithm using RLlib. PPO is one of the proposals that solves the limitations of REINFORCE, introduced in the paper “Proximal Policy Optimization Algorithms” by John Schulman et al. (2017) at OpenAI. But the reader can use the code proposed in this section to test any of the algorithms already programmed in this framework.The entire code of this post can be found on GitHub and can be run as a Colab google notebook using this link.Warning: Given that we are executing our examples in Colab we need to restart the runtime after installing ray package and uninstall pyarrow.The various algorithms you can access are available through ray.rllib.agents. Here, you can find a long list of different implementations in both PyTorch and Tensorflow to begin playing with.If you want to use PPO you can run the following code:import rayfrom ray.rllib.agents.ppo import PPOTrainer, DEFAULT_CONFIGray.init()The ray.init() command starts all of the relevant Ray processes. This must be done before we instantiate any RL agents, for instance PPOTrainer object in our example:config = DEFAULT_CONFIG.copy()config[""num_gpus""] = 1 # in order to use the GPUagent = PPOTrainer(config, 'CartPole-v0')We can pass in a config object many hyperparameters that specify how the network and training procedure should be configured. Changing hyperparameters is as easy as passing them as a dictionary to the config argument. A quick way to see what’s available is to call trainer.config to print out the options that are available for your chosen algorithm:print(DEFAULT_CONFIG){‘num_workers’: 2, ‘num_envs_per_worker’: 1, ‘rollout_fragment_length’: 200, ‘num_gpus’: 0, ‘train_batch_size’: 4000, ...}Once we have specified our configuration, calling the train() method on our trainerobject will update and send the output to a new dictionary called results.result = agent.train()All the algorithms follow the same basic construction alternating from lower case abbreviation to uppercase abbreviation followed by Trainer . For instance, if you want to try a DQN instead, you can call:from ray.rllib.agents.dqn import DQNTrainer, DEFAULT_CONFIGagent = DQNTrainer(config=DEFAULT_CONFIG, env='CartPole-v0')The simplest way to programmatically compute actions from a trained agent is to use trainer.compute_action():action=agent.compute_action(state)This method preprocesses and filters the observation before passing it to the agent policy. Here is a simple example of how to watch the Agent that uses compute_action():def watch_agent(env):   state = env.reset()   rewards = []   img = plt.imshow(env.render(mode=’rgb_array’))   for t in range(2000):       action=agent.compute_action(state)       img.set_data(env.render(mode=’rgb_array’))       plt.axis(‘off’)       display.display(plt.gcf())       display.clear_output(wait=True)        state, reward, done, _ = env.step(action)       rewards.append(reward)       if done:          print(“Reward:”, sum([r for r in rewards]))          break       env.close()Using watch_agent function, we can compare the behavior of the Agent before and after being trained running multiple updates calling the train() method for a given number:for i in range(10):   result = agent.train()   print(f'Mean reward: {result[""episode_reward_mean""]:4.1f}')The last line of code shows how we can monitor the training loop printing information included in the return of the method train().Before trainingAfter trainingThis is a toy implementation of a simple algorithm to show this framework very briefly. The actual value of the RLlib framework lies in its use in large infrastructures executing inherently parallel and, at the same time, complex algorithms were writing the code from scratch is totally unfeasible.As I said, I opted for RLlib after taking a look at all the other frameworks mentioned above. The reasons are diverse; some are already presented in this post. Add that for me; it is relevant that it has already been included in major cloud providers such as AWS and AzureML. Or that there is a pushing company like ANYSCALE that has already raised 20 million and organizes the Ray Summit conference, which will be held online this week (September 30 through October 1) with great speakers (as our friend Oriol Vinyals ;-). Maybe add more context details, but for me, just as important as the above reasons is the fact that there are involved great researchers from the University of California at Berkeley, including the visionary Ion Stoica, whom I met about Spark, and they clearly got it right!Where to go from hereI hope these posts have served to encourage readers to get introduced to this exciting technology that is the real enabler of the latest disruptive advances in the field of Artificial Intelligence. We haven’t really seen much, just the bare minimum of basics that will allow you to continue on your own to discover and enjoy this fabulous world. The following references may be useful.BooksRichard S. Sutton and Andrew G. Barto. Reinforcement Learning: An Introduction, by MIT Press, 2018. [available from incompleteideas.net]. Related independent repo of Python code.Miguel Morales. Grokking Deep Reinforcement Learning. MANNING, 2020.Alexander Zai and Brandon Brown. Deep Reinforcement Learning in Action. MANNING, 2020.Maxim Lapan. Deep Reinforcement Learning Hands-on. Packt Publishing Ltd., 2nd edition, 2020.Phil Winder. Reinforcement Learning, Industrial Applications and Intelligent Agents. O’Really Media Inc., 2020.Micheal Lanham. Hands-on Reinforcement Learning for Games. MANNING, 2020.Online resourcesUCL Course on RL by David SilverUC Berkeley CS 285 by Sergey LevineDeepMind & UCL by Hado Van HasseltStanford CS234 by Emma BrunskillUPC Barcelona Tech by Mario MartinUniversity of Waterloo CS 885 by Pascal PoupartBerkeley DRL Bootcamp by Pieter Abbeel et allUPC Barcelona Tech by Xavier Giró-i-Nieto et allOpenAI by Josh AchiamLilian Weng BlogPapers with codeDeep Reinforcement Learning Explained Seriesby UPC Barcelona Tech and Barcelona Supercomputing CenterAn introductory series that gradually and with a practical approach introduces the reader to this exciting technology that is the real enabler of the latest disruptive advances in the field of Artificial Intelligence.Content of this seriesDeep Reinforcement Learning Explained - Jordi TORRES.AICONTENT OF THIS SERIESby UPC Barcelona Tech & Barcelona Supercomputing CenterAbout this seriesI started to write this series in May, during the period of lockdown in Barcelona. Honestly, writing these posts in my spare time helped me to #StayAtHome because of the lockdown. Thank you for reading this publication in those days; it justifies the effort I made.Our research in DRLOur research group at UPC Barcelona Tech and Barcelona Supercomputing Center is doing research on this topic. Our latest paper in this area is “Explore, Discover and Learn: Unsupervised Discovery of State-Covering Skills” presented in the 37th International Conference on Machine Learning (ICML2020). The paper presents a novel paradigm for unsupervised skill discovery in Reinforcement Learning. It is the last contribution of @vcampos7, one of our Ph.D. students, co-advised with@DocXavi. This paper is co-authored with @alexrtrott, @CaimingXiong, @RichardSocher from Salesforce Research.Written byJordi TORRES.AIProfessor at UPC Barcelona Tech & Barcelona Supercomputing Center. Research focuses on Supercomputing & Artificial Intelligence https://torres.ai @JordiTorresAIFollow134 Sign up for The Daily PickBy Towards Data ScienceHands-on real-world examples, research,  tutorials, and cutting-edge techniques delivered Monday to Thursday. Make learning your daily ritual. Take a lookGet this newsletterBy signing up, you will create a Medium account if you don’t already have one. Review our Privacy Policy for more information about our privacy practices.Check your inboxMedium sent you an email at  to complete your subscription.134 134 Reinforcement LearningArtificial IntelligenceDeep R L ExplainedTowards Data ScienceMachine LearningMore from Towards Data ScienceFollowA Medium publication sharing concepts, ideas, and codes.Read more from Towards Data ScienceMore From Medium5 YouTubers Data Scientists And ML Engineers Should Subscribe ToRichmond Alake in Towards Data Science7 Must-Haves in your Data Science CVElad Cohen in Towards Data Science21 amazing Youtube channels for you to learn AI, Machine Learning, and Data Science for freeJair Ribeiro in Towards Data ScienceThe Roadmap of Mathematics for Deep LearningTivadar Danka in Towards Data Science30 Examples to Master PandasSoner Yıldırım in Towards Data ScienceAn Ultimate Cheat Sheet for Data Visualization in PandasRashida Nasrin Sucky in Towards Data ScienceHow to Get Into Data Science Without a DegreeTerence S in Towards Data ScienceHow To Build Your Own Chatbot Using Deep LearningAmila Viraj in Towards Data ScienceAboutHelpLegalGet the Medium app"
Unsupervised Meta-Learning Is All You Need,https://medium.com/cracking-the-data-science-interview/unsupervised-meta-learning-is-all-you-need-71b6dfa29ccd?source=tag_archive---------3-----------------------,"Meta Learning,Unsupervised Learning,Clustering,Reinforcement Learning,Semi Supervised Learning","Update: This post is part of a blog series on Meta-Learning that I’m working on. Check out part 1, part 2, and part 3.IntroductionIn my previous posts, “Meta-Learning Is All You Need” and “Bayesian Meta-Learning Is All You Need,” we have only discussed meta-learning in the supervised setting, in which we have access to labeled data and hand-specified task distributions. However, acquiring labeled data for many tasks and manually constructing task distributions is challenging and time-consuming. Such dependencies put conceptual limits on the type of problems that can be solved through meta-learning.Can we design a meta-learning algorithm to handle unlabeled data, where the algorithm can come up with its own tasks that prepare for future downstream tasks?Unsupervised meta-learning algorithms effectively use unlabeled data to tune their learning procedures by proposing their task distributions. A robust unsupervised meta-learner, once trained, should be able to take new and different data from a task with labels, acquire task-specific knowledge from the training set, and generalize well on the test set during inference.This blog post is my attempt to explore the unsupervised lens of meta-learning and tackle the most prominent papers in this sub-domain.1 — CACTUsHsu, Levine, and Finn come up with Clustering to Automatically Construct Tasks for Unsupervised Meta-Learning (CACTUs) — an unsupervised meta-learning method that learns a learning procedure, without supervision, that is useful to solve a wide range of new human-specified tasks. With only raw unlabeled observations, the model can learn a reasonable prior such that, after meta-training, when presented with a small dataset for a human-specified task, the model can transfer its previous experience to learn to perform the new task efficiently.The diagram below illustrates CACTUs:Given raw unlabeled images, the algorithm first runs unsupervised learning on the images to get a low-dimensional embedding space.Second, the algorithm proposes tasks by clustering the embeddings multiple times within this low-dimensional latent space. Different data groupings will then be generated. To get a task, the algorithm samples these different groupings and treat each grouping as a separate class label.Third, the algorithm runs meta-learning methods (black-box, optimization-based, or non-parametric) on the tasks. The result of this process is going to be a representation that is suitable for learning downstream tasks.The CACTUs method, as proposed in “Unsupervised Learning via Meta-Learning.”Let’s unpack CACTUs further: The key question is how to construct classification tasks from unlabeled data D = {xᵢ} automatically.CACTUs use k-means clustering to group data points into consistent and distinct subsets based on salient features. If the clusters can recover a semblance of the true class-conditional generative distributions, creating tasks based on treating these clusters as classes should result in useful unsupervised meta-learning. However, the result of k-means is critically dependent on the metric space on which its objective is defined. Thus, CACTUs use SOTA unsupervised learning methods to produce useful embedding spaces. In particular, the authors try out four different embedding methods to generate tasks: ACAI, BiGAN, DeepCluster, and InfoGAN.Adversarially Constrained Autoencoder Interpolation (ACAI) is a convolutional autoencoder architecture whose loss is regularized with a term that encourages meaningful interpolations in the latent space.Bidirectional GAN (BiGAN) is a generative adversarial network where the discriminator produces synthetic images from real embedding and synthetic embedding from real image.DeepCluster is a clustering technique where: first, the features of a convolutional neural network are clustered. Second, the feature clusters are used as labels to optimize the network weights via backpropagation.InfoGAN is another generative adversarial network where the input to the generator is decomposed into a latent code incompressible noise.CACTUs run out-of-the-box unsupervised embedding learning algorithms on D, then map the data {xᵢ} into the embedding space Z, producing {zᵢ}. To build a diverse task set, CACTUs generates P partitions {Pₚ} by running clustering P times, applying random scaling to the dimensions of Z to induce a different metric, represented by a diagonal matrix A, for each run of clustering. With μ_c denoting the learned centroid of cluster C_c, a single run of clustering can be summarized with:Equation 1With the partitions being constructed over {zᵢ}, CACTUs finally meta-learn on the images, so that the learning procedure can adapt to each evaluation task from the rawest level of representation. For this meta-learning phase, the authors try out two different methods to learn the representation: MAML and ProtoNets.Model-Agnostic Meta-Learning (MAML) learns the meta-parameters of a neural network so that they can lead to useful generalization in a few gradient steps.Prototypical Networks (ProtoNets) learn a representation where each class can be effectively identified by its prototype — which is the mean of the class’ training examples in the meta-learned space.Here are the main benefits of CACTUs from the experimental setting conducted in the paper on MNIST, CelebA, Omniglot, and Mini-ImageNet datasets:There is a big advantage of meta-learning on tasks derived from embeddings, rather than using only the embeddings for downstream supervised learning of new tasks.CACTUs is sufficient for all four embedding learning methods that generate tasks.CACTU learns an effective prior to a variety of task types. This means that it is suitable for tasks with different supervision signals, or tasks that deal with features in different scales.However, the authors noted that with its evaluation-agnostic task generation, CACTUs trades off performance in specific use cases for broad applicability and the ability to train on unlabeled data. An exciting direction for future work is making CACTUs more robust towards highly unstructured and unlabeled datasets.2 — UMTRA (2018)Khodadadeh, Boloni, and Shah present Unsupervised Meta-Learning with Tasks constructed by Random Sampling and Augmentation (UMTRA), which performs meta-learning of one-shot and few-shot classifiers in an unsupervised manner on an unlabeled dataset. As seen in the diagram below:UMTRA starts with a collection of unlabeled data. The objects within this collection have to be drawn from the same distribution as the objects classified in the target task. Furthermore, the unlabeled data must have a set of classes significantly larger than the number of classes of the final classifier.Starting from this unlabeled dataset, UMTRA uses statistical diversity properties and domain-specific augmentation to generate the training and validation data for a collection of synthetic tasks.These tasks are then used in the meta-learning process based on a modified classification variant of the Model-Agnostic Meta-Learning (MAML) algorithm.The UMTRA method, as proposed in “Unsupervised Meta-Learning for Few-Shot Image Classification.”More formally speaking:In supervised meta-learning, we have access to a collection of tasks T₁, …, T, drawn from a specific distribution, with both supervised training and validation data. Each task T has N classes of K training/validation samples.In unsupervised meta-learning, we don’t have the collection of tasks T₁, …, T, and their associated labeled training data. Instead, we only have an unlabeled dataset U = { … xᵢ …}, with samples drawn from the same distribution as the target task. Every element of this dataset is associated with a natural class C₁ … Cc.To run the UMTRA algorithm, we need to create tasks Tᵢ from the unsupervised data that can serve the same role as the meta-learning tasks in the full MAML algorithm. For such a task, we need to create both the training data D and the validation data D’.The training data is Dᵢ = {(x₁, 1), …, (x_N, N)} with xᵢ sampled randomly from U.The validation data Dᵢ’ = {(x₁’, 1), …, (x_N’, N)} is created by augmenting the sampled used in the training data using an augmentation function xᵢ’ = A(xᵢ).Experiments on Omniglot and Mini-ImageNet datasets in the paper show that UMTRA outperforms learning-from-scratch approaches and approaches based on unsupervised representation learning. Furthermore, the statistical sampling and augmentation performed by UMTRA can be seen as a cheaper alternative to the dataset-wide clustering performed by CACTUs.3 — Unsupervised Meta-Reinforcement Learning (2018)Throughout this series, I haven’t brought up meta reinforcement learning yet, a family of algorithms that can learn to solve new reinforcement learning tasks more quickly through experience on past tasks. They assume the ability to sample from a pre-specified task distribution. They can solve new tasks drawn from this distribution very quickly. However, specifying a task distribution is tedious and requires a significant amount of supervision that may be difficult to provide for sizeable real-world problem settings. Can we automate the process of task design and remove the need for human supervision entirely?Gupta, Eysenbach, Finn, and Levine apply unsupervised meta-learning to the context of meta reinforcement learning: meta-learning from a task distribution that is acquired automatically, rather than requiring manual design of the meta-training tasks. Given an environment, they want to propose tasks in an unsupervised way and then run meta-reinforcement learning on those tasks. The result is a reinforcement learning algorithm that is tuned for the given environment. Then for that environment, given a reward function from the human, the algorithm can maximize the function with a small amount of experience.The learning procedure, as shown in “Unsupervised Meta-Reinforcement Learning.”Formally speaking, the meta-training setting is a controlled Markov process (CMP) — a Markov decision process without a reward function: C = (S, A, P, γ, p) — where S is the state space, A is the action space, P is the transition dynamics, γ is the discount factor, and p is the initial state distribution. The CMP produces a Markov decision process M = (S, A, P, γ, p, r) — where r is the reward function.f: D -> π is a learning algorithm that inputs a dataset of experience from the MDP (D) and outputs a policy π. This algorithm is evaluated over several episodes: wherein each episode i, f observes all previous data {T₁, …, Tᵢ₋₁} and outputs a policy to be used in iteration i.Equation 2The goal of unsupervised meta-reinforcement learning is to take this CMP and produce an environment-specific learning algorithm f that can quickly learn an optimal policy πᵣ* for any reward function r.The key question is how to propose relevant tasks. The paper attempts to make a set of tasks that are more diverse from each other, where the skills within tasks are entirely different from one another. To cluster skills into discrete parts of the policy space, the authors use Diversity Is All You Need, which is a method that learns skills by maximizing an information-theoretic objective using a maximum entropy policy:The Pseudocode in “Unsupervised Meta-Reinforcement Learning”The agent/policy takes as input a discrete skill (z) to produce actions. This skill z is used to generate the states in a given rollout according to a latent-conditioned policy π(a | s, z).The discriminator network D_{θ} takes as input a state (s) and predicts which skill (z) to be passed into the policy. Diversity Is All You Need enforces a co-operative game where the policy visits which states are discriminable by the discriminator and the discriminator predicts the skill from the state. The objective of both is to maximize the accuracy of the discriminator.Using the discriminator, the authors make the reward function for unsupervised meta-learning to simply be the likelihood of one of the skills given the state: r_z(s, a) = log (D_{θ} (z|s)).Then, the authors use MAML with this reward r_z to acquire a fast learning algorithm f to learn new tasks quickly for the current reinforcement learning setting.In their experiments, the author study three simulated environments of varying difficulty: 2D point navigation, 2D locomotion, and 3D locomotion. The results indicate that unsupervised meta-reinforcement learning effectively acquires accelerated reinforcement learning procedures without manual task design. These procedures exceed the performance of learning from scratch. However, one limitation is that the paper only considers deterministic dynamics and only considers task distributions with optimal posterior sampling. Thus, two exciting directions for future work are experimenting with stochastic dynamics and more realistic task distributions in large-scale datasets and complex tasks.4 — Assume, Augment, and Learn (2019)Antoniou and Storkey propose Assume, Augment, and Learn (AAL) that leverages unsupervised data to generate tasks for few-shot learners. This method is inspired by the ability of humans to find features that can accurately describe a set of randomly clustered data points, even when the clusters are continuously randomly reset. The authors believe that bringing this setting to meta-learning can produce strong representations for the task at hand.The paper uses three separate datasets: a meta-training, a meta-validation, and a meta-test set.The meta-training set does not have any labels and is used to train a few-shot learner using AAL.The meta-validation and meta-test sets have labeled data and are used to evaluate the few-shot tasks. Using a validation set to pick the best-trained model and a test set to produce the final test errors removes any potential unintended over-fitting.Training meta-learning models requires using a large number of tasks. In particular, AAL relies on the set-to-set few-shot learning scheme in Vinyals et al. — where a task is composed of a support (training) set and a target (validation) set. Both sets have a different number of classes but a similar number of samples per class.Put that into the context of a meta-learning setting: Given a task, AAL learns a model that can acquire task-specific knowledge from the support set to perform well in the target set, before throwing away that knowledge.The proposed method in “Assume, Augment and Learn: Unsupervised Few-Shot Meta-Learning via Random Labels and Data Augmentation.”AAL attempts to explore the semantic similarities between data points to learn robust across-task representations that can then be used in a setting where supervised labels are available. It consists of three phases:First, AAL assumes data labels by randomly assigning labels for a randomly sampled set of data points. This allows the model to acquire fast-knowledge in its parameters that can classify the support set well.Second, AAL augments the number of data points in the support set via data augmentation techniques. This sets the number of classes in the support set to match the number of classes in the target set. Still, the samples in the two sets are different enough to allow the target set to serve as a satisfactory evaluation set.Third, AAL learns the tasks via any existing few-shot meta-learning technique, as long as the method can be trained using the set-to-set few-shot learning framework. In the paper, the authors experiment with Model Agnostic Meta-Learning and Prototypical Networks.Experiments on Omniglot and Mini-ImageNet datasets in the paper verify that AAL generalizes well to real-labeled inference tasks. However, its performance heavily depends on the data augmentation strategies employed. Thus, a future direction would be to automate the search for an optimal data augmentation function to produce fully unsupervised systems with a strong performance.The papers discussed above all have a common attribute: the meta-objective of the outer loop is unsupervised, and therefore the learner itself is learned without any labels available. As I researched the meta-learning literature, I found another variant of meta-learning that involves unsupervised learning.In this second variant, meta-learning is used as a means to learn an unsupervised inner loop task. The outer objective in this case can be anything from supervised, unsupervised, or reinforcement-based. This variant can be referred to as Meta-Learning Unsupervised Learning.5 — Unsupervised Update Rules (2018)Metz, Maheswaranathan, Cheung, and Sohl-Dickstein present the first meta-learning approach that tackles unsupervised representation learning, where the inner loop consists of unsupervised learning. The paper proposes to meta-learn an unsupervised update rule by meta-training on a meta-objective that directly optimizes the utility of the unsupervised representation. Unlike hand-designed unsupervised learning rules, this meta-objective directly targets the usefulness of a representation generated from unlabeled data for later supervised tasks. This approach contrasts with transfer learning, where a neural network is instead trained on a similar dataset, and then fine-tuned or post-processed on the target dataset.Furthermore, this is the first representation meta-learning approach to generalize across input data modalities and datasets, the first to generalize across permutation of the input dimensions, and the first to generalize across neural network architectures.The diagram below provides a schematic illustration of the model:The left-hand side shows how to meta-learn an unsupervised learning algorithm. The inner loop computation consists of iteratively applying the UnsupervisedUpdate to a base model. During meta-training, the UnsupervisedUpdate (parametrized by θ) is itself updated by gradient descent on the MetaObjective.The right-hand side goes deeper into the based model and UnsupervisedUpdate. Unlabeled input data (x) is passed through the base model, which is parameterized by W and colored green. The goal of the UnsupervisedUpdate is to modify W to achieve a top layer representation x^L, which performs well at few-shot learning. To train the base model, information is propagated backward by the UnsupervisedUpdate analogous to back-prop (colored blue).The approach described in “Meta-Learning Update Rules for Unsupervised Representation Learning.”Let’s take a look at the design of the model deeper.The base model is a standard fully-connected Multi-Layer Perceptron coupled with batch normalization layers and the ReLU activation unit.The learned update rule is unique to each neuron layer so that the weight updates are a function of pre- and post-synaptic neurons in the base model and can be defined for any base model architecture. This design enables the update rule to generalize across architectures with different widths, depths, and network topologies.The meta-objective determines the quality of the unsupervised representations. It is based on fitting a linear regression to labeled examples with a small number of data points.Based on the experiments on various datasets such as CIFAR 10, MNIST, Fashion MNIST, and IMDB, the performance of this method either matched or exceeded existing unsupervised learning on few-shot image classification and text classification tasks. With no explicitly defined objective, this work is a proof of an algorithm design principle that replaces manual fine-tuning with architectures designed for learning and learned from data via meta-learning.6 — Meta-Learning For Semi-Supervised Learning (2018)Ren, Triantafillou, Ravi, Snell, Swersky, Tenenbaum, Larochelle, and Zemel aim to generalize a few-shot learning setup for semi-supervised classification in two ways:They consider a scenario where the new classes are learned in the presence of additional unlabeled data.They also consider the situation where the new classes to be learned are not viewed in isolation. Instead, many of the unlabeled examples are from different classes; the presence of such distractor classes introduces an additional and more realistic level of difficulty to the few-shot problem.The figure below shows a visualization of training and test episodes in the semi-supervised setting:The training set is a tuple of labeled and unlabeled examples: (S, R).The labeled examples are called the support set S that contains a list of tuples of inputs and targets.The unlabeled examples are called the unlabeled set R that contains only the inputs: R = {x₁, x₂, …, x_m}.The models are trained to perform well when predicting the labels for the examples in the episode’s query set Q.Example of the semi-supervised few-shot learning setup as described in “Meta-Learning for Semi-Supervised Few-Shot Classification.”This paper proposes three novel extensions of Prototypical Networks, a SOTA approach to few-shot learning, to the semi-supervised setting. More precisely, Prototypical Nets learn an embedding function h(x), parameterized as a neural network, that maps examples into space where examples from the same class are close and those from different classes are far.To compute the prototype p_c of each class c, Prototypical Nets average the embedded examples per-class:Equation 3These prototypes define a predictor for the class of any query example x*, which assigns a probability over any class c based on the distance between x* and each prototype, as follows:Equation 4The loss function used to update Prototypical Networks for a given training episode is simply the average negative log-probability of the correct class assignments, for all query examples:Equation 5Training includes minimizing the average loss, iterating over training episodes, and performing a gradient descent update.In the original formulation, Prototypical Networks (seen in the left side of the figure below) do not specify a way to leverage the unlabeled set R. The extensions start from the basic definition of prototypes and provide a procedure to produce refined prototypes (seen in the right side of the figure below) using the unlabeled examples in R.After the refined prototypes are generated, each query example is classified into one of the N classes based on the proximity of its embedded position with corresponding refined prototypes. During training, the authors optimize the average negative log-probability of the correct classification.The prototypical networks before and after refinement, as displayed in “Meta-Learning for Semi-Supervised Few-Shot Classification.”The first extension was borrowed from the inference performed by soft k-means.The regular Prototypical Network’s prototypes p_c are used as the cluster locations.Then, the unlabeled examples get a partial assignment to each cluster based on their Euclidean distance to the cluster locations.Finally, refined prototypes are obtained by incorporating these unlabeled examples.Equation 6The partial assignment is defined as follows:Equation 7The extension above assumes that each unlabeled example belongs to either one of the N classes in the episode. However, it would be more general not to make that assumption and have a model robust to the existence of examples from other classes, termed distractor classes.The second extension added cluster to the assignment, whose purpose is to capture the distractors, thus preventing them from polluting the clusters of the classes of interest:Equation 9The third extension incorporated a soft-masking mechanism on the contributions of unlabeled examples. The idea is to make the unlabeled examples that are closer to a prototype to be masked less than those that are farther:First, normalized distances d_{j,c} are computed between examples xⱼ and prototypes p_c.Then, soft thresholds β_c and slopes γ_c are predicted for each prototype by feeding a small neural network various statistics of the normalized distances for the prototype.Finally, soft masks m_{j, c} for the contribution of each example to each prototype are computed by comparing to the threshold the normalized distances.Equation 9The refined prototypes are obtained as follows:Equation 10From the experiments conducted on Omniglot, miniImageNet, and tieredImageNet, these extensions of Prototypical Networks showed consistent improvements under semi-supervised settings compared to their baselines. For future work, the authors want to incorporate fast weights into their framework so that examples can have different embedding representations given the contents in the episode.7 — Meta-Clustering (2019)Jiang and Verma propose a simple yet highly effective meta-learning model to solve for clustering tasks. The model, called Meta-Clustering, finds the cluster structure directly without having to choose a specific cluster loss for each new clustering problem. There are two key challenges in training such a model for clustering:Since clustering is fundamentally an unsupervised task, true cluster identities for each training task don’t exist.The cluster label for each new data point depends upon the labels assigned to other data points in the same clustering task.To address these issues, the authors:Train their algorithm on simple synthetically generated datasets or other real-world labeled datasets with similar characteristics to generalize to real previously unseen datasets.Use a recurrent network (LSTMs) and train it sequentially to assign clustering labels effectively based on previously seen data points.The problem is formally defined as follows: A meta-clustering model M maps data points to cluster labels. The model is trained to adapt to a set of clustering tasks {Tᵢ}. At the end of meta-training, M would produce clustering labels for new test tasks T_test.Each training task Tᵢ consists of a set of data points X_i and their associated cluster labels L_i.X_i and L_i are partitioned into subsets based on cluster identities.The structure of the test task T_test is different from training and consists of only a set of data points X_test.LSTM Architecture of Meta-Clustering as proposed in “Meta-Learning to Cluster.”As seen above, Meta-Clustering uses a Long-Short Term Memory (LSTM) network to capture long-range dependencies between cluster identity for a current data point and the identities assigned to its neighbors.At each time step t, the LSTM module takes in a data point x and a score vector a_{t-1} from previous time step t — 1 and outputs a new score a_t for the current time step. The score vector encodes the quality of the predicted label assigned to the data point x.The network includes 4 LSTMs layers stacked on top of each other. The first three layers all have 64 units with residual connections, while the last layer can have the number of hidden units as either the number of clusters or the max number of possible clusters.Meta-Clustering optimizes for a loss function that combines classification loss (L_classify) and local loss (L_local):Equation 11Φ refers to the architecture’s parameters, and λ refers to a hyper-parameter that controls the trade-off between the two losses.The Meta-Clustering training algorithm in “Meta-Learning to Cluster.”During each iteration in training, Meta-Clustering samples a batch of training data from the given pool of training tasks and feeds them into the LSTM network sequentially. The LSTM cell states are kept across epochs, enabling the LSTM network to remember the previously seen data points.The Meta-Clustering inference algorithm in “Meta-Learning to Cluster.”During testing, the LSTM network takes into each test task as inputs and returns the clustering as outputs. The data points in each dataset are shuffle across iterations to prevent potential prediction errors introduced by specific sequence orders.From experiments on various synthetic and real-world data, Meta-Clustering achieves better clustering results than by using prevalent pre-existing linear and non-linear benchmark cluster losses. Additionally, Meta-Clustering can transfer its clustering ability to unseen datasets when trained on labeled real datasets of different distributions. Finally, Meta-Clustering is capable of approximating the right number of clusters in simple tasks and reducing the need to pre-specify the number of clusters.8 — Self-Critique and Adapt (2020)Antoniou and Storkey (the same authors of AAL) came up with Self-Critique and Adapt (SCA for short) that frames the problem of learning a loss-function using the set-to-set few-shot learning framework.SCA enables meta-learning-based few-shot systems to learn not only from the support-set input-output pairs but also from the target-set inputs, by learning a label-free loss function, parameterized as a neural network.Doing so grants the models the ability to learn from the target-set input data points, by merely computing a loss, conditioned on base-model predictions of the target set.The label-free loss can be used to compute gradients for the model, and the gradients can then be used to update the base-model at inference time, to improve generalization performance.Furthermore, SCA is model-agnostic and can be applied on top of any end-to-end differentiable, gradient-based, meta-learning method that uses the inner-loop optimization process to acquire task-specific information.A unique proposition of SCA is that it follows a transductive learning approach, which benefits from unsupervised information from the test example points and specification by knowing where we need to focus on model capability.The Self-Critique and Adapt architecture presented in “Learning to Learn via Self-Critique.”As displayed in the figure above:SCA takes a base-model, updates it for the support-set with an existing gradient-based meta-learning method, and then infers predictions for the target-set.Once the predictions have been inferred, they are concatenated along with other based-model related information and are then passed to a learnable critic loss network. This critic network computes and returns a loss for the target-set.The base-model is then updated with SGD for this critic loss.This inner-loop optimization produces a predictive model specific to the support and target-set information. The quality of the inner loop learned predictive model is evaluated using ground truth labels from the training tasks. The outer loop then optimizes the initial parameters and the critic loss to maximize the quality of the inner loop predictions.The Self-Critique and Adapt algorithm presented in “Learning to Learn via Self-Critique.”The SCA algorithm is demonstrated to the left in the paper. The base model of choice is MAML++, which is parametrized as f(θ). The critic loss network is parameterized as C(W). The goal is to learn acceptable parameters θ and W such that f can achieve good generalization performance on the target set T after being optimized for the loss on the support set S.From experiments on the miniImageNet and Caltech-UCSD Birds 200 datasets, the authors found that the critic network can improve well-established gradient-based meta-learning baselines. Some of the most useful conditional information for the critic model were the base model’s predictions, a relational task embedding, and a relational support-target-set network.ConclusionIn this post, I have discussed the motivation for unsupervised meta-learning and the six papers that incorporate this learning paradigm into their meta-learning workflow. In particular, these papers can be classified into two camps:CACTUs, UMTRA, AAL, and Unsupervised Meta-RL belong to the broad Unsupervised Meta-Learning camp. This camp aims to relax the conventional assumption of an annotated set of source tasks for meta-training, while still producing a good downstream performance of supervised few-shot learning. Typically, these synthetic source tasks are constructed without supervision via clustering (CACTUs) or class-preserving data augmentation (UMTRA and AAL).Unsupervised Update Rules, Meta-Semi-Supervised Learning, Meta-Clustering, and SCA belong to the Meta-Learning Unsupervised Learning camp. This camp aims to use meta-learning to train unsupervised learning algorithms (Unsupervised Update Rules and Meta-Clustering) or loss functions (SCA) that work well for downstream supervised learning tasks. This helps deal with the ill-defined-ness of the unsupervised learning problem by transforming it into a problem with a clear meta supervised objective.Stay tuned for part 4 of this series, where I’ll cover Active Learning!If you would like to follow my work on Recommendation Systems, Deep Learning, MLOps, and Data Journalism, you can follow my Medium and GitHub, as well as other projects at https://jameskle.com/. You can also tweet at me on Twitter, email me directly, or find me on LinkedIn. Or join my mailing list to receive my latest thoughts right at your inbox!Written byJames LeBlue Ocean Thinker | https://jameskle.com/ | @le_james94Follow61 61 61 Meta LearningUnsupervised LearningClusteringReinforcement LearningSemi Supervised LearningMore from Cracking The Data Science InterviewFollowTechnical Concepts + Industry Advice In The Data WorldRead more from Cracking The Data Science InterviewMore From MediumRandom Forest and Its ImplementationAfroz Chakure in The StartupComparing RCNN and Conventional CNNKeerti kulkarni in Analytics VidhyaUnderstanding Focal Loss for Pixel-level Classification in Convolutional Neural NetworksShuchen Du in The StartupThe Trade-Offs of Large-Scale Machine LearningOlivier Koch in Criteo R&D BlogA Primer on Multi-task Learning — Part 1Neeraj varshney in Analytics VidhyaFlip Algorithm for Segment Triangulations and Voronoi DiagramNabil MADALI in The StartupEM Algorithm — A simple ExplanationDiego CardonaAre Mushroom Edible or Poisonous ?Haroon KhanAboutHelpLegalGet the Medium app"
Best practices for Reinforcement Learning,https://towardsdatascience.com/best-practices-for-reinforcement-learning-1cf8c2d77b66?source=tag_archive---------4-----------------------,"Reinforcement Learning,Best Practices,Atari,Computer Vision,Machine Learning","Machine learning is research intensive. It contains significantly higher degrees of uncertainty compared to classic programming. This has a significant impact on product management and product development.Image via Shutterstock under license to Nicolas Maquaire.Developing an intelligent product with good performance is very difficult. In addition, the production environment can cost a lot. This combination of challenges can make the business model of many startups risky.In my last article, I described challenges newcomers face when using artificial intelligence in fog computing. More specifically, I detailed what it takes to make an inference on the edge.In this article, I’ll describe what I believe are some best practices to start a Reinforcement Learning (RL) project. I’ll do this by illustrating some lessons I learned when I replicated Deepmind’s performance on video games. This was a fun side-project I worked on.Google achieved super human performance on 42 Atari games with the same network (see Human-level control through deep reinforcement learning). So then, let’s see if we can achieve the same results and find out what best practices are needed to be successful!You can find the source code in the following Github repository; Additionally, for readers who want to learn how my algorithm works, I published Breakout explained and e-greedy and softmax explained. These are two Google Colab notebooks where I explain expected sarsa and the implementation of the two policies, e-greedy and softmax.Time and cardinality cursesThe main concerns each RL practitioner deals with are uncertainty coupled with unlimited technical options, and very long training times.I call these the time and cardinality curses of RL. I believe the best practices for every person or every team starting a reinforcement learning project are:Build a working prototype even if it has poor performance or it’s a simpler problemTry to reduce the training time and memory requirements as much as possibleImprove accuracy by testing different network configurations or technical optionsCheck, check again, and then check again every line of your codeTo these best practices, I would add:Monitor reliability. Sometimes, luck is not repeatableParallelism is your friend. Test different ideas in parallelLet’s start by tackling a very simple textbook case: Open AI GYM Acrobot. Then, we’ll move to some more challenging games: Breakout and Space Invaders.If you’re interested in building knowledge before continuing, I recommend reading Reinforcement Learning by Richard S. Sutton and Andrew G. Barto.If you’re actively participating in a project, I recommend reading Andrew NG’s “Machine Learning Yearning.”Learning with Open AI AcrobotImage by AuthorBefore tackling complex projects in RL, my recommendation is to start with a simple one because you will find more literature on the internet. It will be easier to find solutions and, more importantly, because it’s faster to test new ideas (fail fast, fail good).Open AI offers a wealth of options. Because of my specialization in control systems, I decided to use the Acrobot. I worked on a very similar project while working on my engineering degree: The double pendulum.For this side project, I decided to start with Acrobot.As illustrated, the acrobot system has two joints and two links, where only the joint between the two links is actuated. Initially, the links are hanging downwards, and the goal is to swing the end of the lower link above the horizontal line.Anatomy of the agentWhat is Reinforcement learning?Well, it’s a direct implementation of the idea that we learn by interacting with our environment. In that way, it mimics the brain.Image by AuthorIn RL, at each step of the game, we decide the best action and then we retrieve the reward and move into the next state. For the Acrobot, the state consists of the sin() and cos() of the two rotational joint angles and the joint angular velocities.[cos(theta1) sin(theta1) cos(theta2) sin(theta2) thetaDot1 thetaDot2]In the book Reinforcement Learning, Sutton and Barto describe different Temporal Difference (TD) techniques. TD learning refers to a class of model-free reinforcement learning where a deep network is used to approximate the value function. The value function estimates how good each action is.I started with the most common RL algorithms, Expected Sarsa, and its special case, Q-Learning. Both algorithms use a policy. Roughly speaking, a policy is the agent behavior function. The policy uses the value function estimate to decide the best action. We will use soft policies (ɛ-greedy and softmax), meaning that each action has a chance of being executed. The policy is ɛ-greedy when the best action is selected with a probability of 1-ɛ and randomly chosen with a probability of ɛ. And my favorite, the softmax policy assigns a preference to each action according to its action-value estimate.In Human-level control through deep reinforcement learning, Deepmind uses Q-Learning with an e-greedy policy.Each of the two algorithms we are going to use comes with a few hyperparameters and many options. To name a few, the most important ones are the learning (𝜶) and discounting rates (𝞬), the batch sizes, epsilon (ɛ) or tau (𝝉) for finding the right balance between exploration and exploitation, the size of the experience replay memory, the number of exploration steps, the number of annealing steps, the model update frequency, the weights initialization, and the optimizer.Of course, the list of hyperparameters goes on and on.This is what I mean by the curse of cardinality.The curse of cardinality. Image via Shutterstock under license to Nicolas Maquaire.This is why my first best practice is to build a “working” prototype. And then focus on performance.So then. How can we lift the curse of cardinality?First, I recommend searching the web for similar implementations so as to understand the hyperparameters used by other practitioners. Beyond helping you reach success, this will help develop your intuition, which is extremely important. As a matter of fact, ‘intuition’ is one of the words Andrew Ng uses the most in his fantastic course called Deep Learning Specialization.On my first attempt to rock the Acrobot, a few of my runs converged. Below, we can see the success rate over ten games. Of course, there is definitely room for improvement but my algorithm is learning and that’s a good start.Horizontal axis unit is hours of training. Vertical axis unit is the success rate over ten games. Graph by author.On the graph you can also see that I trained the networks for more than a week. The networks started to converge around the second day. This teaches us a good lesson: As AI practitioners, we wait and wait and wait. This is the curse of time!Looking back now, if I tally the number of hours I’ve spent in front of my machine scrutinizing the loss and accuracy of my many attempts, it’s certainly equivalent to receiving a degree in RL psychology! I am very proud that I became an expert in the psychology of RL’s algorithms.Some of the AI I trained perform like champs (The Good). A few are suicidal and perform less than pure randomness (the Bad). Others have regular burnout (and the Ugly).The Good (blue), the Bad (green) and the Ugly (red). Graph by author.Developing your intuition helps you find solutions to your problems. Here, the bad had a problem with the best action selection and the ugly had an issue with its target network.Like the bias and variance methodology in deep learning, you can develop your own intuition to diagnose your problems quicker.Now that we have a champ, let’s see how we can improve its performance.Training the agentGPU vs CPUOne of the main differences between Deep Learning and Reinforcement Learning is that there is no pre-existing dataset. The dataset (or experience replay, or memory in RL) is created when the agent interacts with the environment. This induces a performance bottleneck as the pipeline depends on CPU operations. This is why the tensorflow operations of most tutorials happen on the CPU. I am pretty sure that your first attempts will show better performances when placing all tensorflow operations on the CPU.Training on the GPU is absolutely not automagical. But when done correctly, it improves speed as you can see from the graph below.Horizontal axis in hours of training. Red with most of the operations on GPU ; Blue with operations on CPU only. Graph by author.The performance bottleneck is created by moving zillions of small objects back and forth between the CPU’s memory and the GPU’s memory.Therefore, it’s important to understand where you create your tensorflow variables and how you leverage the eager execution of tensorflow 2.x.On my machine, the limiting factor is often the CPU’s memory, which stores all the images of the experience buffers. I usually don’t see high GPU memory consumption because the data is created and managed on the CPU’s memory. I use the tensorflow data API to create a dataset. The dataset loads the images and feeds them to the GPU. I also don't see a high utilization rate of the GPU because each process is waiting on both the CPU to play the next step and on the dataset to yield a mini-batch.Additionally, the data type used plays an important performance role. Using int16 instead of float32 will increase the speed and facilitate the management of multiple 1.000.000 experience replays on your machine.Being careful with your data type helps you to train multiple networks in parallel and go faster, which further lifts the curse of time.These steps clearly support our second best practice: Trying to reduce the training time and memory requirements as much as possible.Hyperparameters and network architectureNow that we’ve made the algorithm as efficient as possible with regard to speed and memory consumption, we can focus on lifting the curse of cardinality. Of course, we still have a lot of pieces to fit together, but it’s more manageable. We can now launch multiple runs in parallel and get results faster.I recommend setting all your hyperparameters to the community’s commonly accepted values. The best way to find these values is to find papers that dig into similar use-cases and see what parameters they are using. Then, I recommend doing a manual search on 2 or 3 of the most important hyperparameters. Don’t forget to use the powers of 10 to effectively swipe the entire scope of your hyperparameters. Some are really sensitive (particularly the softmax temperature).For network architecture, I recommend replicating the network architecture from any of the interesting papers you find. Then, try a different number of hidden layers or a different number of nodes. Also, keep in mind that the initialization kernel of your layers is extremely important.For example, I noticed that Expected Sarsa with a softmax policy is not converging when the last dense layers use variance scaling instead of the default Glorot Initialization.To illustrate the improvements you can get from a different number of nodes, we added a new run to the previous comparison between the CPU/GPU. The only difference between the pink and the red plots is that I switched the two last layers (from 256 and 128 perceptrons to 128 and 256).Horizontal axis in hours of training. Red with the new layer arrangement. Graph by author.If you use a classic network architecture and a stack of 4 states as input, hundreds of runs make me think that the most important hyperparameters are:The learning rateThe exploration and annealing parameters (or temperature for softmax)The initialization of your layersThe frequency of the main network parameters updateThe frequency of the target network updatesThe improvements we obtained by following our best practices are pretty good. As you see below, we significantly improved the accuracy and the compute time.Improvements obtained with GPU and a little network tuning. Graph by author.Needless to say, this can have a very important impact on the go-to-market and related costs of a new product. This clearly supports our third best practice: Improve accuracy by testing different network configurations or technical options.As a side note to readers who are interested in further improving the accuracy and convergence time of a similar use-case: my next step would be to use tile coding. I’m pretty sure this will further improve both speed and performance.Thanks to our work on the Acrobot, we now have a good platform to try something more challenging: Reinforcement learning for computer vision.RL and computer vision with AtariBreakout and Space InvadersImage by AuthorBreakout was an arcade game developed and published by Atari on May 13, 1976. To play: a layer of bricks lines the top third of the screen and the goal is to destroy them all!Image by AuthorSpace Invaders was a 1978 arcade game created by Tomohiro Nishikado. Space Invaders was the first fixed shooter game and it set the template for the shoot ’em up genre. The goal is to defeat wave after wave of descending aliens with a horizontally moving laser to earn as many points as possible.Let’s see how our algorithm performs on those games! As a sneak-peek, the two animated gifs were captured during our eval sessions.There is not much to do to support these new games. You need to declare the new environment, but most importantly, you need to adapt your network and the data pipeline. For the new network, we will use a classic ConvNet architecture, the same network as the one Google Deepmind used in the Nature paper: Three convolution layers followed by a few dense layers. Then, we need to update the data pipeline. This is a bit more tricky as we have to store a million images.Most of the tutorials you can find on the internet store a history of states. A history of states stacks five consecutive images generated by the environment. Four images for estimating the current action-value function and the next four for estimating the next action-value. Considering the impact that storing histories has on memory consumption and our second best practice, we’ll only store the states (the images). The data pipeline will re-stack the states on the fly.You can find a detailed explanation of what I did on this Google Collab.As a side note, if like me, you use Open AI Gym to test different algorithms or technical options and want to transfer from Open AI Gym environments to real world problems, be aware that it is crucial to ensure your training is done in a stochastic environment. Many tutorials you can find on the internet use determinist environments. Stochasticity is a crucial requirement for robustness. It is very unlikely your algorithm will work on real world problems if you train and validate in an Open AI Gym deterministic environment. I cover this topic in “Are the space invaders deterministic or stochastic?”Unfortunately, during this update, I made many mistakes and spent an awful amount of time removing these bugs. Why? Because even with a few mistakes, the network was learning and converging. The accuracy was obviously far off, but it was working.This is a crucial difference with classic programming.When the network is learning and plateauing at a modest score, it’s easy to jump to the conclusion that the hyperparameters need to be adjusted. I fell into this trap quite a few times. Often, you remove a bug from the data pipeline and launch a series of runs only to notice a couple of days later that you have another bug. And this is not limited to the data pipeline. I admit I found bugs in each and every part of my code.This supports our fourth best practice: Check, check again, and then re-check every line of your code.In my experience, the best way to handle this is to create a separate notebook to prove that each line of code is working. It’s easy to inadvertently use matrix multiplication instead of element wise multiplication and it’s easy to make mistakes with Tensorflow or Numpy casting. I encourage you to be very cautious with your Tensorflow code.ConclusionWhile we are just scratching the surface of the technical challenges you will have building an intelligent product, I hope this article gave you a good understanding of some best practices to follow to successfully jumpstart your RL project.As for any research intensive projects, the time and cardinality curses should always be factored into your product management and team organization.I am a strong believer that machine learning can truly provide the thrust we need to solve many of our rising concerns.I am happy to help anybody with a solid vision! And, I am very open to feedback.Thank you for reading this article!Written byNicolas MaquaireFounding Partner at Model.fitFollow123 2 Sign up for The Daily PickBy Towards Data ScienceHands-on real-world examples, research,  tutorials, and cutting-edge techniques delivered Monday to Thursday. Make learning your daily ritual. Take a lookGet this newsletterBy signing up, you will create a Medium account if you don’t already have one. Review our Privacy Policy for more information about our privacy practices.Check your inboxMedium sent you an email at  to complete your subscription.123 123 2 Reinforcement LearningBest PracticesAtariComputer VisionMachine LearningMore from Towards Data ScienceFollowA Medium publication sharing concepts, ideas, and codes.Read more from Towards Data ScienceMore From Medium5 YouTubers Data Scientists And ML Engineers Should Subscribe ToRichmond Alake in Towards Data Science7 Must-Haves in your Data Science CVElad Cohen in Towards Data Science21 amazing Youtube channels for you to learn AI, Machine Learning, and Data Science for freeJair Ribeiro in Towards Data Science30 Examples to Master PandasSoner Yıldırım in Towards Data ScienceThe Roadmap of Mathematics for Deep LearningTivadar Danka in Towards Data ScienceAn Ultimate Cheat Sheet for Data Visualization in PandasRashida Nasrin Sucky in Towards Data ScienceHow to Get Into Data Science Without a DegreeTerence S in Towards Data Science4 Types of Projects You Must Have in Your Data Science PortfolioSara A. Metwalli in Towards Data ScienceAboutHelpLegalGet the Medium app"
Playing Optimal Heads-Up Poker with Reinforcement Learning,https://towardsdatascience.com/playing-optimal-heads-up-poker-with-reinforcement-learning-5fefe6261889?source=tag_archive---------5-----------------------,"Reinforcement Learning,Ppo,Poker,Policy Gradient","Photo under CCO from UnsplashReinforcement learning has been at the center of many AI breakthroughs in recent years. The opportunity for algorithms to learn without the onerous constraints of data collection presents huge opportunities for key advancements. Google’s DeepMind has been at the center of Reinforcement Learning, featuring breakthroughs with projects that garnered national attention like AlphaZero, a self-trained competitive agent that became the best Go player in the world in a span of 4 days.¹Traditional reinforcement learning algorithms such as Q-learning, SARSA, etc. work well in contained single agent environments, where they are able to continually explore until they find an optimal strategy. However, a key assumption of these algorithms is a stationary environment, meaning the transition probabilities and other factors remain unchanged episode to episode. When agents are trained against each other, such as in the case of poker, this assumption is impossible as both agents strategies are continually evolving leading to a dynamic environment. Furthermore, the algorithms above are deterministic in nature meaning that one action will always be considered optimal as compared to another action given a state.Deterministic policies, however, do not hold for everyday life or poker. For example, when given an opportunity in poker, a player can bluff, meaning they represent better cards than they actually have by putting in an oversized bet meant to scare the other players into folding. However, if a player bluffs every time the opponents would recognize such a strategy and easily bankrupt the player. This leads to another class of algorithms called policy gradient algorithms which output a stochastic optimal policy that can then be sampled from.Still, a large problem with traditional policy gradient methods is a lack of convergence due to dynamic environments as well as relatively low data efficiency. Luckily, numerous algorithms have come out in recent years that provide for a competitive self play environment that leads to optimal or near-optimal strategy such as Proximal Policy Optimization (PPO) published by OpenAI in 2017.² The uniqueness of PPO stems from the objective function which clips the probability ratio from the previous to the new model, encouraging small policy changes instead of drastic change.Probability ratio from PPO paperObjective function from PPO paper where A(t) is the advantage functionThese methods have been applied successfully to numerous multi-player Atari games, so my hypothesis was that they could easily be adapted to heads up poker. In tournament poker the majority of winnings are concentrated in the winners circle, meaning that to make a profit, wins are much more important than simply “cashing” or making some money each time. A large portion of success in heads up poker is the decision to go all in or not, so in this simulation the agent had two options, fold or go all-in.The rules of poker dictate a “small blind” and a “big blind” to start the betting, meaning that the small blind has to put in a set amount of chips and the big blind has to put in double that amount. Then cards are dealt and the players bet. The only parameters the agents were given were the following: what percentage chance they would win the current hand against a random heads up player, whether they were first to bet, and how much they had already bet. They were then fed to a simple two layer neural network.The packages for comparing winners as well as simulations to determine the percentage likelihood of winning the hand given a players cards can all be found on my Github here. In order to judge the effectiveness of PPO, I decided to compare the performance to that of the Reinforce algorithm a traditional policy gradient method. I chose to test the agents at different big blind levels as well which I set to be 1/50 of their total chips, 1/20 of their total, 1/10, 1/4 and 1/2. After each hand, their chip count was reset and the episode was run again. I ran each through a million simulations and then compared. What was extremely interesting to me was that at no level of blinds, did the difference as measured in chips won between the reinforce and the PPO algorithms become significant at a .05 level. However, the policies of the two different algorithms were extremely different. For example, here is a heatmap of the PPO policy for unsuited cards when the blinds are 1/20 of the stack, and the agent is the first to bet as compared to the reinforce policy in the exact same scenario.Axes are card values with an Ace taking on value 14Reinforce is practically a deterministic policy whereas PPO is a far gentler transition meaning that PPO will bluff more while Reinforce will play only with a likely winner. Interestingly enough, these correspond to two different types of poker players. Those that are called “tight” only play when they think they have favorable odds, whereas “loose” players will play lots of hands, bluff and even fold some large hands if they think they are beaten. As the size of the hands increases, the reinforce agent still has closer to a deterministic policy but increases the number of combinations they will bet all-in on.Axes are card values with an Ace taking on value 14This can be seen as the agent learning a key strategy in poker known as pot odds. This is the concept that as the number of chips you can win increases relative to the size of your next bet you should be willing to play more hands. The reason being that the expected value will allow a lower probability of winning as long as the pot size is relatively large. For example, if the pot is $800 and the bet for you to go all in is $200, you would only need a 20% chance of winning that hand in order to break even in the long run if you bet as you would win $1000 chips 20% of the time thus equaling your bet of 200. However, if the pot is $200 and your all-in bet is $200 you would need a 50% of winning that hand. The agents recognize this aspect and play more loosely with their chips as they are getting pot odds than they had when the blinds were lower. We can see this reach a breaking point when the blinds are 50% of each agent's total chips and practically all hands are likely to be played.Axes are card values with an Ace taking on value 14While the agents did not differ significantly in performance, as you can see from the graphs they had extremely different playing styles which was an interesting finding. I expect that as the complexity increased PPO would do better as it seems to have a smoother function that could adapt an optimal stochastic policy whereas Reinforce approached a deterministic policy. Poker, especially this limited scenario, is just one of many possible applications of policy gradient theorems that are continually being explored. It is truly a very exciting time for reinforcement learning.ReferencesSilver, Hubert, Schrittweiser, Hassabis. AlphaZero: Shedding new light on chess, shogi and go. https://deepmind.com/blog/article/alphazero-shedding-new-light-grand-games-chess-shogi-and-goSchulman, Wolski, Dhariwal, Radford, Klimov. Proximal Policy Optimization Algorithms. https://arxiv.org/abs/1710.03748Written byDerek AustinStudent at Boston College focusing on Data Science and Reinforcement LearningFollow11 Sign up for The Daily PickBy Towards Data ScienceHands-on real-world examples, research,  tutorials, and cutting-edge techniques delivered Monday to Thursday. Make learning your daily ritual. Take a lookGet this newsletterBy signing up, you will create a Medium account if you don’t already have one. Review our Privacy Policy for more information about our privacy practices.Check your inboxMedium sent you an email at  to complete your subscription.11 11 Reinforcement LearningPpoPokerPolicy GradientMore from Towards Data ScienceFollowA Medium publication sharing concepts, ideas, and codes.Read more from Towards Data ScienceMore From Medium5 YouTubers Data Scientists And ML Engineers Should Subscribe ToRichmond Alake in Towards Data Science7 Must-Haves in your Data Science CVElad Cohen in Towards Data Science30 Examples to Master PandasSoner Yıldırım in Towards Data Science21 amazing Youtube channels for you to learn AI, Machine Learning, and Data Science for freeJair Ribeiro in Towards Data ScienceThe Roadmap of Mathematics for Deep LearningTivadar Danka in Towards Data Science4 Types of Projects You Must Have in Your Data Science PortfolioSara A. Metwalli in Towards Data ScienceAn Ultimate Cheat Sheet for Data Visualization in PandasRashida Nasrin Sucky in Towards Data ScienceHow to Get Into Data Science Without a DegreeTerence S in Towards Data ScienceAboutHelpLegalGet the Medium app"
Invitation to All Aspiring Reinforcement Learning Practitioner,https://towardsdatascience.com/invitation-to-all-aspiring-reinforcement-learning-practitioner-5f87384cee67?source=tag_archive---------6-----------------------,"Reinforcement Learning,Machine Learning,Data Science,Artificial Intelligence","Photo by drmakete lab on UnsplashAn open invitation for all aspiring Reinforcement Learning (RL) practitioner to learn RL together with me in the next 3 monthsNot so while ago, I joined an RL Bootcamp held by AWS and Jakarta Machine Learning (JML). We, the participants, will be intensively mentored by the experienced representatives from AWS. The mentors will guide and introduce us to the ‘right path’ to learn about RL in the next 3 months. Not only learning the theories but we will also learn how to apply it in the real application!Isn’t it interesting?? What makes this more interesting for you is:I will share my learning in this Bootcamp to all of you!!So, take a deep breath, make a cup of tea, and I invite you to join this exciting RL journey with me!Photo by Loverna Journey on UnsplashLast week was the first session, it’s basically an introduction session about the program, the participants’ profile, what will we learn, and some introductions to RL. I couldn’t feel more excited when I heard what we are going to learn in the next 3 months, because I’m not learning this only for myself but I also will share my knowledge to all of you!For the starter, I’ll share to you the Introduction to Reinforcement Learning. Enjoy!What is Reinforcement Learning?AI Realm [Image by Author]Reinforcement Learning (RL) is a subset of machine learning which enable an agent to learn through consequences of actions in a specific environment with the aim to maximize the cumulative rewards.To give a better understanding, imagine that you have a new dog pet, named Brown, and you want to teach him some tricks. How? You can reward him when he did the right tricks and give nothing when he did it wrong. Of course, you have to do that several times until he realized what should he do.Photo by Anna Dudkova on UnsplashThe other interesting example of how RL is applied in real-life is an experiment done by YAWScience. In this experiment, the chicken will be rewarded if it succeeds to peg the pink paper rather than other coloured papers. It turns out that the chicken succeeds to peg the right one, even though in each iteration all of the papers are shuffled!As like the other field of sciences, we have to prepare a strong foundation so that we can learn a more complex field easily. To master the RL, we have to understand the pioneer of it: Markov Decision Process (MDP). And before we can understand MDP, we have to understand:Markov PropertyMarkov Process (Markov Chain)Markov Reward ProcessAs I said in the beginning, this Bootcamp provides the ‘right path’ to learn about RL. The mentor told us that the MDP concept is important to have a better understanding of RL and encouraged us to learn it by ourself.I tried to learn that concept outside of the Bootcamp by myself, and I did it! Just like I did, I also believe you can learn it by yourself! BUT I will not let you lost in the wild. I want to make sure that you and I are always on the same page. So, I’ll share some good resources which I’ve watched/read for you:Markov Decision Processes (MDPs) — Structuring a Reinforcement Learning ProblemRL Course by David Silver — Lecture 2: Markov Decision ProcessReinforcement Learning Demystified: Markov Decision Processes (Part 1)Reinforcement Learning Demystified: Markov Decision Processes (Part 2)Important Terms in RLElements involved in an RL Model. [Image by Author]There are several important terms you need to know:Agent. A decision-maker which needs to be ‘educated’ based on its own actions and experiences. Consists of a policy and a learning algorithm.Environment. The agent’s physical world. Basically, it’s nothing more than a simulation.State. The ‘situation’ where the agent is currently at. This consists of the past, present, and future state.Action. The agent’s activity inside the environment.Reward. The feedback from the environment. Can be positive or negative.Policy. The element which responsible to select what action will be done by the agent based on the observations from the environment.Episode. The sequence of states that ends with a terminal state.Reward Function. A function that incentivizes a particular agent’s behaviours and is at the core of RL.Similar to the Bias-Variance tradeoff in the Supervised Machine Learning setting, there’s also a tradeoff in RL. It’s called an Exploration vs Exploitation Tradeoff. Exploration is when the agent tried to gather some new information with the expectation that it can be more useful than current information. While exploitation is when the agent makes the best decision based on what it already knew.To give a better understanding, let say you and your friend Adam are planning to have lunch together tomorrow. Consider these 2 scenarios:“Hey Adam, I heard there’s a new restaurant in this area. Why don’t we try to visit that restaurant for our lunch tomorrow?”“Hey Adam, I know where we should go for tomorrow lunch. Why don’t we have lunch at the restaurant we’ve visited last week?”The first scenario is what we called by Exploration. You and Adam don’t know how it will taste, it can be worse than your favourite restaurant, but it can also be much better. While in the second scenario, you knew that both of you will not have a problem with the taste, and there’s a high probability that you guys will enjoy your tomorrow lunch. This scenario resembles the Exploitation phase.How does it Work?Basic Idea How RL Works. [Image by Author]For now, I’ll explain how RL works in high-level. Not to worry, because, in the future posts, we’ll get back to this and learn the details!The subscript t refers to the time step we are currently in.At the first time step (t=0), the agent receives the environment’s state as the input. Based on that input, it will decide what action it’s going to take. Once the decision is made, the action then transferred back to the environment.Then, the time step is incremented (t=1) and the environment produces a new state. Along with that, the environment also outputs a reward which will be given to the agent later.Finally, both the current state and reward are given to the agent. An agent receives a reward (can be positive or negative) as a consequence of its previous actions.This same process is repeated for future time steps.But…When this looping reached its end?It’s based on the type of your task. In general, there are 2 different types:Episodic Tasks are tasks that have a terminal state. In other words, this kind of tasks has an ending. For example, playing a chess game.Continuing Tasks are tasks that don’t have a terminal state or a never-ending task. For example, a personal assistance robot.So, if you are handling an episodic task, then the looping will be repeated until the terminal state is reached. If you are handling a continuing task, then the looping will never end.Final WordsPhoto by Ravi Pinisetti on UnsplashCongratulations for keeping up to this point!Up to now, you should have known what is Reinforcement Learning, what are the important terms in RL, and how does it work in high-level. If you do follow my advice, then you should also have learnt about the Markov Decision Process concepts. For those who haven’t read about it, I encourage you to take some time to learn that concept.Remember, this is only the beginning of our journey in learning RL! I still have a lot of materials to be shared with you. So, if you love the content and want to keep learning with me in the next 3 months, please follow my Medium account to get the notification about my future posts!In the next episode, we will learn more about how RL is applied in the autonomous race car. Check it out!About the AuthorLouis Owen is a Data Science enthusiast who always hungry for new knowledge. He pursued a Mathematics major at one of the top university in Indonesia, Institut Teknologi Bandung, under the full final-year scholarship. Recently, in July 2020, he was just graduated from his study with honours.Louis has experienced as an analytics/machine learning intern in various field of industry, including OTA (Traveloka), e-Commerce (Tokopedia), FinTech (Do-it), Smart City App (Qlue Smart City) and currently as a Data Science Consultant at The World Bank.Check out Louis’ website to know more about him! Lastly, if you have any queries or any topics to be discussed, please reach out to Louis via LinkedIn.Written byLouis OwenData Science Consultant at The World Bank | Former analytics/machine learning intern at Traveloka, Tokopedia, Do-It, Qlue Smart CityFollow465 Sign up for The Daily PickBy Towards Data ScienceHands-on real-world examples, research,  tutorials, and cutting-edge techniques delivered Monday to Thursday. Make learning your daily ritual. Take a lookGet this newsletterBy signing up, you will create a Medium account if you don’t already have one. Review our Privacy Policy for more information about our privacy practices.Check your inboxMedium sent you an email at  to complete your subscription.465 465 Reinforcement LearningMachine LearningData ScienceArtificial IntelligenceMore from Towards Data ScienceFollowA Medium publication sharing concepts, ideas, and codes.Read more from Towards Data ScienceMore From Medium5 YouTubers Data Scientists And ML Engineers Should Subscribe ToRichmond Alake in Towards Data Science7 Must-Haves in your Data Science CVElad Cohen in Towards Data Science21 amazing Youtube channels for you to learn AI, Machine Learning, and Data Science for freeJair Ribeiro in Towards Data ScienceThe Roadmap of Mathematics for Deep LearningTivadar Danka in Towards Data Science30 Examples to Master PandasSoner Yıldırım in Towards Data ScienceAn Ultimate Cheat Sheet for Data Visualization in PandasRashida Nasrin Sucky in Towards Data ScienceHow to Get Into Data Science Without a DegreeTerence S in Towards Data ScienceHow To Build Your Own Chatbot Using Deep LearningAmila Viraj in Towards Data ScienceAboutHelpLegalGet the Medium app"
"<strong class=""bt"">Monte Carlo Tree Search: Implementing Reinforcement Learning in Real-Time Game Player | Part 2</strong>",https://towardsdatascience.com/monte-carlo-tree-search-implementing-reinforcement-learning-in-real-time-game-player-25b6f6ac3b43?source=tag_archive---------7-----------------------,"AI,Reinforcement Learning,Computer Science,Python,Monte Carlo Tree Search","Photo by Anne Nygård on UnsplashIn the previous article, we covered the fundamental concepts of reinforcement learning and closed the article with these two key questions:1 — how can we find the best move among others if we cannot process all the successive states one by one due to limited amount of time?2 — how do we map the task of finding best move to long-term rewards if we are limited in terms of computational resources and time?In this article, to answer these questions, we go through the Monte Carlo Tree Search fundamentals. Since in the next articles, we will implement this algorithm on “HEX” board game, I try to explain the concepts through examples in this board game environment.If you’re more interested in the code, find it in this link. There is also a more optimized version which is applicable on linux due to utilizing cython and you can find it in here.Here are the outlines:1 — Overview2 — Exploration and Exploitation Trade-off3 — HEX: A Classic Board Game4 — Algorithm structure: Selection and Expansion5 — Algorithm structure: Rollout6 — Algorithm structure: Backpropagation7 — Advantages and DisadvantagesOverviewMonte Carlo method was coined by Stanislaw Ulam for the first time after applying statistical approach “The Monte Carlo method”. The concept is simple. Using randomness to solve problems that might be deterministic in principle. For example, in mathematics, it is used for estimating the integral when we cannot directly calculate it. Also in this image, you can see how we can calculate pi based on Monte-Carlo simulations.Fig 1: Calculating pi with the help of monte carlo simulations (source).The image above indicates the fact that in monte carlo method the more samples we gather, more accurate estimation of target value we will attain.But how does Monte Carlo Methods come in handy for general game playing?We use Monte Carlo method to estimate the quality of states stochastically based on simulations when we cannot process through all the states. Each simulation is a self-play that traverses the game tree from current state until a leaf state (end of game) is reached.So this algorithm is just perfect to our problem.- Since it samples the future state-action space, it can estimate near optimal action in current state by keeping computation effort low (which addresses the first question).- Also the fact that it chooses the best action based on long-term rewards (rewarding based on the result in tree leaves) answers the second question.This process is exactly like when a human wants to estimate the future action to come up with the best possible action in the game of chess. He thinks simulates various games (from current state to the last possible state of future) based on self-play in his/her mind and chooses the one that has the best overall results.Monte Carlo Tree Search (MCTS), which combines monte carlo methods with tree search, is a method for finding optimal decisions in a given domain by taking random samples in the decision space and building a search tree according to the results.Before we explain the algorithm structure, we should first discuss the exploration and exploitation trade-off.Exploration and Exploitation Trade-offAs explained, In reinforcement learning, an agent always aims to achieve an optimal strategy by repeatedly using the best actions that it has found in that problem (remember the chess example in the previous article). However, there is a probability that the current best action is not actually optimal. As such it will continue to evaluate alternatives periodically during the learning phase by executing them instead of the perceived optimal. In RL terms, this is known as exploration exploitation trade-off. All of the algorithms in RL (MCTS as well) are trying to balance the exploration-exploitation trade-off.I think this video best explains the concept of exploration-exploitation:HEX: A Classic Board GameNow it’s time to get to know the Hex game. It has simple rules:Fig 2: HEX board. The winner is white player because it connected both white sides with chaining stones.Black and white alternate turns.On each turn a player places a single stone of its color on any unoccupied cell.The winner is the player who forms a chain of their stones connecting their two opposing board sides.Hex can never end in a draw and be played on any n × n board [1].Now let`s go through the algorithm structure.Algorithm structure1 — Selection and ExpansionIn this step, agent takes the current state of the game and selects a node (Each node represents the state resulted by choosing an action) in tree and traverses the tree. Each move in each state is assigned with two parameters namely as total rollouts and wins per rollouts (they will be covered in rollout section).The strategy to select optimal node among other nodes really matters. Upper Confidence Bound applied to Trees (UCT) is the simplest and yet effective strategy to select optimal node. This strategy is designed to balance the exploitation-exploration trade-off. This is UCT formula:Fig 3: UCT formula. first term (w/n) indicates the exploitation and the second term computes the exploration term ( c * sqrt(log t / n))In this formula i indicates i-th node in children nodes. W is the number of wins per rollouts and n is the number of all rollouts. This part of formula represents the exploitation.Cis the exploration coefficient and it’s a constant in range of [0,1]. This parameter indicates how much agent have to favor unexplored nodes. t is the number of rollouts in parent node. the second term represents the exploration term.Let’s go through an example in order to have all information provided to sink in. Look at the image below:Fig 4: Selection phaseConsider the action C3 in depth 2. is 2 and is 1. t is the parent node number of rollouts which is 4. As you see selection phase stops in the depth where we have an unvisited node. Then in the expansion phase when we visit B1 in depth 4, we add it to tree.2 — Rollout (also called simulation, playout)In this step, based on predefined policy (like completely random selection) we select actions until we reach a terminal state. The result of game for current player is either 0 (if it loses the rollout) or 1 (if it wins the rollout) at the terminal state. In the game of HEX, the terminal state is always reachable and the result of game is loss or win (no draws). But in games like chess we might get in an infinite loop due to the extensibility of chess branch factor and depth of search tree.Fig 5: Illustrating the Rollout phase following the previous steps (selection and expansion)In the image above, after that black player chose B1 in expansion step, in the simulation step a rollout is started to terminal state of game.In here, we chose random actions to reach the terminal state of the game. In terminal state as you see, white player has won the game by connecting the left to right with its stones. Now it’s time to use this information in backpropagation part.3 — BackpropagationIn this part, we update the statistics (rollout number and the number of wins per total rollouts) in the nodes which we traversed in tree for selection and expansion parts.During backpropagation we need to update the rollout numbers and wins/losses stats of nodes. Only thing we need is to figure out the player who won the game in rollout (e.g. white player in figure 4).For the figure 4, since the black player is the winner (who chose the action in terminal state), all the states resulted by black player actions are rewarded by 1 and states which resulted by white player actions are given 0 reward (we can choose punishment by set it to -1).For all states (tree nodes selected through step 1), total rollouts number increases by one as the figure 6 displays.Fig 6: Wins (for black player), Losses (for white player) and total number of rollouts are updated for nodes through tree search.These steps keep repeating until a predefined condition ends the loop (like time limit).Advantages and DisadvantagesAdvantages:1 — MCTS is a simple algorithm to implement.2 — Monte Carlo Tree Search is a heuristic algorithm. MCTS can operate effectively without any knowledge in the particular domain, apart from the rules and end conditions, and can find its own moves and learn from them by playing random playouts.3 — The MCTS can be saved in any intermediate state and that state can be used in future use cases whenever required.4 — MCTS supports asymmetric expansion of the search tree based on the circumstances in which it is operating.Disadvantages:1 — As the tree growth becomes rapid after a few iterations, it might require a huge amount of memory.2 — There is a bit of a reliability issue with Monte Carlo Tree Search. In certain scenarios, there might be a single branch or path, that might lead to loss against the opposition when implemented for those turn-based games. This is mainly due to the vast amount of combinations and each of the nodes might not be visited enough number of times to understand its result or outcome in the long run.3 — MCTS algorithm needs a huge number of iterations to be able to effectively decide the most efficient path. So, there is a bit of a speed issue there.4 — MCTS can return a recommended move at any time because the statistics about the simulated games are constantly updated. The recommended moves aren’t great when the algorithm starts, but they continually improve as the algorithm runs.ConclusionNow we figured out how the MCTS algorithm can efficiently use randomness to sample all the possible scenarios and come up with the best action over its simulations. The quality of action chosen by MCTS in each time lies in the fact that how well it can handle the exploration and exploitation in the environment.OK, now that we covered necessary theoretical concepts so far, we’re good to go to the next level with getting our hands dirty with code. In the next article, first, we’re going to describe the whole framework and necessary modules to implement, then we will implement the basic MCTS with UCT. After that, we will improve the framework by adding more functionality to our code.If you are interested in topics and have questions , I am always available and would love to listen. Find me in linkedin and StackoverFlow.Written byMasoud Masoumi MoghadamFind me in here: https://www.linkedin.com/in/masoud-masoumi-moghadam/Follow59 Sign up for The Daily PickBy Towards Data ScienceHands-on real-world examples, research,  tutorials, and cutting-edge techniques delivered Monday to Thursday. Make learning your daily ritual. Take a lookGet this newsletterBy signing up, you will create a Medium account if you don’t already have one. Review our Privacy Policy for more information about our privacy practices.Check your inboxMedium sent you an email at  to complete your subscription.59 59 AIReinforcement LearningComputer SciencePythonMonte Carlo Tree SearchMore from Towards Data ScienceFollowA Medium publication sharing concepts, ideas, and codes.Read more from Towards Data ScienceMore From Medium5 YouTubers Data Scientists And ML Engineers Should Subscribe ToRichmond Alake in Towards Data Science7 Must-Haves in your Data Science CVElad Cohen in Towards Data Science21 amazing Youtube channels for you to learn AI, Machine Learning, and Data Science for freeJair Ribeiro in Towards Data ScienceThe Roadmap of Mathematics for Deep LearningTivadar Danka in Towards Data Science30 Examples to Master PandasSoner Yıldırım in Towards Data ScienceAn Ultimate Cheat Sheet for Data Visualization in PandasRashida Nasrin Sucky in Towards Data ScienceHow to Get Into Data Science Without a DegreeTerence S in Towards Data ScienceHow To Build Your Own Chatbot Using Deep LearningAmila Viraj in Towards Data ScienceAboutHelpLegalGet the Medium app"
Building Improved AI Agents for Doom using Double Deep Q-learning,https://towardsdatascience.com/discovering-unconventional-strategies-for-doom-using-double-deep-q-learning-609b365781c4?source=tag_archive---------8-----------------------,"Doom,Deep Learning,AI,Reinforcement Learning,Editors Pick","IntroductionOver the last few articles, we’ve discussed and implemented deep Q-learning (DQN) in the VizDoom game environment and examined it’s performance. Deep Q-learning is a highly flexible and responsive online learning approach that utilizes rapid intra-episodic updates to it’s estimations of state-action (Q) values in an environment in order to maximize reward. Q-learning can be thought of as an off-policy approach to TD, where the algorithm aims to select state-action pairs of highest value independent of the current policy being followed, and has been associated with many of the original breakthroughs for the OpenAI Atari gym environments.Gameplay of our vanilla DQN agent, trained over 500 episodes.However, DQN’s have a tendency towards optimistic overestimation Q-values, particularly at the initial stages of training, leading to a risk of suboptimal action selection and hence slower convergence. To understand this problem, recall the Q-learning update equation, which utilizes the current state reward and the highest valued state-value pair to estimate the Q-value of the current state, which is used to train the DQN.Q-learning update.Note the presence of a TD-target within the error term, which consists of the sum of the current reward and the Q-value of the state-action pair of highest value, irrespective of the current policy of the agent — as such, Q-learning is often termed as off-policy TD learning.Hence, Q-learning relies on the “foresight” of selecting a action with the highest value for the next state. But how can we be sure that the best action for the next state is the action with the highest Q-value? By definition, the accuracy of Q-values depends on the state-actions we have previously explored. Hence, the accuracy of the calculated Q-values at the beginning of training tend to be inaccurate, as we haven’t fully explored the state-action space. Taking the maximum Q-value at this state risks selecting a suboptimal action, hindering convergence. This problem is compounded by the fact that that the TD target calculation and action selection are performed with the same network, which can lead to a reinforcing bias towards suboptimal action selection.To address this, Double Deep Q-learning (DDQN), first introduced by Van Hasselt et. al, decouples the action selection from the Q-value target calculation steps by using of two networks. We can achieve this by rewriting the Q-value component of our TD target into two components, as follows:At a particular timestep and state, we use our DQN network to select what the action with the highest Q-value for our next state. A target network is then used to calculate the Q-value of taking that action at the next state, which is then combined with the reward at the current state-action to form a TD target. The networks are periodically symmetrically updated.As a consequence, DDQN can reduce Q-value overestimation and hence, ensure stable learning and faster convergence.In our previous article, we explored how Deep Q-learning can be applied to training an agent to play a basic scenario in the classic FPS game Doom, through the use of the open-source OpenAI gym wrapper library Vizdoomgym. We’ll build upon that article by adapting our approach to feature a DDQN architecture in Pytorch.ImplementationWe’ll be implementing our approach in the same VizDoomgym scenario as in our last article, Defend The Line, with the same multi-objective conditions. Some characteristics of the environment include:An action space of 3: fire, turn left, and turn right. Strafing is not allowed.Brown monsters that shoot fireballs at the player with a 100% hit rate.Pink monsters that attempt to move close in a zig-zagged pattern to bite the player.Respawned monsters can endure more damage.+1 point for killing a monster.- 1 point for dying.Initial state of the “Defend The Line Scenario”Recall that in our original DQN implementation, we already utilized two concurrent networks networks- an evaluation network for action selection, and a periodically updated target network to ensure that the generated TD-targets are stationary. We can leverage this existing setup to build our DDQN architecture without initializing more networks.Note that as the two networks are updated with one another’s weights periodically, the two models are still partially coupled, but what matters is that the action selection and Q-value evaluation are done by separate networks not sharing the same set of a weights at a particular timestep.Our Google Colaboratory implementation is written in Python utilizing Pytorch, and can be found on the GradientCrescent Github. Our approach is based on the approach detailed in Tabor’s excellent Reinforcement Learning course. As our DDQN implementation is similar to our previous vanilla DQN implementation, the overall high-level workflow is shared, and won’t be repeated here.Let’s start by importing all of the necessary packages, including the OpenAI and Vizdoomgym environments. We’ll also install the AV package necessary for Torchvision, which we’ll use for visualization. Note that the runtime must be restarted after installation is complete.#Visualization cobe for running within Colab!sudo apt-get update!sudo apt-get install build-essential zlib1g-dev libsdl2-dev libjpeg-dev nasm tar libbz2-dev libgtk2.0-dev cmake git libfluidsynth-dev libgme-dev libopenal-dev timidity libwildmidi-dev unzip# Boost libraries!sudo apt-get install libboost-all-dev# Lua binding dependencies!apt-get install liblua5.1-dev!sudo apt-get install cmake libboost-all-dev libgtk2.0-dev libsdl2-dev python-numpy git!git clone https://github.com/shakenes/vizdoomgym.git!python3 -m pip install -e vizdoomgym/!pip install avNext, we initialize our environment scenario, inspect the observation space and action space, and visualize our environment..#Check the environment. You'll need to restart the runtime for it to workimport gymimport vizdoomgymenv = gym.make('VizdoomCorridor-v0')# use like a normal Gym environmentstate = env.reset()state, reward, done, info = env.step(env.action_space.sample())print(state.shape)# env.render()env.close()Next, we’ll define our preprocessing wrappers. These are classes that inherit from the OpenAI gym base class, overriding their methods and variables in order to implicitly provide all of our necessary preprocessing. We’ll start defining a wrapper to repeat every action for a number of frames, and perform an element-wise maxima in order to increase the intensity of any actions. You’ll notice a few tertiary arguments such as fire_first and no_ops — these are environment-specific, and of no consequence to us in Vizdoomgym.class RepeatActionAndMaxFrame(gym.Wrapper):  #input: environment, repeat  #init frame buffer as an array of zeros in shape 2 x the obs space    def __init__(self, env=None, repeat=4, clip_reward=False, no_ops=0,                 fire_first=False):        super(RepeatActionAndMaxFrame, self).__init__(env)        self.repeat = repeat        self.shape = env.observation_space.low.shape        self.frame_buffer = np.zeros_like((2, self.shape))        self.clip_reward = clip_reward        self.no_ops = no_ops        self.fire_first = fire_first  def step(self, action):        t_reward = 0.0        done = False        for i in range(self.repeat):            obs, reward, done, info = self.env.step(action)            if self.clip_reward:                reward = np.clip(np.array([reward]), -1, 1)[0]            t_reward += reward            idx = i % 2            self.frame_buffer[idx] = obs            if done:                break        max_frame = np.maximum(self.frame_buffer[0], self.frame_buffer[1])        return max_frame, t_reward, done, info  def reset(self):        obs = self.env.reset()        no_ops = np.random.randint(self.no_ops)+1 if self.no_ops > 0    else 0        for _ in range(no_ops):            _, _, done, _ = self.env.step(0)            if done:                self.env.reset()                if self.fire_first:            assert self.env.unwrapped.get_action_meanings()[1] == 'FIRE'            obs, _, _, _ = self.env.step(1)        self.frame_buffer = np.zeros_like((2,self.shape))        self.frame_buffer[0] = obs    return obsNext, we define the preprocessing function for our observations. We’ll make our environment symmetrical by converting it into the Box space, swapping the channel integer to the front of our tensor, and resizing it to an area of (84,84) from its original (320,480) resolution. We’ll also greyscale our environment, and normalize the entire image by dividing by a constant.class PreprocessFrame(gym.ObservationWrapper):  #set shape by swapping channels axis #set observation space to new shape using gym.spaces.Box (0 to 1.0)    def __init__(self, shape, env=None):        super(PreprocessFrame, self).__init__(env)        self.shape = (shape[2], shape[0], shape[1])        self.observation_space = gym.spaces.Box(low=0.0, high=1.0,                                    shape=self.shape, dtype=np.float32)   def observation(self, obs):        new_frame = cv2.cvtColor(obs, cv2.COLOR_RGB2GRAY)        resized_screen = cv2.resize(new_frame, self.shape[1:],                                    interpolation=cv2.INTER_AREA)        new_obs = np.array(resized_screen, dtype=np.uint8).reshape(self.shape)        new_obs = new_obs / 255.0   return new_obsNext, we create a wrapper to handle frame-stacking. The objective here is to help capture motion and direction from stacking frames, by stacking several frames together as a single batch. In this way, we can capture position, translation, velocity, and acceleration of the elements in the environment. With stacking, our input adopts a shape of (4,84,84,1).class StackFrames(gym.ObservationWrapper):  #init the new obs space (gym.spaces.Box) low & high bounds as repeat of n_steps. These should have been defined for vizdooom    #Create a return a stack of observations    def __init__(self, env, repeat):        super(StackFrames, self).__init__(env)        self.observation_space = gym.spaces.Box( env.observation_space.low.repeat(repeat, axis=0),                              env.observation_space.high.repeat(repeat, axis=0),                            dtype=np.float32)        self.stack = collections.deque(maxlen=repeat)    def reset(self):        self.stack.clear()        observation = self.env.reset()        for _ in range(self.stack.maxlen):            self.stack.append(observation)        return  np.array(self.stack).reshape(self.observation_space.low.shape)    def observation(self, observation):        self.stack.append(observation)    return np.array(self.stack).reshape(self.observation_space.low.shape)Finally, we tie all of our wrappers together into a single make_env() method, before returning the final environment for use.def make_env(env_name, shape=(84,84,1), repeat=4, clip_rewards=False,             no_ops=0, fire_first=False):    env = gym.make(env_name)    env = PreprocessFrame(shape, env)    env = RepeatActionAndMaxFrame(env, repeat, clip_rewards, no_ops, fire_first)        env = StackFrames(env, repeat)    return envNext, let’s define our model, a deep Q-network. This is essentially a three layer convolutional network that takes preprocessed input obserations, with the generated flattened output fed to a fully-connected layer, generating probabilities of taking each action in the game space as an output. Note there are no activation layers here, as the presence of one would result in a binary output distribution. Our loss is the squared difference of our estimated Q-value of our current state-action and our predicted state-action value. We’ll use the RMSProp optimizer to minimize our loss during training.import osimport torch as Timport torch.nn as nnimport torch.nn.functional as Fimport torch.optim as optimimport numpy as npclass DeepQNetwork(nn.Module):    def __init__(self, lr, n_actions, name, input_dims, chkpt_dir):        super(DeepQNetwork, self).__init__()        self.checkpoint_dir = chkpt_dir        self.checkpoint_file = os.path.join(self.checkpoint_dir, name)        self.conv1 = nn.Conv2d(input_dims[0], 32, 8, stride=4)        self.conv2 = nn.Conv2d(32, 64, 4, stride=2)        self.conv3 = nn.Conv2d(64, 64, 3, stride=1)        fc_input_dims = self.calculate_conv_output_dims(input_dims)        self.fc1 = nn.Linear(fc_input_dims, 512)        self.fc2 = nn.Linear(512, n_actions)        self.optimizer = optim.RMSprop(self.parameters(), lr=lr)        self.loss = nn.MSELoss()        self.device = T.device('cuda:0' if T.cuda.is_available() else 'cpu')        self.to(self.device)  def calculate_conv_output_dims(self, input_dims):        state = T.zeros(1, *input_dims)        dims = self.conv1(state)        dims = self.conv2(dims)        dims = self.conv3(dims)        return int(np.prod(dims.size()))  def forward(self, state):        conv1 = F.relu(self.conv1(state))        conv2 = F.relu(self.conv2(conv1))        conv3 = F.relu(self.conv3(conv2))        # conv3 shape is BS x n_filters x H x W        conv_state = conv3.view(conv3.size()[0], -1)        # conv_state shape is BS x (n_filters * H * W)        flat1 = F.relu(self.fc1(conv_state))        actions = self.fc2(flat1)        return actions  def save_checkpoint(self):        print('... saving checkpoint ...')        T.save(self.state_dict(), self.checkpoint_file)  def load_checkpoint(self):        print('... loading checkpoint ...')        self.load_state_dict(T.load(self.checkpoint_file))Recall that the update function for Q-learning requires the following:The current state sThe current action aThe reward following the current action rThe next state s’The next action a’To supply these parameters in meaningful quantities, we need to evaluate our current policy following a set of parameters and store all of the variables in a buffer, from which we’ll draw data in minibatches during training. Hence, we need a replay memory buffer from which to store and draw observations from.import numpy as npclass ReplayBuffer(object):    def __init__(self, max_size, input_shape, n_actions):        self.mem_size = max_size        self.mem_cntr = 0        self.state_memory = np.zeros((self.mem_size, *input_shape),                                     dtype=np.float32)        self.new_state_memory = np.zeros((self.mem_size, *input_shape),                                         dtype=np.float32)        self.action_memory = np.zeros(self.mem_size, dtype=np.int64)        self.reward_memory = np.zeros(self.mem_size, dtype=np.float32)        self.terminal_memory = np.zeros(self.mem_size, dtype=np.bool)#Identify index and store  the the current SARSA into batch memory    def store_transition(self, state, action, reward, state_, done):        index = self.mem_cntr % self.mem_size        self.state_memory[index] = state        self.new_state_memory[index] = state_        self.action_memory[index] = action        self.reward_memory[index] = reward        self.terminal_memory[index] = done        self.mem_cntr += 1def sample_buffer(self, batch_size):        max_mem = min(self.mem_cntr, self.mem_size)        batch = np.random.choice(max_mem, batch_size, replace=False)        #I believe batch here is creating a list limit that is acquired through max_mem, whihch we use to subselect memory        states = self.state_memory[batch]        actions = self.action_memory[batch]        rewards = self.reward_memory[batch]        states_ = self.new_state_memory[batch]        terminal = self.terminal_memory[batch]     return states, actions, rewards, states_, terminalNext, we’ll define our agent, which differs form our vanilla DQN implementation. Our agent be using an epsilon greedy policy with a decaying exploration rate, in order to maximize exploitation over time. To learn to predict state-action-values that maximize our cumulative reward, our agent will be using the discounted future rewards obtained by sampling the stored memory.You’’ll notice that we initialize two copies of our DQN as part of our agent, with methods to copy weight parameters of our original network into a target network. While our vanilla approach utilized this setup to generate stationary TD-targets, our DDQN approach will expand beyond this:States, Actions, Rewards, and Next States (SARS) are retrieved from the Replay Memory.The evaluation network is used to generate the Q-values of all the actions of the current state .The evaluation network is used to create the Q-value of the next state, and the highest Q-values are saved as max_actions.The target network is also used to create the Q-values of the next state.The TD-target for the current state is calculated by combining the reward in the current states with the Q-values derived from the target network for the next state through the max_actions identified by the evaluation network.A loss function is calculated by comparing the TD-target with the current state Q-values, which is then used to train the network.import numpy as npimport torch as T#from deep_q_network import DeepQNetwork#from replay_memory import ReplayBufferclass DDQNAgent(object):    def __init__(self, gamma, epsilon, lr, n_actions, input_dims,                 mem_size, batch_size, eps_min=0.01, eps_dec=5e-7,                 replace=1000, algo=None, env_name=None, chkpt_dir='tmp/dqn'):        self.gamma = gamma        self.epsilon = epsilon        self.lr = lr        self.n_actions = n_actions        self.input_dims = input_dims        self.batch_size = batch_size        self.eps_min = eps_min        self.eps_dec = eps_dec        self.replace_target_cnt = replace        self.algo = algo        self.env_name = env_name        self.chkpt_dir = chkpt_dir        self.action_space = [i for i in range(n_actions)]        self.learn_step_counter = 0        self.memory = ReplayBuffer(mem_size, input_dims, n_actions)        self.q_eval = DeepQNetwork(self.lr, self.n_actions,                                    input_dims=self.input_dims,                                    name=self.env_name+'_'+self.algo+'_q_eval',                                    chkpt_dir=self.chkpt_dir)        self.q_next = DeepQNetwork(self.lr, self.n_actions,                                    input_dims=self.input_dims,                                    name=self.env_name+'_'+self.algo+'_q_next',                                    chkpt_dir=self.chkpt_dir)    #Epsilon greedy action selection    def choose_action(self, observation):        if np.random.random() > self.epsilon:          # Add dimension to observation to match input_dims x batch_size by placing in list, then converting to tensor            state = T.tensor([observation],dtype=T.float).to(self.q_eval.device)            actions = self.q_eval.forward(state)            #Return tensor, but use item() to reutnr integer            action = T.argmax(actions).item()        else:            action = np.random.choice(self.action_space)        return actiondef store_transition(self, state, action, reward, state_, done):        self.memory.store_transition(state, action, reward, state_, done)  def sample_memory(self):        state, action, reward, new_state, done = \                                      self.memory.sample_buffer(self.batch_size)        states = T.tensor(state).to(self.q_eval.device)        rewards = T.tensor(reward).to(self.q_eval.device)        dones = T.tensor(done).to(self.q_eval.device)        actions = T.tensor(action).to(self.q_eval.device)        states_ = T.tensor(new_state).to(self.q_eval.device)        return states, actions, rewards, states_, dones  def replace_target_network(self):        if self.learn_step_counter % self.replace_target_cnt == 0:            self.q_next.load_state_dict(self.q_eval.state_dict())  def decrement_epsilon(self):        self.epsilon = self.epsilon - self.eps_dec \                           if self.epsilon > self.eps_min else  self.eps_min  def save_models(self):        self.q_eval.save_checkpoint()        self.q_next.save_checkpoint()  def load_models(self):        self.q_eval.load_checkpoint()        self.q_next.load_checkpoint()    #Main DDQN difference here    def learn(self):        #First check if memory is even big enough        if self.memory.mem_cntr < self.batch_size:            return        self.q_eval.optimizer.zero_grad()        #Replace target network if appropriate        self.replace_target_network()        states, actions, rewards, states_, dones = self.sample_memory()        #Fetch indices for  matrix multiplication for q_pred        indices = np.arange(self.batch_size)        #Calculate the value of the states taken using the eval network        # We use the indices here to make sure our output q_pred is of shape batch_size instead of batch_size x action_size         q_pred = self.q_eval.forward(states)[indices, actions]        # calculate the state action value of the next state according to target network        q_next = self.q_next.forward(states_)        # calculate the state action value of the next state according to eval network        q_eval = self.q_eval.forward(states_)        #Calculate the maximum action value for the new states according to the eval network        max_actions = T.argmax(q_eval, dim=1)                #Set q_next to 0 for terminal states        q_next[dones] = 0.0        q_target = rewards + self.gamma*q_next[indices, max_actions]        loss = self.q_eval.loss(q_target, q_pred).to(self.q_eval.device)        loss.backward()        self.q_eval.optimizer.step()        self.learn_step_counter += 1        self.decrement_epsilon()With all of supporting code defined, let’s run our main training loop. We’ve defined most of this in the initial summary, but let’s recall for posterity.For every step of a training episode, we feed an input image stack into our network to generate a probability distribution of the available actions, before using an epsilon-greedy policy to select the next actionWe then input this into the network, and obtain information on the next state and accompanying rewards, and store this into our buffer. We update our stack and repeat this process over a number of pre-defined steps.At the end of an episode, we feed the next states into our network in order to obtain the next action. We also calculate the next reward by discounting the current one.We generate our target y-values through the Q-learning update function , and train our network.By minimizing the training loss, we update the network weight parameters to output improved state-action values for the next policy.We evaluate models by tracking their average score (measured over 100 training steps).env = make_env('VizdoomCorridor-v0')best_score = -np.infload_checkpoint = Falsen_games = 5000agent = DDQNAgent(gamma=0.99, epsilon=1.0, lr=0.0001,input_dims=(env.observation_space.shape),n_actions=env.action_space.n, mem_size=5000, eps_min=0.1,batch_size=32, replace=1000, eps_dec=1e-5,chkpt_dir='/content/', algo='DDQNAgent',env_name='vizdoogym')if load_checkpoint:  agent.load_models()fname = agent.algo + '_' + agent.env_name + '_lr' + str(agent.lr) +'_'+ str(n_games) + 'games'figure_file = 'plots/' + fname + '.png'n_steps = 0scores, eps_history, steps_array = [], [], []for i in range(n_games):  done = False  observation = env.reset()score = 0  while not done:    action = agent.choose_action(observation)    observation_, reward, done, info = env.step(action)    score += rewardif not load_checkpoint:      agent.store_transition(observation, action,reward, observation_, int(done))      agent.learn()    observation = observation_    n_steps += 1scores.append(score)  steps_array.append(n_steps)avg_score = np.mean(scores[-100:])if avg_score > best_score:    best_score = avg_score              print('Checkpoint saved at episode ', i)    agent.save_models()print('Episode: ', i,'Score: ', score,' Average score: %.2f' % avg_score, 'Best average: %.2f' % best_score,'Epsilon: %.2f' % agent.epsilon, 'Steps:', n_steps)eps_history.append(agent.epsilon)if load_checkpoint and n_steps >= 18000:    breakWe’ve graphed the average score of our agents together with our epsilon rate, across 500, 1000, and 2000 episodes below.Reward distribution of our agent after 500 episodes.Reward distribution of our agent after 1000 episodes.Reward distribution of our agent after 2000 episodes.Looking at the results and comparing them to our vanilla DQN implementation, you’ll immediately notice a significantly improved score score distribution across 500, 1000, and 2000 episodes. moreover, you’ll notice how the oscillations are significantly constrained, suggesting improved convergence when compared to the vanilla implementation.We can visualize the performance of our agent at 500 episodes below. You can compare this to the gameplay video at the top of the article, from our vanilla DQN implementation trained over the same duration of episodes.But at 1000 episodes and beyond, we start seeing interesting behaviour — the agent stops engaging the monsters, and simply keeps turning in circles. This has been observed before for the Vizdoom environment at the 2017 Spark AI summit.While at the summit presentation, they resolved this issue through reward function engineering, it’s important to understand why this occurs this. As each respawn is documented to significantly increase the health of the monsters, it’s possible that the agent discovered that high damage of the fireballs of the brown monsters is significantly more reliable with each respawn. However, given that the movement pattern of the pink monsters is somewhat random, relying on them is not a reliable strategy over multiple episodes of gameplay.Solving this problem would require either a modification of the environment (replacing turning with strafing may be one option), or through reward engineering — adding a reward corresponding to the time duration of survival, for example.That wraps up this implementation on Double Deep Q-learning. In our next article, we’ll move on to examining the performance of our agent in these environments with more advanced Q-learning approaches.We hope you enjoyed this article, and hope you check out the many other articles on GradientCrescent, covering applied and theoretical aspects of AI. To stay up to date with the latest updates on GradientCrescent, please consider following the publication and following our Github repository.SourcesSutton et. al, “Reinforcement Learning”Tabor, “Reinforcement Learning in Motion”Simonini, “Improvements in Deep Q Learning*Written byAdrian Yijie XuPhD Student, AI disciple — https://github.com/EXJUSTICE/ https://www.linkedin.com/in/yijie-xu-0174a325/Follow86 Sign up for The Daily PickBy Towards Data ScienceHands-on real-world examples, research,  tutorials, and cutting-edge techniques delivered Monday to Thursday. Make learning your daily ritual. Take a lookGet this newsletterBy signing up, you will create a Medium account if you don’t already have one. Review our Privacy Policy for more information about our privacy practices.Check your inboxMedium sent you an email at  to complete your subscription.86 86 DoomDeep LearningAIReinforcement LearningEditors PickMore from Towards Data ScienceFollowA Medium publication sharing concepts, ideas, and codes.Read more from Towards Data ScienceMore From Medium5 YouTubers Data Scientists And ML Engineers Should Subscribe ToRichmond Alake in Towards Data Science7 Must-Haves in your Data Science CVElad Cohen in Towards Data Science21 amazing Youtube channels for you to learn AI, Machine Learning, and Data Science for freeJair Ribeiro in Towards Data ScienceThe Roadmap of Mathematics for Deep LearningTivadar Danka in Towards Data Science30 Examples to Master PandasSoner Yıldırım in Towards Data ScienceAn Ultimate Cheat Sheet for Data Visualization in PandasRashida Nasrin Sucky in Towards Data ScienceHow to Get Into Data Science Without a DegreeTerence S in Towards Data ScienceHow To Build Your Own Chatbot Using Deep LearningAmila Viraj in Towards Data ScienceAboutHelpLegalGet the Medium app"
Policy-Gradient Methods,https://towardsdatascience.com/policy-gradient-methods-104c783251e0?source=tag_archive---------9-----------------------,"Artificial Intelligence,Deep Learning,Deep R L Explained,Reinforcement Learning,Towards Data Science","This is a new post devoted to Policy-Gradient Methods, in the “Deep Reinforcement Learning Explained” series. Policy-Gradient methods are a subclass of Policy-Based methods that estimate an optimal policy’s weights through gradient ascent.Summary of approaches in Reinforcement Learning presented until know in this series. The classification is based on whether we want to model the value or the policy (source: https://torres.ai)Intuitively, gradient ascent begins with an initial guess for the value of policy’s weights that maximizes the expected return, then, the algorithm evaluates the gradient at that point that indicates the direction of the steepest increase of the function of expected return, and so we can make a small step in that direction. We hope that we end up at a new value of policy’s weights for which the value of the expected return function is a little bit larger. The algorithm then repeats this process of evaluating the gradient and taking steps until it considers that it is eventually reached the maximum expected return.IntroductionAlthough we have coded a deterministic policy in the previous post, Policy-based methods can learn either stochastic or deterministic policies. With a stochastic policy, our neural network’s output is an action vector that represents a probability distribution (rather than returning a single deterministic action).The policy we will follow in the new method presented in this Post is selecting an action from this probability distribution. This means that if our Agent ends up in the same state twice, we may not end up taking the same action every time. Such representation of actions as probabilities has many advantages, for instance the advantage of smooth representation: if we change our network weights a bit, the output of the neural network will change, but probably just a little bit.In the case of a deterministic policy, with a discrete numbers output, even a small adjustment of the weights can lead to a jump to a different action. However, if the output is a probability distribution, a small change of weights will usually lead to a small change in output distribution. This is a very important property due gradient optimization methods are all about tweaking the parameters of a model a bit to improve the results.But how can be changed network’s parameters to improve the policy? If you remember from Post 6, we solved a very similar problem using the Cross-Entropy method: our network took observations as inputs and returned the probability distribution of the actions. In fact, the cross-entropy method is, somehow, a preliminary version of the methods that we will introduce in this Post.The key idea underlying policy gradients is reinforcing good actions: to push up the probabilities of actions that lead to higher return, and push down the probabilities of actions that lead to a lower return, until you arrive at the optimal policy. The policy gradient method will iteratively amend the policy network weights (with smooth updates) to make state-action pairs that resulted in positive return more likely, and make state-action pairs that resulted in negative return less likely.To introduce this idea we will start with a simple policy gradient method called REINFORCE algorithm ( original paper). This algorithm is the fundamental policy gradient algorithm on which nearly all the advanced policy gradient algorithms are based.REINFORCE: Mathematical definitionsLet’s look at a more mathematical definition of the algorithm since it will be good for us in order to understand the most advanced algorithms in following Posts.TrajectoryThe first thing we need to define is a trajectory, just a state-action-rewards sequence (but we ignore the reward). A trajectory is a little bit more flexible than an episode because there are no restrictions on its length; it can correspond to a full episode or just a part of an episode. We denote the length with a capital H, where H stands for Horizon, and we represent a trajectory with τ:The method REINFORCE is built upon trajectories instead of episodes because maximizing expected return over trajectories (instead of episodes) lets the method search for optimal policies for both episodic and continuing tasks.Although for the vast majority of episodic tasks, where a reward is only delivered at the end of the episode, it only makes sense just to use the full episode as a trajectory; otherwise, we don’t have enough reward information to meaningfully estimate the expected return.Return of a trajectoryWe denote the return for a trajectory τ with R(τ), and it is calculated as the sum reward from that trajectory τ:The parameter Gk is called the total return, or future return, at time step k for the transition kIt is the return we expect to collect from time step k until the end of the trajectory, and it can be approximated by adding the rewards from some state in the episode until the end of the episode using gamma γ:Expected returnRemember that the goal of this algorithm is to find the weights θ of the neural network that maximize the expected return that we denote by U(θ) and can be defined as:To see how it corresponds to the expected return, note that we have expressed the return R(τ) as a function of the trajectory τ. Then, we calculate the weighted average, where the weights are given by P(τ;θ), the probability of each possible trajectory, of all possible values that the return R(τ) can take. Note that probability depends on the weights θ in the neural network because θ defines the policy used to select the actions in the trajectory, which also plays a role in determining the states that the agent observes.Gradient ascentAs we already introduced, one way to determine the value of θ that maximizes U(θ) function is through gradient ascent.Equivalent to Hill Climbing algorithm presented in the previous Post, intuitively we can visualize that the gradient ascent draws up a strategy to reach the highest point of a hill, U(θ), just iteratively taking small steps in the direction of the gradient:source: https://torres.aiMathematically, our update step for gradient ascent can be expressed as:where α is the step size that is generally allowed to decay over time (equivalent to the learning rate decay in deep learning). Once we know how to calculate or estimate this gradient, we can repeatedly apply this update step, in the hopes that θ converges to the value that maximizes U(θ).Gradient ascent is closely related to gradient descent, where the differences are that gradient descent is designed to find the minimum of a function (steps in the direction of the negative gradient), whereas gradient ascent will find the maximum (steps in the direction of the gradient). We will use this approach in our code in PyTorch.Sampling and estimateTo apply this method, we will need to be able to calculate the gradient ∇​U(θ); however, we won’t be able to calculate the exact value of the gradient since that is computationally too expensive because, to calculate the gradient exactly, we’ll have to consider every possible trajectory, becoming an intractable problem in most cases.Instead of doing this, the method samples trajectories using the policy and then use those trajectories only to estimate the gradient. This sampling is equivalent to the approach of Monte Carlo presented in Post 13 of this series, and for this reason, method REINFORCE is also known as Monte Carlo Policy Gradients.PseudocodeIn summary, the pseudocode that describes in more detail the behavior of this method can be written as:Gradient estimation formulaLet’s look a bit more closely at the equation of step 3 in the pseudocode to understand it. We begin by making some simplifying assumptions, for example, assuming that corresponds to a full episode.Remember that R(τ) is just the cumulative rewards from the trajectory τ (the only one trajectory) at each time step. Assume that the reward signal at time step t and the sample play we are working with gives the Agent a reward of positive one (Gt=+1) if we won the game and a reward of negative one (Gt=-1) if we lost. In the other hand, the termlooks at the probability that the Agent selects action at from state st in time step t. Remember that π with the subscript θ refers to the policy which is parameterized by θ. Then, the full expression takes the gradient of the log of that probability isThis will tell us how we should change the weights of the policy θ if we want to increase the log probability of selecting action at from state st. Specifically, suppose we nudge the policy weights by taking a small step in the direction of this gradient. In that case, it will increase the log probability of selecting the action from that state, and if we step in the opposite direction will decrease the log probability.The following equation will do all of these updates all at once for each state-action pair, at and st, at each time step t in the trajectory:To see this behavior, assume that the Agent won the episode. Then, Gt is just a positive one (+1), and what the sum does is add up all the gradient directions we should step in to increase the log probability of selecting each state-action pair. That’s equivalent to just taking H+1 simultaneous steps where each step corresponds to a state-action pair in the trajectory.In the opposite, if the Agent lost, Gt becomes a negative one, which ensures that instead of stepping in the direction of the steepest increase of the log probabilities, the method steps in the direction of the steepest decrease.The proof of how to derive the equation that approximates the gradient can be safely skipped, what interests us much more is the meaning of the expression.Why optimize log probability instead of probabilityIn Gradient methods where we can formulate some probability 𝑝 which should be maximized, we would actually optimize the log probability log𝑝 instead of the probability p for some parameters 𝜃.The reason is that generally, work better to optimize log𝑝(𝑥) than 𝑝(𝑥) due to the gradient of log𝑝(𝑥) is generally more well-scaled. Remember that probabilities are bounded by 0 and 1 by definition, so the range of values that the optimizer can operate over is limited and small.In this case, sometimes probabilities may be extremely tiny or very close to 1, and this runs into numerical issues when optimizing on a computer with limited numerical precision. If we instead use a surrogate objective, namely log p (natural logarithm), we have an objective that has a larger “dynamic range” than raw probability space, since the log of probability space ranges from (-∞,0), and this makes the log probability easier to compute.Coding REINFORCENow, we will explore an implementation of the REINFORCE to solve OpenAI Gym’s Cartpole environment.The entire code of this post can be found on GitHub and can be run as a Colab google notebook using this link.InitializationsFirst, we will import all necessary packages with the following lines of code:import numpy as npimport torchimport gymfrom matplotlib import pyplot as pltAnd also the OpenAI Gym’s Cartpole Environment:env = gym.make('CartPole-v0')Policy NetworkWe will build a neural network that serves as a policy network. The policy network will accept a state vectors as inputs, and it will produce a (discrete) probability distribution over the possible actions.obs_size = env.observation_space.shape[0] n_actions = env.action_space.n  HIDDEN_SIZE = 256model = torch.nn.Sequential(             torch.nn.Linear(obs_size, HIDDEN_SIZE),             torch.nn.ReLU(),             torch.nn.Linear(HIDDEN_SIZE, n_actions),             torch.nn.Softmax(dim=0)     )The model is only two linear layers, with a ReLU activation function for the first layer, and the Softmax function for the last layer. By default, the initialization is with random weights).print (model)With the result of the neural network, the Agent samples from the probability distribution to take an action that will be executed in the Environment.act_prob = model(torch.from_numpy(curr_state).float())action = np.random.choice(np.array([0,1]),p=act_prob.data.numpy())prev_state = curr_statecurr_state, _, done, info = env.step(action)The second line of this code samples an action from the probability distribuion produced by the policy network obtained in the firt line. Then in the last line of this code the Agent takes the action.The training loopThe training loop trains the policy network by updating the parameters θ to following the pseudocode steps describes in the previous section.First we define the optimizer and initialize some variables:learning_rate = 0.003optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)Horizon = 500MAX_TRAJECTORIES = 500gamma = 0.99score = []where is learning_rate is the step size α , Horizon is the H and gammais γ in the previous pseudocode. Using these variables, the main loop with the number of iterations is defined by MAX_TRAJECTORIESis coded as:for trajectory in range(MAX_TRAJECTORIES):    curr_state = env.reset()    done = False    transitions = []         for t in range(Horizon):        act_prob = model(torch.from_numpy(curr_state).float())        action = np.random.choice(np.array([0,1]),                  p=act_prob.data.numpy())        prev_state = curr_state        curr_state, _, done, info = env.step(action)         transitions.append((prev_state, action, t+1))         if done:             break    score.append(len(transitions))    reward_batch = torch.Tensor([r for (s,a,r) in                    transitions]).flip(dims=(0,))     batch_Gvals =[]    for i in range(len(transitions)):        new_Gval=0        power=0        for j in range(i,len(transitions)):             new_Gval=new_Gval+                      ((gamma**power)*reward_batch[j]).numpy()             power+=1        batch_Gvals.append(new_Gval)    expected_returns_batch=torch.FloatTensor(batch_Gvals)    expected_returns_batch /= expected_returns_batch.max()    state_batch = torch.Tensor([s for (s,a,r) in transitions])     action_batch = torch.Tensor([a for (s,a,r) in transitions])     pred_batch = model(state_batch)     prob_batch = pred_batch.gather(dim=1,index=action_batch                 .long().view(-1,1)).squeeze()         loss= -torch.sum(torch.log(prob_batch)*expected_returns_batch)         optimizer.zero_grad()    loss.backward()    optimizer.step()With score list we will keep track of the trajectory length over training time. We keep track of the actions and states in the list transactions for the transactions of the current trajectory.Following we compute the expected return for each transaction (code snippet from the previous listing):batch_Gvals =[]for i in range(len(transitions)):   new_Gval=0   power=0   for j in range(i,len(transitions)):       new_Gval=new_Gval+((gamma**power)*reward_batch[j]).numpy()       power+=1   batch_Gvals.append(new_Gval)expected_returns_batch=torch.FloatTensor(batch_Gvals)expected_returns_batch /= expected_returns_batch.max()The listbatch_Gvals is used to compute the expected return for each transaction as it is indicated in the previous pseudocode. The list expected_return stores the expected returns for all the transactions of the current trajectory. Finally this code normalizes the rewards to be within in the [0,1] interval to improve numerical stability.The loss function requires an array of action probabilities, prob_batch, for the actions that were taken and the discounted rewards:loss = - torch.sum(torch.log(prob_batch) * disc_returns_batch)For this purpose we recomputes the action probabilities for all the states in the trajectory and subsets the action-probabilities associated with the actions that were actually taken with the following two lines of code:pred_batch = model(state_batch) prob_batch = pred_batch.gather(dim=1,index=action_batch                 .long().view(-1,1)).squeeze()An important detail is the minus sign in the loss function of this code:loss= -torch.sum(torch.log(prob_batch)*expected_returns_batch)Why we introduced a - in the log_prob? In general, we prefer to set things up so that we are minimizing an objective function instead of maximizing, since it plays nicely with PyTorch’s built-in optimizers (using stochastic gradient descend) . We should instead tell PyTorch to minimize 1-π . This loss function approaches 0 as π nears 1, so we are encouraging the gradients to maximize π for the action we took.Also, let’s remember that we use a surrogate objective, namely –log π (where log is the natural logarithm), because we have an objective that has a larger dynamic range than raw probability space (bounded by 0 and 1 by definition), since the log of probability space ranges from (–∞,0), and this makes the log probability easier to compute.Finally, mention that we included in the code the following lines of code to control the progres of the training loop:if trajectory % 50 == 0 and trajectory>0:   print('Trajectory {}\tAverage Score: {:.2f}'          .format(trajectory, np.mean(score[-50:-1])))We can visualise the results of this code running the following code:def running_mean(x):    N=50    kernel = np.ones(N)    conv_len = x.shape[0]-N    y = np.zeros(conv_len)    for i in range(conv_len):        y[i] = kernel @ x[i:i+N]        y[i] /= N    return yscore = np.array(score)avg_score = running_mean(score)plt.figure(figsize=(15,7))plt.ylabel(""Trajectory Duration"",fontsize=12)plt.xlabel(""Training Epochs"",fontsize=12)plt.plot(score, color='gray' , linewidth=1)plt.plot(avg_score, color='blue', linewidth=3)plt.scatter(np.arange(score.shape[0]),score,             color='green' , linewidth=0.3)You should be able to obtain a plot with a nicely increasing trend of the trajectory duration.We also can render how the Agent applies the policy with the following code:def watch_agent():  env = gym.make('CartPole-v0')  state = env.reset()  rewards = []  img = plt.imshow(env.render(mode='rgb_array'))  for t in range(2000):    pred = model(torch.from_numpy(state).float())    action = np.random.choice(np.array([0,1]),              p=pred.data.numpy())    img.set_data(env.render(mode='rgb_array'))     plt.axis('off')    display.display(plt.gcf())    display.clear_output(wait=True)    state, reward, done, _ = env.step(action)    rewards.append(reward)    if done:        print(""Reward:"", sum([r for r in rewards]))        break   env.close()watch_agent()Policy-based versus Value-based methodsNow that we know the two families of methods, what are the main differences between them?Policy methods, such REINFORCE, directly optimize the policy. Value methods, such as DQN, do the same indirectly, learning the value first and, obtaining the policy based on this value.Policy methods are on-policy and require fresh samples from the Environment (obtained with the policy). Instead, Value methods can benefit from old data obtained from the old policy.Policy methods are usually less sample-efficient, which means they require more interaction with the Environment. Remember that value methods can benefit from large replay buffers.Policy methods will be the more natural choice in some situations, and in other situations, value methods will be a better option. In any case, and as we will see from the next post, both families of methods can be combined to achieve hybrid methods that take advantage of each of them’ properties.SummaryIn this post, we have explained in detail the REINFORCE algorithm, and we have coded it. As a stochastic gradient method, REINFORCE works well in simple problems, and has good theorical convergence properties.As R. Sutton and G. Barton indicates in the textbook Reinforcement Learning: An Introduction, by construction, the expected update over a trajectory is in the same direction as the performance gradient. This assures an improvement in expected performance for sufficiently small, and convergence to a local optimum under standard stochastic approximation conditions for decreasing . However, as a Monte Carlo method REINFORCE may be of high variance and thus produce slow learning.But because we are using full Monte-Carlo return for calculating the gradient, the method may be of high variance and it is a problem for learning.Also, there are some limitations associated with REINFORCE algorithm:The update process is very inefficient. We run the policy once, update once, and then throw away the trajectory.The gradient estimate is very noisy. There is a possibility that the collected trajectory may not be representative of the policy.There is no clear credit assignment. A trajectory may contain many good/bad actions and whether or not these actions are reinforced depends only on the final total output.In summary, REINFORCE works well for a small problem like CartPole, but for a more complicated, for instance, Pong Environment, it will be painfully slow. Can REINFORCE be improved? Yes, there are many training algorithms that the research community created: A2C, A3C, DDPG, TD3, SAC, PPO, among others. However, programming these algorithms requires a more complex mathematical treatment, and its programming becomes more convoluted than that of REINFORCE. For this reason, in the next post, we will introduce Reinforcement Learning frameworks that simplify the use of these advanced methods, and above, all distributed algorithms.See you in the next!Deep Reinforcement Learning Explained — Jordi TORRES.AIContent of this seriesWritten byJordi TORRES.AIProfessor at UPC Barcelona Tech & Barcelona Supercomputing Center. Research focuses on Supercomputing & Artificial Intelligence https://torres.ai @JordiTorresAIFollow10 1 Sign up for The Daily PickBy Towards Data ScienceHands-on real-world examples, research,  tutorials, and cutting-edge techniques delivered Monday to Thursday. Make learning your daily ritual. Take a lookGet this newsletterBy signing up, you will create a Medium account if you don’t already have one. Review our Privacy Policy for more information about our privacy practices.Check your inboxMedium sent you an email at  to complete your subscription.10 10 1 Artificial IntelligenceDeep LearningDeep R L ExplainedReinforcement LearningTowards Data ScienceMore from Towards Data ScienceFollowA Medium publication sharing concepts, ideas, and codes.Read more from Towards Data ScienceMore From Medium5 YouTubers Data Scientists And ML Engineers Should Subscribe ToRichmond Alake in Towards Data Science7 Must-Haves in your Data Science CVElad Cohen in Towards Data Science21 amazing Youtube channels for you to learn AI, Machine Learning, and Data Science for freeJair Ribeiro in Towards Data Science30 Examples to Master PandasSoner Yıldırım in Towards Data ScienceThe Roadmap of Mathematics for Deep LearningTivadar Danka in Towards Data ScienceAn Ultimate Cheat Sheet for Data Visualization in PandasRashida Nasrin Sucky in Towards Data ScienceHow to Get Into Data Science Without a DegreeTerence S in Towards Data Science4 Types of Projects You Must Have in Your Data Science PortfolioSara A. Metwalli in Towards Data ScienceAboutHelpLegalGet the Medium app"
